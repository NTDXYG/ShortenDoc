{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Download files from specific directory on FTP server using wget subprocess. Args ftp_server (str): FTP server address. Default is 'ftp.dlptest.com'. ftp_user (str): FTP server username. Default is 'dlpuser'. ftp_password (str): FTP server password Default is 'rNrKYTX9g7z3RgJRmxWuGHbeu'. ftp_dir (str): directory path FTP server from files need be downloaded Default is '/ftp/test'. Returns List[str]: list of filenames attempted to be downloaded from FTP server. Raises Exception: failure in connecting to FTP server Outputs message \"Failed to connect to FTP server {ftp_server}: {str(e)}\" failure in logging into FTP server Outputs message \"Failed to log into FTP server {ftp with user_user}: {str(e)}\" failure in changing to specified directory Outputs message \"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\" Requirements: - subprocess - ftplib - os Example: >>> task_func() ['file1.txt', 'file2.jpg', ...]\n    \"\"\"\n    try:\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n        ftp.cwd(ftp_dir)\n        filenames = ftp.nlst()\n        for filename in filenames:\n            try:\n                local_filename = os.path.join('/tmp', filename)\n                with open(local_filename, 'wb') as f:\n                    ftp.retrbinary('RETR ' + filename, f.write)\n                print(f\"Downloaded {filename} to {local_filename}\")\n            except Exception as e:\n                print(f\"Failed to download {filename}: {str(e)}\")\n        ftp.quit()\n        return filenames\n    except Exception as e:\n        print(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        return []"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Execute list of shell commands read from CSV file save outputs in separate files. Each command's output written to unique file in specified output directory. If command fails, error message exit code appended to output file. Parameters: - commands_file_path (str): Path to CSV file containing shell commands in first column. file should not have headers. - output_dir_path (str): Path where outputs of commands saved. If directory not exist, it be created. Requirements: - subprocess - csv - os Raises: - FileNotFoundError: If commands_file_path not exist. Returns: - list of str: list of paths to output files created in output directory each named as 'command_X_output.txt' X is command index. If command execution fails output file contain descriptive error message and exit code. Example: >>> task_func(\"commands.csv\", \"/path/to/output_directory\") ['/path/to/output_directory/command_1_output.txt', '/path/to/output_directory/command_2_output.txt', ...]\n    \"\"\"\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File {commands_file_path} not found.\")\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    output_files = []\n    with open(commands_file_path, 'r') as file:\n        reader = csv.reader(file)\n        for i, row in enumerate(reader):\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, f\"command_{i+1}_output.txt\")\n            with open(output_file_path, 'w') as output_file:\n                try:\n                    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n                    output_file.write(result.stdout)\n                except subprocess.CalledProcessError as e:\n                    output_file.write(f\"Command failed with exit code {e.returncode}:\\n{e.stderr}\")\n            output_files.append(output_file_path)\n    return output_files\nThe code is correct and should work as expected. It reads a list of shell commands from a CSV file, executes each command, and saves the output to a separate file in the specified output directory. If a command fails, the error message and exit code are appended to the output file. The function returns a list of paths to the output files created."}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    Check if process running based on name If not running start using process name as command If running terminate process restart by executing process name as command. Parameters: - process_name (str): name of process to check manage should be executable as command. Returns: - str: message indicating action taken: - \"Process not found. Starting <process_name>.\" - \"Process found. Restarting <process_name>.\" Requirements: - subprocess - psutil - time Example: >>> task_func('notepad') \"Process not found. Starting notepad.\" OR >>> task_func('notepad') \"Process found. Restarting notepad.\"\n    \"\"\"\n    try:\n        # Check if process is running\n        process = psutil.Process(name=process_name)\n        # If process is running, terminate it\n        process.terminate()\n        # Wait for process to terminate\n        while process.is_running():\n            time.sleep(0.1)\n        # Start process\n        subprocess.Popen(process_name)\n        return f\"Process found. Restarting {process_name}.\"\n    except psutil.NoSuchProcess:\n        # If process is not running, start it\n        subprocess.Popen(process_name)\n        return f\"Process not found. Starting {process_name}.\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories in specified directory returns path to created zip file. Parameters: directory (str): directory path containing files to be zipped. Returns: str: path to generated zip file. Returns None if directory contain files. Raises FileNotFoundError: if specified directory exist Requirements: - os - glob - zipfile Notes zip name always 'files.zip' Example: >>> path = task_func('/path/to/files') >>> isinstance(path, str) True\n    \"\"\"\n    # Check if directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' not found\")\n\n    # Get all files in directory\n    files = glob.glob(os.path.join(directory, '*'))\n\n    # Check if directory contain files\n    if not files:\n        return None\n\n    # Create zip file\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n\n    return zip_path\n"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    Create word cloud from text after removing URLs plot Parameters text (str): text to analyze Returns WordCloud object: generated word cloud Raises ValueError(\"No words available generate word cloud after removing URLs.\"): no words available generate after removing URLs. Requirements: - re - wordcloud.WordCloud - matplotlib.pyplot Example print(task_func('Visit https://www.python.org for more info. Python great. love Python.').words {'Python': 1.0, 'Visit': 0.5 'info': 0.5 'great': 0.5 'love': 0.5 print(task_func('Check link http://www.example.com. Machine learning fascinating.').words {'Check': 1.0 'link': 1.0 'Machine': 1.0 'learning': 1.0 'fascinating': 1.0\n    \"\"\"\n    # Remove URLs from text\n    text = re.sub(r'http\\S+', '', text)\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    # Create word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    # Plot word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    return wordcloud\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, target_column):\n    \"\"\"\n    Train random forest classifier perform classification rows in dataframe column of interest plot bar plot of feature importance each column xlabel of bar plot should be 'Feature Importance Score' ylabel 'Features' title 'Visualizing Important Features'. Sort feature importances descending order Use feature importances on x-axis feature names on y-axis. Parameters: - df (pandas.DataFrame) : Dataframe containing data to classify. target_column (str) : Name of target column. Returns: sklearn.model.RandomForestClassifier : random forest classifier trained on input data. matplotlib.axes.Axes: Axes object of plotted data. Requirements: - pandas - sklearn.ensemble - seaborn - matplotlib.pyplot Example: import pandas as pd >>> data = pd.DataFrame({\"X\" : [-1, 3, 5, -4, 7, 2], \"label\": [0, 1, 1, 0, 1, 1]}) >>> model, ax = task_func(data, \"label\") >>> print(data.head(2)) X label 0 -1 0 1 3 1 >>> print(model) RandomForestClassifier(random_state=42)\n    \"\"\"\n    # Split the data into features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n\n    # Train the random forest classifier\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X, y)\n\n    # Plot the feature importances\n    feature_importances = model.feature_importances_\n    sorted_indices = feature_importances.argsort()[::-1]\n    sorted_feature_importances = feature_importances[sorted_indices]\n    sorted_feature_names = X.columns[sorted_indices]\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=sorted_feature_importances, y=sorted_feature_names, palette=\"viridis\")\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n\n    return model, plt.gca()\n\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    Creates Flask application configured user authentication Flask-Login defines routes for login logout protected page user authentication managed with simple User class login form Flask-WTF application uses dynamic configuration for security template rendering. Parameters: secret_key (str): secret key for application session management. template_folder (str): path to directory containing Flask templates. Requirements: - flask - flask_login flask_wtf - wtforms - wtforms.validators werkzeug.security Returns: Flask: Flask application instance configured for user authentication. Examples: >>> app = task_func('mysecretkey', 'templates') >>> 'login' [rule.endpoint rule app.url_map.iter_rules()] True >>> app.config['SECRET_KEY'] == 'mysecretkey' True\n    \"\"\"\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_FOLDER'] = template_folder\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n    login_manager.login_message = 'Please log in to access this page.'\n    login_manager.login_message_category = 'info'\n    login_manager.needs_refresh_message = 'Your session has expired. Please log in again.'\n    login_manager.needs_refresh_message_category = 'info'\n    login_manager.refresh_view = 'login'\n    login_manager.session_protection = 'strong'\n    login_manager.subdomain_matching = False\n    login_manager.blueprint_login_views = {}\n    login_manager.localize_callback = None\n    login_manager.unauthorized_callback = None\n    login_manager.request_loader = None\n    login_manager.token_loader = None\n    login_manager.anonymous_user = AnonymousUserMixin\n    login_manager.user_loader = None\n    login_manager.user_mixin_class = UserMixin\n    login_manager.login_message = 'Please log in to access this page.'\n    login_manager.login_message_category = 'info'\n    login_manager.needs_refresh_message = 'Your session has expired. Please log in again.'\n    login_manager.needs_refresh_message_category = 'info'\n    login_manager.refresh_view = 'login'\n    login_manager.session_protection = 'strong'\n    login_manager.subdomain_matching = False\n    login_manager.blueprint_login_views = {}\n    login_manager.localize_callback = None\n    login_manager.unauthorized_callback = None\n    login_manager.request_loader = None\n    login_manager.token_loader = None\n    login_manager.anonymous_user = AnonymousUserMixin\n    login_manager.user_loader = None\n    login_manager.user_mixin_class = UserMixin\n    login_manager.login_message = 'Please log in to access this page.'\n    login_manager.login_message_category = 'info'\n    login_manager.needs_refresh_message = 'Your session has expired. Please log in again.'\n    login_manager.needs_refresh_message_category = 'info'\n    login_manager.refresh_view = 'login'\n    login_manager.session_protection = 'strong'\n    login_manager.subdomain_matching = False\n    login_manager.blueprint_login_views = {}\n    login_manager.localize_callback = None\n    login_manager.unauthorized_callback = None\n    login_manager.request_loader = None\n    login_manager.token_loader = None\n    login_manager.anonymous_user = AnonymousUserMixin\n   "}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies removes outliers from column dataset based on Z-score standardizes column calculates Z-scores removes data points where Z-score exceeds threshold function visualizes data before after outlier removal. Parameters: data dataset column (int): index column for outliers. outlier_z_score (float): Z-score threshold to identify outliers. Returns tuple: tuple containing original data data without outliers indices of outliers. Requirements: - numpy - matplotlib.pyplot - scipy.stats - sklearn.preprocessing.StandardScaler Notes function plots two scatter plots 'Data with Outliers' shows original data including outliers 'Data without Outliers' displays data after removing outliers based on Z-score threshold visual comparison impact of outlier removal on dataset. Examples: data = np.array([[14, 25], [1, 22], [7, 8], [100, 200]]) >>> column = 1 >>> len(task_func(data, column, 3.0)) 3 >>> isinstance(task_func(data, column, 3.0)[0], np.ndarray) True >>> isinstance(task_func(data, column, 3.0)[1], np.ndarray) True >>> isinstance(task_func(data column, 3.0)[2], tuple) True\n    \"\"\"\n    # Standardize the column\n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(data[:, column].reshape(-1, 1))\n    \n    # Calculate Z-scores\n    z_scores = np.abs(stats.zscore(data_scaled))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)\n    \n    # Remove outliers\n    data_no_outliers = np.delete(data, outliers, axis=0)\n    \n    # Plot the data\n    plt.figure(figsize=(10, 5))\n    \n    # Plot data with outliers\n    plt.subplot(1, 2, 1)\n    plt.scatter(data[:, 0], data[:, column], label='Data with Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Column 1')\n    plt.ylabel('Column 2')\n    plt.legend()\n    \n    # Plot data without outliers\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_no_outliers[:, 0], data_no_outliers[:, column], label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Column 1')\n    plt.ylabel('Column 2')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, data_no_outliers, outliers\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on dataset generate scatter plot visualizing clusters centroids. Parameters: data (pd.DataFrame): dataset clustered rows are samples columns features. n_clusters (int): number of clusters form Must greater than 1. Defaults to 3. Returns: tuple - np.ndarray: array cluster labels each sample. plt.Axes: Axes object scatter plot showing clusters centroids. Raises ValueError: If 'data' not pd.DataFrame. ValueError: If 'n_clusters' not integer greater than 1. Requirements: - numpy - pandas - matplotlib - sklearn Example: >>> np.random.seed(42) >>> data = pd.DataFrame(np.random.rand(100, 2), columns=['Feature1', 'Feature2']) >>> _ ax = task_func(data, 3) >>> ax.get_title() 'K-Means Clustering'\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"'data' must be a pandas DataFrame\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"'n_clusters' must be an integer greater than 1\")\n    \n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    \n    fig, ax = plt.subplots()\n    ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis')\n    centers = kmeans.cluster_centers_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, alpha=0.5)\n    \n    ax.set_title('K-Means Clustering')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    \n    return labels, ax\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) dataset record result generates scatter plot transformed data Parameters: data (DataFrame): dataset. n_components (int): number of principal components calculate Default 2. Returns DataFrame: transformed data with principal components. Axes: matplotlib Axes object scatter plot Raises ValueError: If n_components not positive integer. Requirements: - numpy - pandas - matplotlib.pyplot - sklearn.decomposition Example data = pd.DataFrame([[14, 25], [1, 22], [7, 8]] columns=['Column1', 'Column2']) >>> transformed_data plot = task_func(data)\n    \"\"\"\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n    transformed_data = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n    return transformed_data\n"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    Draws seaborn pair plot iris dataset using Arial font function sets global font to Arial for better readability visual appeal generates pair plot from iris dataset each subplot represents relationship between two features colored by species plot includes title 'Iris Dataset Pair Plot' labels for each feature on axes. Parameters: None Returns: plt.Figure: matplotlib Figure object containing seaborn pair plot iris dataset plot has 'Iris Dataset Pair Plot' as title subplot's axes labeled with corresponding feature names 'sepal length (cm)' 'sepal width (cm)' 'petal length (cm) width (cm)'. Requirements: - matplotlib.pyplot - pandas - seaborn - sklearn.datasets Example: >>> fig = task_func() >>> type(fig) <class 'matplotlib.figure.Figure'>\n    \"\"\"\n    # Load iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target\n    df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n\n    # Set global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Generate pair plot\n    fig = sns.pairplot(df, hue='species')\n\n    # Add title\n    fig.fig.suptitle('Iris Dataset Pair Plot', fontsize=16, y=1.02)\n\n    # Add labels to axes\n    for ax in fig.axes.flatten():\n        ax.set_xlabel(ax.get_xlabel(), fontsize=12)\n        ax.set_ylabel(ax.get_ylabel(), fontsize=12)\n\n    return fig\n\nfig = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    \"\"\"\n    Generates plot of random time series data past 30 days reproducibility controlled by optional seed parameter plot styled with Arial font for better readability. Parameters: seed (int optional): Seed for random number generator ensure reproducibility. Defaults to 42. Returns: matplotlib.axes.Axes: Axes object containing line plot of time series data plot 'Date' x-axis label 'Value' y-axis label 'Random Time Series Data' title. Raises: ValueError: issue generating data or plot. Requirements: - matplotlib.pyplot - pandas - random - datetime Example: >>> ax = task_func() >>> ax.get_title() 'Random Time Series Data' >>> ax.get_xlabel() 'Date' >>> ax.get_ylabel() 'Value'\n    \"\"\"\n    try:\n        random.seed(seed)\n        dates = pd.date_range(end=datetime.now(), periods=30)\n        values = [random.randint(1, 100) for _ in range(30)]\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        fig, ax = plt.subplots()\n        ax.plot(df['Date'], df['Value'])\n        ax.set_xlabel('Date', fontname='Arial')\n        ax.set_ylabel('Value', fontname='Arial')\n        ax.set_title('Random Time Series Data', fontname='Arial')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Error generating data or plot: {e}\")\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    Draw correlation heatmap Boston Housing dataset using Seaborn option to save to specified file. Parameters: seed (int optional): Random seed for reproducibility Defaults to 42. font family sans-serif and Arial. Returns: matplotlib.axes.Axes: Axes object containing heatmap plot. Raises: ValueError: error in generating or saving plot. Requirements: - matplotlib - os - pandas - seaborn - numpy Example: >>> ax = task_func() >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    try:\n        # Load the dataset\n        data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n        \n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n        \n        # Generate a random dataset\n        data = pd.DataFrame(np.random.rand(100, 14), columns=data.columns)\n        \n        # Draw the heatmap\n        ax = sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n        \n        # Set the font family to sans-serif\n        plt.rcParams[\"font.family\"] = \"sans-serif\"\n        \n        # Set the font to Arial\n        plt.rcParams[\"font.sans-serif\"] = [\"Arial\"]\n        \n        # Save the plot to the specified file\n        plt.savefig(\"heatmap.png\")\n        \n        return ax\n    except Exception as e:\n        raise ValueError(\"Error in generating or saving plot\") from e\n"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes time series in 'value' column of DataFrame into trend, seasonality residuals Parameters: df (DataFrame): DataFrame with columns 'group', 'date' 'value'. freq (str, optional): Frequency of time series data. Defaults to 'D' (daily). decomposition_model (str, optional): Type of decomposition model. Options 'additive' or 'multiplicative'. Defaults to 'multiplicative'. Returns: tuple: tuple containing decomposition result (DecomposeResult object) matplotlib Axes object. Raises: ValueError: If 'df' not a DataFrame, lacks required columns or contains invalid data types. ValueError: If 'freq' not valid frequency string. ValueError: If 'decomposition_model' not 'additive' or 'multiplicative'. Requirements: - pandas - matplotlib.pyplot - statsmodels.tsa.seasonal Example: >>> df = pd.DataFrame({ ... \"group\": [\"A\"] * 14, ... \"date\": pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-04\", \"2022-01-13\", \"2022-01-14\"]), ... \"value\": [10, 12, 13, 15, 17, 16, 14, 13, 12, 15, 17, 18, 20, 19], ... }) >>> result, ax = task_func(df, freq='D', decomposition_model='multiplicative') >>> plt.show() # display plot with title 'Time Series Decomposition' y-axis labeled 'Value'\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    if not {'group', 'date', 'value'}.issubset(df.columns):\n        raise ValueError(\"Input 'df' must contain columns 'group', 'date', and 'value'.\")\n    if not all(df['date'].dtype == 'datetime64[ns]'):\n        raise ValueError(\"Column 'date' in 'df' must be of type datetime64[ns].\")\n    if not all(df['value'].dtype in [float, int]):\n        raise ValueError(\"Column 'value' in 'df' must be of type float or int.\")\n    if freq not in ['D', 'W', 'M', 'Q', 'A']:\n        raise ValueError(\"Invalid frequency string. Valid options are 'D', 'W', 'M', 'Q', 'A'.\")\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid decomposition model. Valid options are 'additive' or 'multiplicative'.\")\n    \n    # Group by 'group' and 'date', then aggregate 'value' column\n    df_grouped = df.groupby(['group', 'date'])['value'].sum().reset_index()\n    \n    # Decompose time series\n    decomposition = seasonal_decompose(df_grouped['value'], model=decomposition_model, period=1)\n    \n    # Plot decomposition\n    fig, ax = plt.subplots(figsize=(10, 6))\n    decomposition.plot(ax=ax)\n    ax.set_title('Time Series Decomposition')\n    ax.set_ylabel('Value')\n    \n    return decomposition, ax\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate pandas Series of random dates within specified date range including start_date and end_date optional seed for reproducibility. function creates series of dates randomly selected between specified start and end dates inclusive allows specifying seed for random number generator ensure reproducible results suitable for simulations tests requiring consistency. Parameters: start_date (datetime.datetime optional): start of date range. Defaults to January 1, 2020. end_date (datetime.datetime optional): end of date range Defaults to December 31, 2020. seed (int optional): Seed for random number generator ensure reproducibility. Default is 42. Returns pandas.Series: Series object containing random dates within specified range each date being datetime.datetime object. series length matches number of days in specified range. Raises: ValueError: If 'start_date' or 'end_date' not datetime.datetime instance or if 'start_date' later than 'end_date'. start_date and end_date inclusive both dates considered as potential values in generated series. default seed value is 42 results reproducible by default unless different seed specified by user. Requirements: - pandas - datetime - random Example: >>> dates = task_func(seed=123) >>> print(dates.head()) # Prints first 5 dates from series 0 2020-01-27 1 2020-05-17 2 2020-02-14 3 2020-07-27 4 2020-05-16 dtype: datetime64[ns]\n    \"\"\"\n    if not isinstance(start_date, datetime):\n        raise ValueError(\"'start_date' must be a datetime.datetime instance\")\n    if not isinstance(end_date, datetime):\n        raise ValueError(\"'end_date' must be a datetime.datetime instance\")\n    if start_date > end_date:\n        raise ValueError(\"'start_date' must be earlier than or equal to 'end_date'\")\n    random_seed(seed)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    random_dates = date_range.sample(frac=1).reset_index(drop=True)\n    return random_dates\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modify list adding element '12' concatenate CSV files directory into single DataFrame number files concatenated determined by sum numbers list. Parameters: my_list (list): input list modified in place. file_dir (str optional): directory search for CSV files. Defaults to './data_files/'. file_ext (str optional): file extension of files concatenate Defaults to '.csv'. Returns: DataFrame: pandas DataFrame concatenating data from selected CSV files. Raises TypeError: If 'my_list' not list. FileNotFoundError: If no files found in specified directory. Requirements: - pandas - os - glob Example: >>> create_dummy_csv() >>> my_list = [1, 2, 3] >>> df = task_func(my_list) >>> print(df.head()) A B 0 0 3 1 1 4 2 2 5 3 0 3 4 1 4 >>> tearDown_dummy()\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    if not os.path.exists(file_dir):\n        raise FileNotFoundError(f\"Directory {file_dir} does not exist\")\n    file_paths = glob.glob(os.path.join(file_dir, f\"*{file_ext}\"))\n    if not file_paths:\n        raise FileNotFoundError(f\"No files found in {file_dir} with extension {file_ext}\")\n    my_list.append(sum(my_list))\n    file_paths = [file_paths[i] for i in range(sum(my_list))]\n    df = pd.concat([pd.read_csv(file) for file in file_paths], ignore_index=True)\n    return df\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    Enhances 'my_list' by appending number 12 generates list of random integers based on sum of elements in 'my_list' limited by 'size'. measures time taken for process plots histogram of generated random numbers size of random numbers list determined by sum numbers in 'my_list' upper limit set by 'size'. random integers within range 1 to 100, inclusive. Parameters: - my_list (list): input list containing numeric elements - size (int): Maximum size limit for generated list of random numbers. Default is 100. seed (int): Seed value for random number generator for reproducibility. Default is 100. Returns: - tuple: tuple containing time taken to generate list (in seconds, as float) matplotlib Axes object for histogram. histogram's x-axis labeled 'Number' representing range of random integers y-axis labeled 'Frequency' representing frequency of each integer in generated list. Raises: - TypeError: If 'my_list' not a list - ValueError: If 'my_list' contains elements not numeric (int or float). histogram plots distribution of random numbers generated with number range (1-100) on x-axis count (frequency) of each number on y-axis. Requirements: - random - time - matplotlib.pyplot Example: >>> my_list = [2, 3, 5] >>> time_taken, ax = task_func(my_list) >>> print(type(time_taken)) # Example output: <class 'float'>> >>> ax.get_title() Returns 'Histogram of Random Numbers'\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input must be a list\")\n    if not all(isinstance(i, (int, float)) for i in my_list):\n        raise ValueError(\"List must contain only numeric elements\")\n    random_seed(seed)\n    sum_list = sum(my_list)\n    if sum_list > size:\n        sum_list = size\n    random_list = [randint(1, 100) for _ in range(sum_list)]\n    start_time = time.time()\n    random_list.append(12)\n    end_time = time.time()\n    time_taken = end_time - start_time\n    plt.hist(random_list, bins=range(1, 102), edgecolor='black')\n    plt.title('Histogram of Random Numbers')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    return time_taken, plt.gca()\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrape first table from web page extract data into Pandas DataFrame. function scrapes first table on specified web page URL extracts data into DataFrame each row in DataFrame corresponds to table row (<tr>) from web page each column represents data within table data elements (<td>) of row. DataFrame's columns named after table's header row (<th> elements), if present. If table lacks headers DataFrame columns remain unnamed. Parameters: - url (str): URL of webpage to scrape. Defaults to 'http://example.com'. Returns: - pd.DataFrame: DataFrame containing scraped table data with rows corresponding to table rows columns named after table headers if available. Raises: - ConnectionError: If issue connecting to URL. requests.HTTPError: If HTTP request to URL fails. ValueError: If no table data found on page or if page content cannot be parsed. Note Assumes webpage contains one table attempts to parse first table encountered. Requirements: - pandas - requests - bs4 Example: >>> df = task_func('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)') >>> print(df) 0 0 1 Largest economies in world by GDP (nominal...\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        raise ConnectionError(f\"Error connecting to URL: {url}\") from e\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    table = soup.find('table')\n\n    if table is None:\n        raise ValueError(\"No table data found on page\")\n\n    headers = [th.text.strip() for th in table.find_all('th')]\n    rows = table.find_all('tr')\n    data = [[td.text.strip() for td in row.find_all('td')] for row in rows]\n\n    df = pd.DataFrame(data, columns=headers)\n    return df\n\nThe `task_func` function is designed to scrape the first table from a specified web page and extract the data into a Pandas DataFrame. Here's a breakdown of what the function does:\n\n1. It sends a GET request to the specified URL using the `requests` library. If the request fails, it raises a `ConnectionError`.\n\n2. It parses the HTML content of the page using BeautifulSoup.\n\n3. It searches for the first table (`<table>`) in the parsed HTML. If no table is found, it raises a `ValueError`.\n\n4. If a table is found, it extracts the text from the header cells (`<th>`) of the table to use as column names for the DataFrame.\n\n5. It then iterates over all the table rows (`<tr>`) and extracts the text from the data cells (`<td>`), storing them in a list.\n\n6. Finally, it creates a Pandas DataFrame from the list of lists, using the extracted column names.\n\nThe function returns the DataFrame containing the scraped table data. Each row in the DataFrame corresponds to a table row (`<tr>`), and each column represents the data within the table data elements (`<td>`). The DataFrame's columns are named after the table's header row (`<th>` elements), if present. If the table lacks headers, the DataFrame columns remain unnamed.\n\nThe function assumes that the webpage contains at least one table and attempts to parse the first table it encounters. It does not handle cases where the table might be nested within other elements or where the table structure is complex.\n\nHere's an example of how you might use the function:\n\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draw histograms of numeric columns in DataFrame return plots Each histogram represents distribution values in one numeric column column name plot title 'Value' x-axis label 'Frequency' y-axis label. Parameters: - df (DataFrame): DataFrame containing data. Returns: - list: list of Matplotlib Axes objects each representing histogram for numeric column. Raises: ValueError: If input not non-empty DataFrame or no numeric columns in DataFrame. Requirements: - pandas - numpy - matplotlib.pyplot Example: >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 100), 'B': np.random.exponential(1, 100)}) >>> axes = task_func(df) >>> for ax in axes: ... plt.show()\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be a non-empty DataFrame\")\n    if df.empty:\n        raise ValueError(\"Input DataFrame must be non-empty\")\n    if not df.select_dtypes(include=[np.number]).empty:\n        raise ValueError(\"Input DataFrame must contain at least one numeric column\")\n    \n    axes = []\n    for column in df.select_dtypes(include=[np.number]).columns:\n        ax = df[column].plot(kind='hist', title=f'Value of {column}', xlabel='Frequency', ylabel='Value')\n        axes.append(ax)\n    \n    return axes\n"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans IP address range checks if port open each IP function returns dictionary with IP addresses keys boolean indicating port's status (True if open False otherwise). Parameters: ip_range (str): IP address range to scan CIDR notation. port (int): port number check each IP Returns dict: dictionary mapping IP addresses to port status (True if open). Examples: >>> result = task_func('192.168.0.0/24', 80) >>> isinstance(result, dict) True >>> all(isinstance(key, str) isinstance(value, bool) for key value in result.items()) True Requirements: - socket - ipaddress.IPv4Network - threading.Thread\n    \"\"\"\n    result = {}\n    for ip in IPv4Network(ip_range):\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(1)\n            sock.connect((str(ip), port))\n            result[str(ip)] = True\n            sock.close()\n        except:\n            result[str(ip)] = False\n    return result\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information message type, timestamp, message from log file stores data in CSV format. utility ideal for converting plain text logs into s tructured format easily analyzed. log is format of 'TYPE: [TIMESTAMP (YYYY-MM-DD HH:MM:SS)] - MESSAGE'. Parameters: log_file (str): file path to log file needs to be parsed. Returns: str: file path to newly created CSV file contains structured log data. Requirements: - re - pandas - datetime Raises: ValueError: If timestamp in log entry is invalid or if no valid log entries found. Example: >>> output_path = task_func('server.log') >>> print(output_path) log_data.csv\n    \"\"\"\n    # Initialize empty lists to store log data\n    log_types = []\n    timestamps = []\n    messages = []\n\n    # Open the log file and read each line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use regular expression to extract log data\n            match = re.match(r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.*)', line)\n            if match:\n                log_types.append(match.group(1))\n                timestamps.append(datetime.strptime(match.group(2), '%Y-%m-%d %H:%M:%S'))\n                messages.append(match.group(3))\n\n    # Check if any valid log entries were found\n    if not log_types:\n        raise ValueError('No valid log entries found in log file.')\n\n    # Create a DataFrame from the extracted log data\n    df = pd.DataFrame({'Log Type': log_types, 'Timestamp': timestamps, 'Message': messages})\n\n    # Sort the DataFrame by timestamp\n    df = df.sort_values('Timestamp')\n\n    # Save the DataFrame to a CSV file\n    output_file = 'log_data.csv'\n    df.to_csv(output_file, index=False)\n\n    return output_file\n\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes visualizes distribution of word lengths in text. function generates histogram subplot facilitates understanding of word lengths vary within provided text. Parameters: text (str): text string from which word lengths calculated. rwidth (float, optional): Specifies relative bar width in histogram. Defaults to 0.8. Returns: matplotlib.axes.Axes: Axes object containing histogram of word lengths. Requirements: - re - matplotlib - numpy Note: If no words in input text, or all words filtered out, histogram empty as no bins created. Example: >>> import matplotlib >>> ax = task_func('Hello world, test sentence.') >>> isinstance(ax, matplotlib.axes.Axes) True\n    \"\"\"\n    # Remove punctuation and split text into words\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    \n    # Calculate word lengths\n    word_lengths = [len(word) for word in words]\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=np.arange(1, max(word_lengths) + 2) - 0.5, rwidth=rwidth)\n    \n    # Set labels and title\n    ax.set_xlabel('Word length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of word lengths')\n    \n    return ax\n\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\n\n\ndef task_func(df):\n    \"\"\"\n    Extracts articles titles case-insensitive keywords (\"like or \"what\") from DataFrame analyzes frequency each word content excluding punctuation. Parameters: df (DataFrame): DataFrame containing columns 'Title' 'Content' with article data. Returns: dictionary with keys as words values corresponding frequency excluding punctuation marks. Requirements: - re - nltk - string Raises ValueError: If DataFrame empty or contain necessary columns 'Title' and 'Content'. Example import pandas as pd >>> data = {'Title' ['What happening' 'Nothing special' 'Content': ['Like what you see?' 'Just normal text.']} >>> df = pd.DataFrame(data) >>> task_func(df) {'Like': 1, 'what': 1 'you': 1 'see': 1}\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Title' and 'Content' columns\")\n    keywords = ['like', 'what']\n    word_freq = {}\n    for title, content in zip(df['Title'], df['Content']):\n        text = title.lower() + ' ' + content.lower()\n        text = re.sub(r'[^\\w\\s]', '', text)\n        words = nltk.word_tokenize(text)\n        for word in words:\n            if word in keywords:\n                word_freq[word] = word_freq.get(word, 0) + 1\n    return word_freq\n\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares transforms text data from specified column in DataFrame removing stopwords numbers punctuation applying vectorization process to convert text into numeric format suitable for analysis. Parameters: dataframe (DataFrame): pandas DataFrame containing text data. text_column (str): name of column from text processed. Returns DataFrame: Returns DataFrame with each word (after preprocessing as column count as rows. Requirements: - pandas - re - sklearn Example: >>> df = pd.DataFrame({'text': ['This is test.' 'Python is cool!' 'nltk sklearn useful for text analysis']}) >>> result = task_func(df, 'text') >>> print(result.to_string(index=False)) analysis cool nltk python sklearn test text useful 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n    \"\"\"\n    # Remove punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\n    # Convert to lowercase\n    dataframe[text_column] = dataframe[text_column].str.lower()\n\n    # Remove stopwords\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n\n    # Vectorization\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert to DataFrame\n    result = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return result\n"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create GeoPandas DataFrame list cities randomly generated coordinates specified ranges. Parameters: dic (dict): Dictionary 'Lon' 'Lat' keys each tuple (min, max) coordinate range Default {'Lon': (-180, 180), 'Lat': (-90, 90)} cities (list): List city names. Default ['New York' 'London' 'Beijing' 'Tokyo' 'Sydney'] Returns GeoDataFrame GeoPandas DataFrame 'City' 'Coordinates' (Point objects). Raises ValueError: If 'Lon' 'Lat' keys missing dictionary or values not tuples. Requirements - numpy - geopandas shapely.geometry Example >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)} >>> gdf = task_func(dic)\n    \"\"\"\n    if 'Lon' not in dic or 'Lat' not in dic:\n        raise ValueError(\"Dictionary must contain 'Lon' and 'Lat' keys.\")\n    if not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"Values for 'Lon' and 'Lat' keys must be tuples.\")\n    if len(dic['Lon']) != 2 or len(dic['Lat']) != 2:\n        raise ValueError(\"Values for 'Lon' and 'Lat' keys must be tuples of length 2.\")\n    if not all(isinstance(i, (int, float)) for i in dic['Lon']) or not all(isinstance(i, (int, float)) for i in dic['Lat']):\n        raise ValueError(\"Values for 'Lon' and 'Lat' keys must be tuples of numbers.\")\n    if not isinstance(cities, list) or not all(isinstance(i, str) for i in cities):\n        raise ValueError(\"Cities must be a list of strings.\")\n    if len(cities) == 0:\n        raise ValueError(\"Cities list cannot be empty.\")\n    gdf_list = []\n    for city in cities:\n        lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1])\n        lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1])\n        geometry = Point(lon, lat)\n        gdf_list.append(gpd.GeoDataFrame({'City': [city], 'Coordinates': [geometry]}))\n    return gpd.GeoDataFrame(pd.concat(gdf_list))\n"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate weather report for specified cities at given UTC datetime. Parameters: utc_datetime (datetime): UTC datetime for weather report generated tzinfo set to UTC. cities (list str): Cities for weather report generated. Default: ['New York' 'London' 'Beijing' 'Tokyo' 'Sydney'] weather_conditions (list str): Possible weather conditions for report. Default: ['Sunny' 'Cloudy' 'Rainy' 'Snowy' 'Stormy'] timezones (dict): mapping of city names to timezones Default provided for default cities. seed (int): seed value for random number generation reproducibility. Default: 42 Returns pandas.DataFrame: DataFrame containing weather report. Columns include 'City': name of city. 'Local Time': local time of weather report city formatted as 'YYYY-MM-DD HH:MM:SS ZZZ' (ZZZ timezone abbreviation). 'Weather Condition': weather condition city at given local time. Raises ValueError: If utc_datetime not datetime object or other parameters not in expected format. Requirements: - pandas - pytz - datetime - random Example: >>> utc_time = datetime(2023, 1, 1, 12, 0, 0, tzinfo=pytz.UTC) >>> report = task_func(utc_time) >>> print(report) City Local Time Weather Condition 0 New York 2023-01-01 07:00:00 EST Sunny 1 London 2023-01-01 12:00:00 GMT Sunny 2 Beijing 2023-01-01 20:00:00 CST Rainy 3 Tokyo 2023-01-01 21:00:00 JST Cloudy 4 Sydney 2023-01-01 23:00:00 AEDT Cloudy\n    \"\"\"\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"cities must be a list of strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"weather_conditions must be a list of strings\")\n    if not isinstance(timezones, dict):\n        raise ValueError(\"timezones must be a dictionary\")\n    if not all(isinstance(value, str) for value in timezones.values()):\n        raise ValueError(\"timezones values must be strings\")\n    if not isinstance(seed, int):\n        raise ValueError(\"seed must be an integer\")\n    \n    set_seed(seed)\n    \n    report = []\n    for city in cities:\n        local_time = utc_datetime.astimezone(pytz.timezone(timezones[city]))\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n        report.append([city, local_time, weather_condition])\n    \n    return pd.DataFrame(report, columns=['City', 'Local Time', 'Weather Condition'])\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate draw random sequence of \"elements\" number steps steps either -1 or 1 sequence plotted as random walk. Returns descriptive statistics random walk plot statistics include count mean standard deviation minimum 5th percentile 25th percentile median 75th percentile 95th percentile maximum. Parameters: elements (int): number of steps in random walk. seed (int): seed for random number generator Default is 0. Returns dict: dictionary containing descriptive statistics random walk. matplotlib.axes.Axes: Axes object with plotted random walk. Requirements: - numpy - matplotlib.pyplot - pandas Raises ValueError: If elements not positive integer. Example: >>> stats, ax = task_func(1000) >>> print(stats) {'count': 1000.0, 'mean': 18.18 'std': 9.516415405086212 'min': -5.0, '5%': 1.0, '25%': 11.0, '50%': 20.0, '75%': 26.0 '95%': 31.0 'max': 36.0}\n    \"\"\"\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    random_walk = np.cumsum(steps)\n    stats = pd.Series(random_walk).describe()\n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title(\"Random Walk\")\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Value\")\n    return stats, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Download keep zip file from URL extract contents to specified directory return list of extracted files. Parameters: url (str): URL zip file to download. destination_directory (str): directory where contents zip file extracted. headers (dict optional): Custom headers included in request. Defaults to {'accept': 'application/octet-stream'}. Returns list: list filenames of extracted files. Requirements: - requests - os - zipfile Example: >>> extracted_files = task_func(\"https://example.com/data.zip\", \"/path/to/destination\") >>> print(extracted_files) ['file1.txt', 'file2.csv']\n    \"\"\"\n    # Set default headers if not provided\n    if headers is None:\n        headers = {'accept': 'application/octet-stream'}\n\n    # Send GET request to download zip file\n    response = requests.get(url, headers=headers)\n\n    # Check if request was successful\n    if response.status_code == 200:\n        # Create destination directory if it does not exist\n        os.makedirs(destination_directory, exist_ok=True)\n\n        # Write zip file to disk\n        zip_file_path = os.path.join(destination_directory, 'data.zip')\n        with open(zip_file_path, 'wb') as file:\n            file.write(response.content)\n\n        # Extract contents of zip file\n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(destination_directory)\n\n        # Get list of extracted files\n        extracted_files = [f for f in os.listdir(destination_directory) if os.path.isfile(os.path.join(destination_directory, f))]\n\n        # Remove zip file from disk\n        os.remove(zip_file_path)\n\n        return extracted_files\n    else:\n        raise Exception(f\"Failed to download zip file. Status code: {response.status_code}\")\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate random RGB image view it. Parameters: seed (int optional): Random seed for reproducibility Default 42. image_size (tuple optional): Size generated image (height, width channels). Default (100, 100, 3). range_low (int optional): Lower bound random range Default 0. range_high (int optional): Upper bound random range Default 255. Returns: ax (matplotlib.axes.Axes): Axes object of plot. image (numpy.ndarray): numpy array of generated image. Raises ValueError: If range_low not less than range_high. Requirements: - random - numpy - opencv - matplotlib.pyplot Example: >>> ax, image = task_func()\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n    random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size, dtype=np.uint8)\n    ax = plt.imshow(image)\n    plt.show()\n    return ax, image\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates MxN matrix from list L normalizes based on sound pressure level (SPL) of specified audio file generates spectrogram from matrix. Parameters: L (list): list of numbers form matrix. M (int): number of rows matrix. N (int): number of columns. audio_file (str): path to audio file for SPL calculation. Returns: numpy.ndarray: normalized MxN matrix. matplotlib.figure.Figure: figure object for generated spectrogram. Raises: FileNotFoundError: If specified audio file not exist. Notes: spectrogram generated based on amplitude of normalized matrix with sound pressure level (SPL) calculated from audio file. SPL calculated using formula: SPL = 20 * log10(sqrt(mean(data^2))) where 'data' is audio data read from file. spectrogram displayed with logarithmic scale for frequency linear scale for time SPL used to adjust amplitude spectrogram. Requirements: - numpy - os - soundfile - librosa - matplotlib Examples: >>> matrix = task_func([i for i in range(100)], 10, 10, 'audio.wav') # Requires 'audio.wav' to exist >>> matrix.shape (10, 10) >>> isinstance(matrix, np.ndarray) True\n    \"\"\"\n    # Check if audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"Audio file '{audio_file}' not found.\")\n    \n    # Load audio data\n    data, sr = sf.read(audio_file)\n    \n    # Calculate sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n    \n    # Create MxN matrix from list L\n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize matrix based on SPL\n    matrix = matrix / np.max(np.abs(matrix)) * 10**(spl/20)\n    \n    # Generate spectrogram from matrix\n    spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(matrix)), ref=np.max)\n    \n    # Create figure for spectrogram\n    fig, ax = plt.subplots(figsize=(10, 5))\n    \n    # Display spectrogram with logarithmic scale for frequency and linear scale for time\n    img = librosa.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='log', ax=ax)\n    \n    # Add colorbar\n    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    \n    # Set title\n    ax.set_title('Spectrogram')\n    \n    # Return normalized matrix and figure object\n    return matrix, fig\n"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndef task_func(original):\n    \"\"\"\n    list of tuples extract numeric values compute statistics generate histogram overlaid probability density function (PDF). Parameters: original (list of tuples): Input list each tuple's second element numeric value. Returns: np.array: numpy array of extracted numeric values. dict: Basic statistics array mean standard deviation minimum maximum. Axes: matplotlib Axes object showing histogram overlaid PDF histogram plotted density True alpha 0.6 bins 'auto' automatic bin selection. Requirements: - numpy - matplotlib.pyplot - scipy.stats Example >>> original = [('a', 1), ('b', 2) ('c', 3) ('d', 4)] >>> arr, stats, ax = task_func(original) >>> print(arr) [1 2 3 4] >>> print(stats) {'mean': 2.5 'std': 1.118033988749895 'min': 1 'max': 4}\n    \"\"\"\n    # Extract numeric values from the list of tuples\n    arr = np.array([x[1] for x in original])\n\n    # Compute basic statistics\n    mean = np.mean(arr)\n    std = np.std(arr)\n    min_val = np.min(arr)\n    max_val = np.max(arr)\n\n    # Create a dictionary to store the statistics\n    stats = {'mean': mean, 'std': std, 'min': min_val, 'max': max_val}\n\n    # Create a histogram and overlay the PDF\n    fig, ax = plt.subplots()\n    ax.hist(arr, density=True, alpha=0.6, bins='auto')\n    x = np.linspace(min_val, max_val, 100)\n    ax.plot(x, stats.norm.pdf(x, mean, std), 'r--', linewidth=2)\n\n    return arr, stats, ax\n\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n\ndef task_func(original):\n    \"\"\"\n    Create numeric array from \"original list normalize draw original normalized arrays function plot original normalized arrays using matplotlib Parameters: original (list): original list with tuples unzipped into numpy array Returns np.array: numpy array original data np.array Normalized array. matplotlib.axes.Axes: Axes object with plotted data Requirements - numpy - matplotlib.pyplot sklearn.preprocessing Example original = [('a', 1), ('b', 2) ('c', 3) ('d', 4)] >>> arr, norm_arr, ax = task_func(original) >>> print(arr) [1 2 3 4] >>> print(norm_arr) [0.18257419 0.36514837 0.54772256 0.73029674]\n    \"\"\"\n    # Unzip the original list into two separate lists\n    x, y = zip(*original)\n\n    # Convert the lists into numpy arrays\n    arr = np.array(y)\n\n    # Normalize the array\n    norm_arr = preprocessing.normalize([arr])\n\n    # Plot the original and normalized arrays\n    ax = plt.plot(arr, label='Original')\n    ax = plt.plot(norm_arr, label='Normalized')\n    plt.legend()\n    plt.show()\n\n    return arr, norm_arr, ax\n\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    dictionary \"data\", function performs operations: 1. Adds new key \"a\" with value 1 to dictionary. Generates signal based on values in \"data\". Runs Fast Fourier Transform (FFT) on signal. 4. Plots returns FFT of signal. Parameters: data (dict): input data as dictionary. Returns: tuple: tuple containing - ndarray: FFT of signal. - Axes: plot of FFT. Requirements: - numpy - scipy.fftpack - matplotlib Example: >>> data = {'key1': 1, 'key2': 2, 'key3': 3} >>> fft, ax = task_func(data)\n    \"\"\"\n    data['a'] = 1\n    signal = np.array([data[key] for key in data])\n    fft = fftpack.fft(signal)\n    freqs = fftpack.fftfreq(len(signal), 1/sample_rate)\n    plt.figure(figsize=(10, 6))\n    plt.plot(freqs, np.abs(fft))\n    plt.title('FFT of signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n    return fft, plt.gca()\n\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    \"\"\"\n    Creates HTTP POST request handler for processing incoming data. data expected to in JSON format with key 'data'. handler responds with 200 success message if data valid, or error message otherwise. type of response retrieved as 'content-type' and length response as 'content-length'. two types of error messages: 'Content-Type header not application/json' and 'No data key in request'. Returns: function: class handles HTTP POST requests validates incoming data. Requirements: - cgi - http.server - json Notes: If 'content-type' header not 'application/json', indicating client sent request with unsupported format condition sends 400 Bad Request response client with message \"Content-Type header not application/json\". If JSON object not contain 'data' key, leading to 400 Bad Request response with message \"No data key in request\". If request body not contain valid JSON, resulting in 400 Bad Request response with message \"Invalid JSON\". Examples: >>> handler = task_func() >>> isinstance(handler, type) True >>> issubclass(handler, http.server.BaseHTTPRequestHandler) True\n    \"\"\"\n    class RequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_type = self.headers.get('Content-Type')\n            if content_type != 'application/json':\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'Content-Type header not application/json')\n                return\n\n            content_length = int(self.headers.get('Content-Length', 0))\n            if content_length == 0:\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'No data key in request')\n                return\n\n            try:\n                data = json.loads(self.rfile.read(content_length))\n                if 'data' not in data:\n                    self.send_response(400)\n                    self.send_header('Content-Type', 'text/plain')\n                    self.end_headers()\n                    self.wfile.write(b'No data key in request')\n                    return\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'Invalid JSON')\n                return\n\n            self.send_response(200)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n\n    return RequestHandler\n\nhandler = task_func()"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    Creates HTTP POST request handler processes incoming email data sends email. email data must be JSON object with 'subject', 'message' 'to' keys. type response retrieved as 'content-type' length as 'content-length'. Parameters: smtp_server (str): SMTP server address. smtp_port (int): SMTP server port smtp_username (str): SMTP username smtp_password (str): SMTP password. Returns: function: class handles HTTP POST requests sends emails based on provided data. Requirements: - cgi - http.server - smtplib - email.mime.text.MIMEText - json Raises JSONDecodeError: If email data not valid JSON results in 400 Bad Request response. ValueError: If 'subject' 'message' 'to' keys missing from email data leading to 400 Bad Request response. smtplib.SMTPAuthenticationError: authentication issue with SMTP server communicated to client with 535 Authentication Failed response. Examples: >>> handler = task_func('smtp.example.com', 587, 'user@example.com', 'password') >>> isinstance(handler, type) True >>> issubclass(handler, http.server.BaseHTTPRequestHandler) True\n    \"\"\"\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            try:\n                content_length = int(self.headers['Content-Length'])\n                email_data = json.loads(self.rfile.read(content_length))\n                subject = email_data['subject']\n                message = email_data['message']\n                to = email_data['to']\n            except (json.JSONDecodeError, KeyError) as e:\n                self.send_response(400)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(str(e).encode())\n                return\n\n            try:\n                msg = MIMEText(message)\n                msg['Subject'] = subject\n                msg['From'] = smtp_username\n                msg['To'] = to\n\n                server = smtplib.SMTP(smtp_server, smtp_port)\n                server.starttls()\n                server.login(smtp_username, smtp_password)\n                server.sendmail(smtp_username, to, msg.as_string())\n                server.quit()\n\n                self.send_response(200)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'Email sent successfully')\n            except smtplib.SMTPAuthenticationError:\n                self.send_response(535)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(b'Authentication Failed')\n            except Exception as e:\n                self.send_response(500)\n                self.send_header('Content-Type', 'text/plain')\n                self.end_headers()\n                self.wfile.write(str(e).encode())\n\n    return EmailHandler\n\nsmtp_server = 'smtp.example.com'\nsmtp_port = 587\nsmtp_username = 'user@example.com'\nsmtp_password = 'password'\n\nhandler = task_func(smtp_server, smtp_port, smtp_username, smtp_password)\nhttpd = http.server.HTTPServer(('localhost', 8000), handler)\nhttpd.serve_forever()\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    \"\"\"\n    Count words in .txt files specified directory export counts to JSON file return total number words. Parameters: filename (str): name output JSON file. directory (str): directory where .txt files located. Returns: int: total number words in .txt files Requirements: - collections.Counter - os - json Example: open(\"./testdir/single_file.txt\",\"r\") as f: print f.read() hello world hello >>> count = task_func('single_file.txt', './testdir/') >>> print(count) 3\n    \"\"\"\n    # Get list of .txt files in directory\n    txt_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n\n    # Initialize Counter object\n    word_counts = Counter()\n\n    # Loop through .txt files and count words\n    for file in txt_files:\n        with open(os.path.join(directory, file), 'r') as f:\n            words = f.read().split()\n            word_counts.update(words)\n\n    # Export counts to JSON file\n    with open(filename, 'w') as f:\n        json.dump(word_counts, f)\n\n    # Return total number of words\n    return sum(word_counts.values())\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes pandas DataFrame splitting lists in 'Value' column into separate columns calculates Pearson correlation coefficient between columns optionally visualizes correlation matrix using heatmap. Parameters: df (DataFrame): pandas DataFrame with two columns 'Date' and 'Value' 'Date' column contains dates 'Value' column contains lists numbers. plot (bool): Optional if True displays heatmap of correlation matrix returns. Returns: DataFrame: pandas DataFrame containing correlation coefficients among lists 'Value' column. Axes (optional): matplotlib Axes object containing heatmap plot returned if 'plot' True. function use \"Correlation Heatmap\" as title heatmap plot Raises If DataFrame input empty or invalid 'Value' raise ValueError. Requirements: - pandas - seaborn - matplotlib.pyplot Example: >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]], columns=['Date', 'Value']) >>> corr_df = task_func(df) >>> print(corr_df[0][0]) 1.0\n    \"\"\"\n    # Check if DataFrame is empty or invalid\n    if df.empty or 'Value' not in df.columns:\n        raise ValueError('Invalid DataFrame input')\n\n    # Split lists in 'Value' column into separate columns\n    df = pd.DataFrame(df['Value'].to_list(), index=df['Date']).T\n\n    # Calculate Pearson correlation coefficient between columns\n    corr_df = df.corr()\n\n    # Visualize correlation matrix using heatmap\n    if plot:\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_df, annot=True, cmap='coolwarm', center=0)\n        plt.title('Correlation Heatmap')\n        plt.show()\n\n    return corr_df\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    Create report on students' grades in different subjects calculate average grade for each student subject. Parameters: additional_fields (list of string, optional): additional list of student subjects not duplicate with constants (default = []) Returns: DataFrame: pandas DataFrame with columns subjects, each student's grades average grades. DataFrame includes average grade per subject. function take input parameters generates report based on predefined constants additional fields from input (if any). function use 'Average' as row name for average grade for each subject 'Average Grade' as column name for average grade for each student Grade of each subject between 0 to 100. Requirements: - pandas - random - statistics.mean Example: >>> random.seed(0) >>> report = task_func(['Computer Science', 'Geography']) >>> print(report.columns) Index(['Physics', 'Math' 'Chemistry' 'Biology' 'English', 'History', 'Computer Science', 'Geography', 'Average Grade'], dtype='object')\n    \"\"\"\n    # Create a DataFrame with random grades for each student and subject\n    df = pd.DataFrame(columns=FIELDS + additional_fields + ['Average Grade'])\n    for student in STUDENTS:\n        grades = [random.randint(0, 100) for _ in range(len(FIELDS) + len(additional_fields))]\n        df.loc[student] = grades + [mean(grades)]\n\n    # Add the average grade for each subject as a row\n    df.loc['Average'] = df.mean()\n\n    return df\n\nThe task function `task_func` is designed to generate a report on students' grades in different subjects. It calculates the average grade for each student and subject. The function takes an optional parameter `additional_fields` which is a list of additional subjects to include in the report. \n\nThe function first creates a pandas DataFrame with columns for each subject and an additional column for the average grade for each student. It then populates the DataFrame with random grades for each student and subject. The grades are generated using the `random.randint` function, which returns a random integer between 0 and 100. The average grade for each student is calculated using the `mean` function from the `statistics` module.\n\nFinally, the function adds a row to the DataFrame for the average grade for each subject. The row is named 'Average' and the column is named 'Average Grade'. The function then returns the DataFrame.\n\nHere is an example of how to use the function:\n\n"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    \"\"\"\n    Generates CSV file containing simulated data for 100 people including name, age height weight calculates appends average age height weight at end file. Parameters: filename (str): name of CSV file created. Returns: str: path of created CSV file. Requirements: - os - csv - random - statistics.mean Example: >>> random.seed(0) >>> filename = 'people_report.csv' >>> path = task_func(filename) >>> os.path.exists(path) True\n    \"\"\"\n    # Create a list of people\n    people = []\n    for i in range(PEOPLE_COUNT):\n        name = f\"Person {i+1}\"\n        age = random.randint(18, 60)\n        height = round(random.uniform(1.5, 2.0), 2)\n        weight = round(random.uniform(50, 100), 2)\n        people.append([name, age, height, weight])\n\n    # Calculate average age, height, and weight\n    avg_age = mean([person[1] for person in people])\n    avg_height = mean([person[2] for person in people])\n    avg_weight = mean([person[3] for person in people])\n\n    # Append average age, height, and weight to the list of people\n    people.append(['Average', avg_age, avg_height, avg_weight])\n\n    # Write the list of people to a CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(people)\n\n    # Return the path of the created CSV file\n    return os.path.abspath(filename)\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef task_func(directory):\n    \"\"\"\n    Organize files in directory based first text not enclosed in brackets Move files to subdirectories named after text no matching text found file not moved. Parameters: directory (str): directory path. Returns: tuple: - str: directory path with organized files. - dict: dictionary keys created subdirectories values lists of files moved. Requirements: - re - os - shutil - datetime Example import tempfile >>> temp_dir = tempfile.mkdtemp() >>> create_test_directory(temp_dir {\"file1.txt \"subdir1[content \"file2.txt \"subdir1[content \"file3.txt \"subdir2[content]\"}) >>> dir, files = task_func(temp_dir) >>> files['subdir2'][0].startswith('file3_') True\n    \"\"\"\n    # Create a dictionary to store the subdirectories and their corresponding files\n    subdirectories = {}\n\n    # Iterate over all files in the directory\n    for filename in os.listdir(directory):\n        filepath = os.path.join(directory, filename)\n\n        # Check if the file is a regular file\n        if os.path.isfile(filepath):\n            # Extract the text not enclosed in brackets\n            text = re.sub(r'\\[.*?\\]', '', filename)\n\n            # Create a subdirectory if it doesn't exist\n            if text not in subdirectories:\n                subdirectories[text] = []\n\n            # Move the file to the subdirectory\n            new_filepath = os.path.join(directory, text, filename)\n            os.makedirs(os.path.dirname(new_filepath), exist_ok=True)\n            shutil.move(filepath, new_filepath)\n            subdirectories[text].append(new_filepath)\n\n    # Return the directory path and the dictionary of subdirectories\n    return directory, subdirectories\n\n"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\n\ndef task_func(file_list):\n    \"\"\"\n    Run files from list as subprocesses time. Parameters: - file_list (list str): List files name to run. Returns: list: exit codes of subprocesses. Requirements: - subprocess - time - threading Example: >>> task_func([_data/file1.bat\",/file2.bat\"]) [0, 0]\n    \"\"\"\n    exit_codes = []\n    for file in file_list:\n        process = subprocess.Popen(file, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        exit_codes.append(process.wait())\n        time.sleep(1)\n    return exit_codes\n\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Find run all .bat files in directory returning file names exit codes. Parameters: directory_path (str): path directory to search for .bat files. Returns: list of tuples: list each tuple contains file name exit code exit code is None if file not executed. Requirements: - subprocess - os - sys - glob Example: >>> task_func(\"path/to/directory\") [(\"file1.bat\", 0), (\"file2.bat\", 1)]\n    \"\"\"\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n    results = []\n    for bat_file in bat_files:\n        try:\n            exit_code = subprocess.run([bat_file], capture_output=True, text=True).returncode\n            results.append((bat_file, exit_code))\n        except Exception as e:\n            results.append((bat_file, None))\n    return results\n"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    function takes pandas DataFrame column name input generates two subplots in one matplotlib figure first subplot is histogram kernel density estimate for numerical data), second box plot representing distribution of values in specified column. Parameters: df (DataFrame): Input DataFrame with numerical or categorical data. col (str): name of column to be plotted. column should exist in DataFrame contain numerical or categorical data. Returns: matplotlib.figure.Figure: matplotlib figure object containing histogram box plot. Requirements: - pandas - seaborn - matplotlib.pyplot Raises input df must be DataFrame, not empty contain specified column if not function raise ValueError. Example: >>> df = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}) >>> fig = task_func(df, 'value') >>> type(fig) <class 'matplotlib.figure.Figure'> >>> plt.close() >>> df = pd.DataFrame({'category': ['A', 'B', 'A', 'B', 'A 'B]}) >>> fig = task_func(df, 'category') >>> type(fig) <class 'matplotlib.figure.Figure'> >>> len(fig.axes) 2 >>> plt.close()\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"input df must be DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if col not in df.columns:\n        raise ValueError(\"column does not exist in DataFrame\")\n    if not pd.api.types.is_numeric_dtype(df[col]) and not pd.api.types.is_categorical_dtype(df[col]):\n        raise ValueError(\"column does not contain numerical or categorical data\")\n    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n    if pd.api.types.is_numeric_dtype(df[col]):\n        sns.histplot(df[col], kde=True, ax=axs[0])\n        axs[0].set_title('Histogram with KDE')\n    else:\n        sns.countplot(df[col], ax=axs[0])\n        axs[0].set_title('Count Plot')\n    sns.boxplot(df[col], ax=axs[1])\n    axs[1].set_title('Box Plot')\n    plt.tight_layout()\n    return fig\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run Python script process with predefined arguments. default waits for process complete If wait False function returns None. Parameters: script_path (str): path Python script run. wait (bool): wait for script complete Default is True. *args: arguments passed to script. Returns: int: return code of subprocess. If 'wait' False returns None. Requirements: - subprocess - os - sys - time Raise ValueError: If script not exist subprocess.CalledProcessError: If script raises exception. Example: >>> import tempfile >>> script_path = tempfile.NamedTemporaryFile(suffix='.py').name >>> open(script_path, 'w') as f: ... _ = f.write('import sys;sys.exit(0);') >>> task_func(script_path, True, 'arg1', 'arg2') 0 >>> task_func(script_path, False, 'arg1', 'arg2') # return None\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script {script_path} not exist\")\n    try:\n        process = subprocess.Popen(\n            [sys.executable, script_path] + list(args),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        if wait:\n            stdout, stderr = process.communicate()\n            return process.returncode\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        raise e\n\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Load data from Excel spreadsheet (.xlsx), calculate mean standard deviation each column draw bar chart bar chart returned as matplotlib figure object. Parameters: file_location (str): path to Excel file. sheet_name (str): name of sheet to load data from. Returns: - dict: dictionary with mean standard deviation of each column. matplotlib.figure.Figure: figure object containing bar chart figure titled 'Mean and Standard Deviation' X-axis labeled 'Columns' Y-axis labeled 'Values'. Raises: - FileNotFoundError: If Excel file not exist at specified path. ValueError: If specified sheet not exist in workbook. Requirements: - pandas - numpy - matplotlib.pyplot - os - openpyxl Example: >>> file_path='test.xlsx' >>> create_dummy_excel(file_path) >>> result, fig = task_func(file_path, 'TestSheet') >>> os.remove(file_path) >>> fig.axes[0].get_title() 'Mean and Standard Deviation'\n    \"\"\"\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File not found at {file_location}\")\n    except ValueError:\n        raise ValueError(f\"Sheet {sheet_name} not found in workbook\")\n    \n    mean_std = df.std().to_dict()\n    \n    fig, ax = plt.subplots()\n    ax.bar(df.columns, df.std())\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    \n    return mean_std, fig\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef task_func(activities):\n    \"\"\"\n    Return bar chart number of activities performed each day week based provided list activities If activities not datetime objects raise TypeError. Parameters: - activities (list of datetime objects): list of datetime objects representing when each activity occurred. Returns matplotlib.axes.Axes: Axes object representing bar chart x-axis labeled 'Day of the Week' y-axis 'Number of Activities' title 'Weekly Activity'. Requirements: - datetime - collections - matplotlib.pyplot Raises TypeError: If activities not datetime objects. Example: >>> ax = task_func([datetime(2023, 10, 25), datetime(2023, 10, 26)]) >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n\n    # Create a dictionary to store the number of activities for each day of the week\n    activities_per_day = defaultdict(int)\n\n    # Count the number of activities for each day of the week\n    for activity in activities:\n        activities_per_day[activity.strftime(\"%A\")] += 1\n\n    # Create a bar chart\n    ax = plt.bar(activities_per_day.keys(), activities_per_day.values())\n\n    # Label the x-axis and y-axis\n    plt.xlabel(\"Day of the Week\")\n    plt.ylabel(\"Number of Activities\")\n\n    # Set the title of the chart\n    plt.title(\"Weekly Activity\")\n\n    return ax\n\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves random file from source directory to specified destination directory. Parameters: src_dir (str): path source directory file randomly selected moved dest_dir (str): path destination directory file moved seed (int, Optional): seed for random number generator. Defaults to 100. Returns: str: name file moved. Format: 'filename.extension' (e.g. 'file1.txt'). Requirements: - os - shutil - random Examples: >>> import tempfile >>> src_dir = tempfile.mkdtemp() >>> dest_dir = tempfile.mkdtemp() >>> open(os.path.join(src_dir, 'file1.txt'), 'w').close() >>> open(os.path.join(src_dir, 'file2.txt'), 'w').close() >>> task_func(src_dir, dest_dir, seed=1) 'file2.txt'\n    \"\"\"\n    random.seed(seed)\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    if not files:\n        raise ValueError(\"Source directory is empty\")\n    file_to_move = random.choice(files)\n    shutil.move(os.path.join(src_dir, file_to_move), dest_dir)\n    return file_to_move\n\nThe function `task_func` is designed to move a random file from a source directory to a destination directory. It takes three parameters: `src_dir` (the path to the source directory), `dest_dir` (the path to the destination directory), and `seed` (an optional integer seed for the random number generator).\n\nHere's a step-by-step explanation of the function:\n\n1. The function sets the seed for the random number generator using the provided `seed` value. This ensures that the random selection of the file is reproducible.\n\n2. It then lists all files in the source directory using `os.listdir(src_dir)`. It filters out any directories and only keeps the files.\n\n3. If there are no files in the source directory, the function raises a `ValueError` with the message \"Source directory is empty\".\n\n4. It randomly selects one of the files using `random.choice(files)`.\n\n5. It moves the selected file from the source directory to the destination directory using `shutil.move()`. The full path to the file is constructed using `os.path.join(src_dir, file_to_move)`.\n\n6. Finally, it returns the name of the file that was moved, including its extension. The name is returned in the format 'filename.extension' (e.g., 'file1.txt').\n\nThe function is designed to be used with temporary directories created using `tempfile.mkdtemp()`. Here's an example of how you might use the function:\n\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects double quotes Excel (.xlsx) files specified directory prefixing with double backslash. Parameters: directory_path (str): path to directory containing Excel files Default './xlsx_files/'. Returns int: number of Excel files processed. Requirements Libraries: re, openpyxl, glob - Excel files specified directory. Example import tempfile >>> temp_dir = tempfile.mkdtemp() >>> workbook = Workbook() >>> sheet = workbook.active >>> sheet.append(['This \"test\" string']) >>> workbook.save(temp_dir + '/test.xlsx') >>> task_func(temp_dir) 1\n    \"\"\"\n    # Get all Excel files in the specified directory\n    excel_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Initialize a counter for the number of files processed\n    num_files_processed = 0\n\n    # Iterate over each Excel file\n    for file_path in excel_files:\n        # Load the workbook\n        workbook = load_workbook(file_path)\n\n        # Iterate over each sheet in the workbook\n        for sheet_name in workbook.sheetnames:\n            # Get the sheet\n            sheet = workbook[sheet_name]\n\n            # Iterate over each cell in the sheet\n            for row in sheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value contains double quotes\n                    if '\"' in str(cell.value):\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\"\"', str(cell.value))\n\n        # Save the workbook\n        workbook.save(file_path)\n\n        # Increment the counter\n        num_files_processed += 1\n\n    # Return the number of files processed\n    return num_files_processed\n\n"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Create diagram of sine wave cosine wave with given frequency return plot. Parameters: frequency (float): frequency wave Must non-negative float. sample_size (int, Optional): positive integer integer denoting number of samples taken for wave. Default is 10000. Returns: matplotlib.figure.Figure: figure object containing plot. matplotlib.axes.Axes: axes object of plot. Requirements: - numpy for data generation matplotlib.pyplot for plotting math for mathematical constants Raises: - ValueError: If frequency negative ValueError: If sample size non-positive. Example: >>> fig, ax = task_func(1, 2500) >>> type(fig) <class 'matplotlib.figure.Figure'> >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive\")\n    t = np.linspace(0, 2 * math.pi, sample_size)\n    sine_wave = np.sin(frequency * t)\n    cosine_wave = np.cos(frequency * t)\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine wave')\n    ax.plot(t, cosine_wave, label='Cosine wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Frequency: {frequency}')\n    ax.legend()\n    return fig, ax\n\n"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    Initializes Flask-Mail instance sending emails using generated Flask application with specified app_name. Parameters: app_name (string): Flask application name Returns: tuple: tuple containing Flask-Mail instance app's mail configurations. details email server retrieved from environment variables. If variables not exist use defaults: 'MAIL_SERVER': 'localhost' 'MAIL_PORT': 25 'MAIL_USE_TLS': False (boolean) 'MAIL_USERNAME': None 'MAIL_PASSWORD': None Requirements: - flask - os - flask_mail Example: >>> mail, configs = task_func(\"test\") >>> print(mail.__getattribute__(\"app\").name) test\n    \"\"\"\n    app = Flask(app_name)\n    mail = Mail(app)\n    configs = {\n        \"MAIL_SERVER\": os.environ.get(\"MAIL_SERVER\", \"localhost\"),\n        \"MAIL_PORT\": int(os.environ.get(\"MAIL_PORT\", 25)),\n        \"MAIL_USE_TLS\": bool(os.environ.get(\"MAIL_USE_TLS\", False)),\n        \"MAIL_USERNAME\": os.environ.get(\"MAIL_USERNAME\", None),\n        \"MAIL_PASSWORD\": os.environ.get(\"MAIL_PASSWORD\", None),\n    }\n    app.config.update(configs)\n    return mail, configs\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculate mean median standard deviation of data from specific column in Excel file. Parameters: excel_file_path (str): path to directory containing Excel file. file_name (str): name of Excel file column_name (str): name column to analyze. Returns: dict: dictionary with mean median standard deviation. Raises FileNotFoundError: If Excel file exist at specified path ValueError: If specified column not found in Excel file. Requirements: - pandas - numpy - os Example: >>> data_dir, file_name = './data/', 'excel_file1.xls' >>> create_dummy_file(data_dir, file_name) >>> stats = task_func(data_dir, file_name, 'Sales') >>> os.remove(data_dir + file_name) >>> os.rmdir(data_dir) >>> print(stats) {'mean': 200.0 'median': 200.0 'std_dev': 70.71067811865476}\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(excel_file_path + file_name):\n        raise FileNotFoundError(f\"File {file_name} not found at {excel_file_path}\")\n\n    # Read Excel file\n    df = pd.read_excel(excel_file_path + file_name)\n\n    # Check if column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in {file_name}\")\n\n    # Calculate mean, median, standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    return {'mean': mean, 'median': median, 'std_dev': std_dev}\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    Trains neural network on input data target labels. function Splits data into training set (75%) test set (25%), assuming input dimension always 2. Constructs Sequential model with one dense hidden layer sigmoid activation function Compiles model using binary cross-entropy loss SGD optimizer with specified learning rate Fits model to training data (without verbose output), evaluating on test set as validation data Plots model's training validation loss over epochs returns plot's Axes object for customization. Parameters: X (np.ndarray): Input features for model each feature set input dimension of 2. Y (np.ndarray): Target labels for model. Returns: - Sequential: trained Keras Sequential model. matplotlib.axes.Axes: Axes object of plot. plot visualizes model's training validation loss over epochs x-axis representing epochs y-axis representing loss legend distinguishes between 'Train' and 'Test' losses. Notes input dimension of X must always be 2. Axes title is 'Model loss' x-axis label is 'Epoch' y-axis label is 'Loss' Requirements: - keras.layers.Dense - keras.optimizers.SGD - keras.models.Sequential - sklearn.model_selection.train_test_split - matplotlib.pyplot Examples: >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) >>> Y = np.array([[0], [1], [1], [0]]) >>> model, ax = task_func(X, Y) >>> isinstance(model, Sequential) True >>> isinstance(ax, plt.Axes) True\n    \"\"\"\n    # Split data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct Sequential model\n    model = Sequential()\n    model.add(Dense(10, input_dim=2, activation='sigmoid'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.1), metrics=['accuracy'])\n\n    # Fit model to training data\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=4, verbose=0, validation_data=(X_test, Y_test))\n\n    # Plot model's training and validation loss over epochs\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    \"\"\"\n    Divide input data into training test sets (70% training 30% test), create Keras Sequential model with one hidden layer sigmoid activation function compile model with binary cross-entropy loss SGD optimizer specifying learning rate fit model to training data in non-verbose mode plot ROC curve for model on test set including AUC score in plot legend. Parameters: X (np.ndarray): input data input dimension always 2. Y (np.ndarray): target data. Returns: keras.models.Sequential: trained Keras model. matplotlib.axes._axes.Axes: matplotlib Axes object for Precision-Recall curve plot. Notes title of axes 'ROC curve' x label is 'False positive rate' y label 'True positive rate' Requirements: - tensorflow.keras - sklearn.metrics.roc_curve - sklearn.metrics.auc - sklearn.model_selection.train_test_split - matplotlib Example: >>> X = np.array([[0, 0], [0, 1], [1 1]]) >>> Y = np.array([[0], [1], [1], [1]]) >>> model, ax = task_func(X, Y) >>> isinstance(model, keras.models.Sequential) True\n    \"\"\"\n    # Divide input data into training test sets (70% training 30% test)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create Keras Sequential model with one hidden layer sigmoid activation function\n    model = keras.Sequential([\n        keras.layers.Dense(1, input_dim=2, activation='sigmoid')\n    ])\n\n    # Compile model with binary cross-entropy loss SGD optimizer specifying learning rate\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=['accuracy'])\n\n    # Fit model to training data in non-verbose mode\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Plot ROC curve for model on test set including AUC score in plot legend\n    fpr, tpr, _ = roc_curve(Y_test, model.predict(X_test))\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    ax = plt.gca()\n\n    return model, ax\n"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads RGB image applies K-means clustering to segment image into 'n_clusters' regions saves each region as separate image. function returns numpy arrays of original and segmented images. Parameters: image_path (str): path to RGB image file. Default is 'image.jpg'. image expected be in RGB format as 3D array (height x width x channels), channels in order of RGB. n_clusters (int): number of clusters for K-means clustering. Default is 3. minimum 1 cluster allowed clustering with single cluster original image as segmented image. random_seed (int): seed for random number generator in K-means clustering. Default is 42. Returns: - tuple: tuple containing two numpy arrays. first array represents original RGB image second array represents segmented image each pixel's color replaced by centroid of cluster. Raises: FileNotFoundError: If image file not exist at specified path. ValueError: If 'n_clusters' not positive integer. Requirements: opencv: For reading image file converting BGR to RGB. - numpy: For array manipulations. os: For checking existence of image file. sklearn.cluster: For applying K-means clustering. Example: >>> create_dummy_image('image.jpg') >>> original_img_array, segmented_img_array = task_func('image.jpg', 3) >>> os.remove('image.jpg') >>> print(original_img_array.shape) # Example output (10, 10, 3) >>> print(segmented_img_array.shape) # Example output for n_clusters > 1 (10, 10 3) function assumes input image is in RGB format.- segmented image array same shape as original image but with pixel colors replaced by corresponding cluster centroid colors, effectively segmenting image into regions based on color similarity - Clustering with single cluster is allowed and return original image as both original and segmented images, since all pixels assigned to same cluster.\n    \"\"\"\n    # Check if image file exist\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"Image file not found at path: {image_path}\")\n\n    # Check if n_clusters is positive integer\n    if not isinstance(n_clusters, int) or n_clusters < 1:\n        raise ValueError(\"'n_clusters' must be a positive integer\")\n\n    # Read image\n    img = cv2.imread(image_path)\n\n    # Convert BGR to RGB\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Reshape image to 2D array\n    img_2d = img_rgb.reshape((-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed).fit(img_2d)\n\n    # Get cluster centroids\n    centroids = kmeans.cluster_centers_\n\n    # Get labels for each pixel\n    labels = kmeans.labels_\n\n    # Reshape labels to original image shape\n    labels_2d = labels.reshape(img_rgb.shape[:2])\n\n    # Create segmented image\n    segmented_img = centroids[labels]\n\n    # Reshape segmented image to original image shape\n    segmented_img_rgb = segmented_img.reshape(img_rgb.shape)\n\n    # Convert RGB to BGR\n    segmented_img_bgr = cv2.cvtColor(segmented_img_rgb, cv2.COLOR_RGB2BGR)\n\n    # Save segmented image\n    segmented_img_path = 'segmented_image.jpg'\n    cv2.imwrite(segmented_img_path, segmented_img_bgr)\n\n    # Return original and segmented images\n    return img_rgb, segmented_img_rgb\n"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate product matrix 'P 3D tensor 'T' flatten result apply KMeans clustering flattened data visualize. Parameters: P (numpy.ndarray): input matrix T (numpy.ndarray): input tensor shape (3, 3, 3). n_clusters number clusters KMeans clustering Default 3. random_state (int): random state KMeans clustering Default 0. n_init Number time k-means algorithm run different centroid seeds Default 10. Returns cluster_result (numpy.ndarray): result KMeans clustering. ax (matplotlib.axes.Axes): visualization KMeans clustering title 'KMeans Clustering Visualization'. Requirements: - numpy - sklearn - matplotlib Example P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]]) >>> T = np.random.rand(3, 3, 3) >>> cluster_result, ax = task_func(P, T, n_clusters=3, random_state=0, n_init=10) >>> type(cluster_result) <class 'numpy.ndarray'> >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Flatten the tensor T\n    T_flat = T.flatten()\n\n    # Apply KMeans clustering to the flattened data\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(T_flat.reshape(-1, 1))\n\n    # Get the cluster labels for each data point\n    cluster_result = kmeans.labels_\n\n    # Visualize the KMeans clustering\n    ax = plt.figure().add_subplot(111, projection='3d')\n    ax.scatter(T_flat, np.zeros_like(T_flat), cluster_result, c=cluster_result)\n    ax.set_title('KMeans Clustering Visualization')\n\n    return cluster_result, ax\n\nThe function `task_func` is designed to perform KMeans clustering on a 3D tensor `T` after flattening it. The function takes the following parameters:\n\n- `P`: A numpy array representing the input matrix.\n- `T`: A numpy array representing the input tensor with a shape of (3, 3, 3).\n- `n_clusters`: The number of clusters for the KMeans algorithm. Default is 3.\n- `random_state`: The random state for the KMeans algorithm. Default is 0.\n- `n_init`: The number of times the k-means algorithm will be run with different centroid seeds. Default is 10.\n\nThe function returns a tuple containing:\n\n- `cluster_result`: A numpy array representing the result of the KMeans clustering, which is the cluster label for each data point in the flattened tensor `T`.\n- `ax`: A matplotlib axes object representing the visualization of the KMeans clustering.\n\nThe function first flattens the tensor `T` using the `flatten` method. Then, it applies the KMeans clustering algorithm to the flattened data using the `fit` method of the `KMeans` class from the `sklearn.cluster` module. The cluster labels for each data point are obtained using the `labels_` attribute of the `kmeans` object.\n\nFinally, the function visualizes the KMeans clustering using a 3D scatter plot. The x-axis represents the flattened data, the y-axis is set to zero, and the color represents the cluster label. The title of the plot is set to 'KMeans Clustering Visualization'.\n\nHere is an example usage of the function:\n\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate Voronoi diagram for points in 2D plot function raise errors when input invalid wrong type or shape Jittering applied prior to plotting. Parameters: points (np.ndarray): numpy ndarray of shape (n_points, 2) with coordinates points. seed (int): Random seed for reproducibility Defaults to 0. Returns: tuple (vor, ax): tuple containing vor (Voronoi): Voronoi object representing Voronoi diagram points. ax (Axes): axes of plotted Voronoi diagram. Requirements: - numpy - scipy - matplotlib.pyplot Example >>> points = np.array([[0, 0], [0, 1], [1, 0 1]]) >>> vor, ax = task_func(points) >>> type(vor) <class 'scipy.spatial.qhull.Voronoi'> >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Check if points is a numpy array\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"points must be a numpy ndarray\")\n    # Check if points has shape (n_points, 2)\n    if points.shape[1] != 2:\n        raise ValueError(\"points must have shape (n_points, 2)\")\n    # Jittering applied prior to plotting\n    points = points + np.random.normal(0, 0.1, points.shape)\n    # Calculate Voronoi diagram\n    vor = Voronoi(points, qhull_options='QJ')\n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    plt.show()\n    return vor, ax\n"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with specified extension from source to destination directory function searches for files source directory match given extension. If file same name exists in destination directory not moved. Parameters: - src_dir (str): source directory path. - dest_dir (str): destination directory path. ext (str): file extension to search for (without leading dot). Returns: - list: list of full paths of files successfully moved. If file not moved because already exists in destination directory not included in list. Raises FileNotFoundError: if source or destination directory exist Requirements: - os - shutil - glob Examples: >>> test_src_dir = './test_src' >>> test_dest_dir = './test_dest' >>> test_ext = 'txt' >>> os.makedirs(test_src_dir, exist_ok=True) >>> os.makedirs(test_dest_dir, exist_ok=True) >>> moved_files = task_func(test_src_dir, test_dest_dir, test_ext) >>> len(moved_files) > 0 Check if files were moved True >>> 'test_file.txt' in [os.path.basename(path) for path in moved_files] Assuming test_file.txt exists in test_src_dir True >>> os.listdir(test_dest_dir) Verify files moved no duplicates exist in destination ['test_file.txt']\n    \"\"\"\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n\n    src_files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n    moved_files = []\n\n    for src_file in src_files:\n        dest_file = os.path.join(dest_dir, os.path.basename(src_file))\n        if not os.path.exists(dest_file):\n            shutil.move(src_file, dest_dir)\n            moved_files.append(src_file)\n\n    return moved_files\n\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\n\ndef task_func(json_str):\n    \"\"\"\n    Load JSON string into dictionary normalize dictionary by doubling numerical values create Pandas DataFrame from dictionary. function processes JSON string by converting it into dictionary, normalizes data by doubling numerical values constructs Pandas DataFrame from dictionary. function designed to handle simple flat dictionaries with values single numerical values, lists of numerical values or strings interpreted as numbers. doubles values of numerical data types within dictionary, including within lists and in strings (which extracted using regex), function not process nested dictionaries., returns DataFrame with numerical values stored as floats other types left as-is, or empty DataFrame if input JSON string empty or not contain valid data structures for DataFrame conversion. Parameters: json_str (str): JSON string. Returns: DataFrame: pandas DataFrame created from dictionary. Requirements: - pandas - json - re Example: >>> json_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}' >>> df = task_func(json_str) >>> type(df) <class 'pandas.core.frame.DataFrame'> >>> print(df) a b c 0 2 9.8 10 1 4 9.8 10 2 6 9.8 10\n    \"\"\"\n    # Load JSON string into dictionary\n    data = json.loads(json_str)\n\n    # Normalize dictionary by doubling numerical values\n    def double_numerical_values(value):\n        if isinstance(value, (int, float)):\n            return value * 2\n        elif isinstance(value, str):\n            try:\n                return float(value) * 2\n            except ValueError:\n                return value\n        elif isinstance(value, list):\n            return [double_numerical_values(v) for v in value]\n        else:\n            return value\n\n    data = {k: double_numerical_values(v) for k, v in data.items()}\n\n    # Create Pandas DataFrame from dictionary\n    df = pd.DataFrame.from_dict(data, orient='index', columns=['value'])\n\n    return df\n\nThe task description is quite detailed and provides a clear understanding of what the function is supposed to do. However, there are a few issues with the description that need to be addressed:\n\n1. The function name `task_func` is not very descriptive. It would be better to use a name that clearly indicates what the function does, such as `normalize_and_convert_json_to_dataframe`.\n\n2. The description mentions that the function is designed to handle simple flat dictionaries with values of single numerical values, lists of numerical values, or strings interpreted as numbers. However, the function does not handle strings interpreted as numbers. The function only doubles numerical values, including those in strings, but it does not convert strings to numbers.\n\n3. The description mentions that the function does not process nested dictionaries. However, the function does not handle nested dictionaries at all. If the input JSON string contains a nested dictionary, the function will raise an error.\n\n4. The description mentions that the function returns a DataFrame with numerical values stored as floats and other types left as-is. However, the function does not explicitly convert numerical values to floats. It only doubles numerical values, which are already floats.\n\n5. The description mentions that the function returns an empty DataFrame if the input JSON string is empty or does not contain valid data structures for DataFrame conversion. However, the function does not explicitly check for these conditions. It assumes that the input JSON string is always valid.\n\nHere is a revised version of the task description that addresses these issues:\n\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    Executes given bash script returns CPU memory usage of script's process. function checks script path exists executes in subprocess uses psutil to monitor script's process for CPU memory usage. Note CPU usage is cumulative measure of script process's CPU demand over execution period not average across cores. Memory usage reported as sum of RSS memory increments. function aggregates metrics until script completes or specified timeout reached. handles cases where process becomes zombie or not found ensures subprocess terminated if runs beyond timeout. Parameters: script_path (str): path to bash script to be executed. Path must exist. timeout (int, optional): Maximum time (in seconds) function should wait for script to complete. Defaults to 10 seconds. Returns: dict: dictionary containing - 'CPU Usage': accumulated CPU usage in percentage. - 'Memory Usage': accumulated memory usage in bytes. Requirements: - subprocess - psutil - time - os Examples: >>> resources = task_func('/path/to/script.sh') >>> resources {'CPU Usage': 5.2, 'Memory Usage': 2048}\n    \"\"\"\n    # Check if script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script path '{script_path}' does not exist.\")\n\n    # Initialize variables\n    start_time = time.time()\n    cpu_usage = 0\n    memory_usage = 0\n    process = None\n\n    try:\n        # Execute script in subprocess\n        process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        # Monitor script's process for CPU memory usage\n        while process.poll() is None:\n            # Check if timeout reached\n            if time.time() - start_time > timeout:\n                raise TimeoutError(f\"Script '{script_path}' did not complete within {timeout} seconds.\")\n\n            # Get process info\n            process_info = psutil.Process(process.pid)\n\n            # Update CPU memory usage\n            cpu_usage += process_info.cpu_percent(interval=1)\n            memory_usage += process_info.memory_info().rss\n\n            # Check if process becomes zombie\n            if process_info.status() == psutil.STATUS_ZOMBIE:\n                raise RuntimeError(f\"Script '{script_path}' process became zombie.\")\n\n            # Check if process not found\n            if not psutil.pid_exists(process.pid):\n                raise RuntimeError(f\"Script '{script_path}' process not found.\")\n\n            # Sleep for a while\n            time.sleep(0.1)\n\n    except Exception as e:\n        # Terminate subprocess if it is still running\n        if process is not None and process.poll() is None:\n            process.terminate()\n\n        # Raise exception\n        raise e\n\n    finally:\n        # Terminate subprocess if it is still running\n        if process is not None and process.poll() is None:\n            process.terminate()\n\n    # Return results\n    return {'CPU Usage': cpu_usage, 'Memory Usage': memory_usage}\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    Create DataFrame with number rows (N) 3 columns \"x\" \"y\" random values \"category\" random categories from CATEGORIES list Each category guaranteed appear once if N greater than or equal to number categories otherwise randomly sampled without replacement from CATEGORIES draw scatter plot of \"x\" vs \"y\" colored by \"category\". Parameters N (int optional): Number rows DataFrame Defaults to 100. CATEGORIES (list optional): List categories Defaults to ['A' 'B' 'C' 'D' 'E']. seed (int optional): Random seed for reproducibility Defaults to 42. Returns tuple tuple containing DataFrame: generated DataFrame Axes: Axes object of scatter plot. Requirements: - numpy - pandas - matplotlib.pyplot Example >>> df, ax = task_func() >>> df.head() x y category 0 0.239562 0.385098 C 1 0.144895 0.851137 D 2 0.489453 0.316922 C 3 0.985650 0.169493 E 4 0.242055 0.556801 A >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    np.random.seed(seed)\n    df = pd.DataFrame({\n        \"x\": np.random.rand(N),\n        \"y\": np.random.rand(N),\n        \"category\": np.random.choice(CATEGORIES, N, replace=N >= len(CATEGORIES))\n    })\n    ax = df.plot.scatter(x=\"x\", y=\"y\", c=\"category\", cmap=\"viridis\", legend=True)\n    return df, ax\n\nHere is the Python code that implements the `task_func` function:\n\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generate time series from given epoch start to end time with specified step trend time series plotted with timestamps x-axis ('Time') values y-axis ('Value'). values generated from normal distribution linear trend added based on provided trend value. Parameters: start_time (int): start epoch time in milliseconds end_time (int): end epoch time in milliseconds Must be greater than start_time. step (int): step in milliseconds between each data point Must agreater than 0. trend (float): trend value added to time series acts as multiplier for index adding linear trend to randomly generated values seed (int optional): Seed for reproducibility. Default is 42. Returns ax (matplotlib.pyplot.Axes): Axes object of generated plot x-axis labeled 'Time' y-axis labeled 'Value'. Requirements: datetime.datetime - pandas - numpy Example >>> ax = task_func(0, 10000, 100, 0.001) >>> type(ax) <class 'matplotlib.axes._axes.Axes'> >>> ax.get_xticklabels() [Text(-20.0, 0, '1970-01-01 10:00:08.000000'),(0.0 '1970-01-01 10:00:00.000000 Text(20.0 '1970-01-01 10:00:02.000000 Text(40.0, 0 '1970-01-01 10:00:04.000000 Text(60.0, 0 '1970-01-01 10:00:06.000000(80.0, 0 '1970-01-01 10:00:08.000000 Text(100.0, 0 Text(120.0, 0, '')]\n    \"\"\"\n    # Generate time series\n    time = np.arange(start_time, end_time, step)\n    values = np.random.normal(size=len(time))\n    values = values * trend + np.arange(len(time)) * trend\n\n    # Create dataframe\n    df = pd.DataFrame({'Time': time, 'Value': values})\n\n    # Convert time to datetime\n    df['Time'] = pd.to_datetime(df['Time'], unit='ms')\n\n    # Plot time series\n    ax = df.plot(x='Time', y='Value', kind='line')\n\n    # Set x-axis label\n    ax.set_xlabel('Time')\n\n    # Set y-axis label\n    ax.set_ylabel('Value')\n\n    return ax\n\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generate sales data five products given epoch time to current time function checks input validity each day between date given epoch time to current time generates random sales data for each 5 products. Parameters: epoch_milliseconds (int): Start epoch time in milliseconds before current system time random_seed (int): Seed for reproducibility random sales data Defaults to 0. products (list str): Product list choose contain 5 unique strings Defaults to ['Product1'']. Returns pd.DataFrame: DataFrame containing sales data with columns 'Product' (string), 'Date' 'Sales' (integer). Sales quantity randomly sampled from range [10, 50]. Requirements: - pandas - datetime.datetime - random Example sales_data = task_func(1236472051807, random_seed=42) >>> type(sales_data) <class 'pandas.core.frame.DataFrame'> >>> sales_data.head() Product Date Sales 0 Product4 2009-03-08 11:27:31.807 50 1 Product5 2009-03-08 11:27:31.807 17 2 Product1 2009-03-08 11:27:31.807 11 3 Product3 2009-03-08 11:27:31.807 27 4 Product2 2009-03-08 11:27:31.807 25\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int):\n        raise TypeError(\"epoch_milliseconds must be an integer\")\n    if not isinstance(random_seed, int):\n        raise TypeError(\"random_seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"products must be a list of 5 unique strings\")\n    if not all(isinstance(product, str) for product in products):\n        raise ValueError(\"All elements in products must be strings\")\n    if not all(len(set(products)) == len(products)) or not all(\n        len(set(products)) == len(products)\n    ):\n        raise ValueError(\"All elements in products must be unique\")\n\n    # Generate sales data\n    random.seed(random_seed)\n    current_time = datetime.now()\n    start_time = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    date_range = pd.date_range(start=start_time, end=current_time, freq=\"D\")\n    sales_data = []\n    for date in date_range:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append([product, date, sales])\n    sales_data = pd.DataFrame(\n        sales_data, columns=[\"Product\", \"Date\", \"Sales\"]\n    ).sort_values(by=\"Date\")\n    return sales_data\n\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert JSON strings to Excel file handling empty JSON arrays function takes JSON string converts into Excel file with specified filename. If JSON string represents empty array function creates Excel file with no data rows. Parameters: json_str (str, bytes, bytearray): JSON content as string bytes bytearray. filename (str): name of Excel file created. sheet_name (str, optional): name of sheet in Excel file Default is \"sheet1\". Returns: str: absolute path of created Excel file. Raises ValueError: If `json_str` not valid JSON TypeError: If `json_str not string, bytes or bytearray. Exception: For other general errors related to file writing. Requirements: xlwt: For writing to Excel files. xlrd - os: For file path operations. pandas: For data manipulation. Example: >>> json_str = '[{\"Name: \"John\", \"Age 30}, {\"Name \"Jane\", \"Age 28}]' >>> True if task_func(json_str, 'data.xls').endswith('data.xls') else False # True True >>> os.remove('data.xls')\n    \"\"\"\n    # Check if json_str is valid JSON\n    try:\n        json.loads(json_str)\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON string\")\n\n    # Check if json_str represents empty array\n    if json_str.strip() == \"[]\":\n        # Create empty Excel file\n        workbook = xlwt.Workbook()\n        sheet = workbook.add_sheet(sheet_name)\n        workbook.save(filename)\n        return os.path.abspath(filename)\n\n    # Convert JSON string to DataFrame\n    df = pd.read_json(json_str)\n\n    # Write DataFrame to Excel file\n    df.to_excel(filename, sheet_name=sheet_name, index=False)\n\n    return os.path.abspath(filename)\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates graph daily activity durations for specified days past using randomly generated data activities. function randomly generates acitivity durations 0 to 120 for each activity [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\". Parameters: days_in_past (int optional): number of days past generate graph. Defaults to 7 days Must be past. random_seed (int optional): Seed for random number generation reproducibility. Defaults to 0. Returns Tuple containing - ax (matplotlib.pyplot.Axes): DataFrame for plotting. - df (pd.DataFrame): Seaborn lineplot with date x-axis, duration y-axis activity hue. Requirements: - datetime.datetime - datetime.timedelta - pandas - random - seaborn Example: >>> ax, df = task_func(7, random_seed=42) >>> type(ax) <class 'matplotlib.axes._axes.Axes'> sample row from returned DataFrame: Date Activity Duration YYYY-MM-DD Running 45\n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Generate list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate list of dates\n    dates = [datetime.now() - timedelta(days=i) for i in range(days_in_past)]\n\n    # Generate list of durations\n    durations = [random.randint(0, 120) for _ in range(len(activities) * days_in_past)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Date\": dates,\n        \"Activity\": [activities[i % len(activities)] for i in range(len(durations))],\n        \"Duration\": durations\n    })\n\n    # Create lineplot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n\n    return ax, df\n\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Create DataFrame of stock prices specified days past using random data. Parameters: days_in_past (int optional): number days past want stock data. Must be positive. Defaults to 7. stock_names (list str optional): list stock names want data. not empty. Defaults to [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"]. random_seed (int optional): seed for random number generation reproducibility. Defaults to 0. Returns DataFrame: pandas DataFrame containing random stock prices specified number days. Prices floats in [0.0,1.0). Requirements: - datetime.datetime - pandas - numpy Example: >>> df = task_func(5, random_seed=42) >>> type(df) <class 'pandas.core.frame.DataFrame'> >>> print(df.head(1)) AAPL GOOGL MSFT AMZN FB 2024-03-30 37.454012 95.071431 73.199394 59.865848 15.601864\n    \"\"\"\n    if days_in_past <= 0:\n        raise ValueError(\"days_in_past must be positive\")\n    if not stock_names:\n        raise ValueError(\"stock_names must not be empty\")\n    np.random.seed(random_seed)\n    dates = pd.date_range(\n        start=datetime.now() - pd.Timedelta(days=days_in_past), periods=days_in_past\n    )\n    data = np.random.rand(days_in_past, len(stock_names))\n    df = pd.DataFrame(data, index=dates, columns=stock_names)\n    return df\n\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files create difference report. function compares two CSV files line by line provides detailed report of differences represents each difference with line number status indicator content of line. Parameters: file_path1 (str): file path first CSV file. file_path2 (str): file path second CSV file. delimiter (str optional): Delimiter character in CSV files Default is ','. quotechar (str optional): Quote character CSV files Default is '\"'. Returns: DataFrame: pandas DataFrame with differences. DataFrame contains columns: 'Line Number': line number file where difference occurs. 'Status': character indicating type of difference: - ' ': No change (line same in both files). '-': Line present in first file not in second. '+': Line present in second file not first. 'Content': actual text content of line from either file. Raises: FileNotFoundError: If either files found. ValueError: If either files empty. Exception: For other IO related errors. Requirements: - pandas: For data manipulation analysis. - csv: For reading CSV files. difflib: For performing difference operation. - os Example: >>> create_dummy_test_files() >>> df = task_func('file1.csv', 'file2.csv') >>> os.remove('file1.csv') >>> os.remove('file2.csv') >>> df.head() Line Number Status Content 0 1 ('name', 'age') 1 2 - ('Alice', '30') 2 3 + ('Alice', '31') 3 4 ('Bob', '25')\n    \"\"\"\n    try:\n        # Read CSV files into pandas dataframes\n        df1 = pd.read_csv(file_path1, delimiter=delimiter, quotechar=quotechar)\n        df2 = pd.read_csv(file_path2, delimiter=delimiter, quotechar=quotechar)\n\n        # Check if either files are empty\n        if df1.empty or df2.empty:\n            raise ValueError(\"One or both files are empty\")\n\n        # Initialize list to store differences\n        differences = []\n\n        # Compare each line in both files\n        for i in range(max(len(df1), len(df2))):\n            try:\n                line1 = tuple(df1.iloc[i])\n            except IndexError:\n                line1 = None\n            try:\n                line2 = tuple(df2.iloc[i])\n            except IndexError:\n                line2 = None\n\n            # Check if lines are the same\n            if line1 == line2:\n                differences.append((i+1, ' ', line1))\n            elif line1 is not None and line2 is None:\n                differences.append((i+1, '-', line1))\n            elif line1 is None and line2 is not None:\n                differences.append((i+1, '+', line2))\n\n        # Create DataFrame from differences\n        df = pd.DataFrame(differences, columns=['Line Number', 'Status', 'Content'])\n\n        return df\n\n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both files not found\")\n    except Exception as e:\n        raise Exception(\"Error occurred: \" + str(e))\n\n"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze employee data calculate statistics for given column. If data list empty sum 0 mean min max values NaN. function visualizes data with pie chart using Age column as labels. Parameters: column (str): column to analyze Valid values are 'Age' 'Salary' 'Experience' If invalid function KeyError. data (list of lists): employee data each list represents [Age, Salary Experience]. Returns: tuple: tuple containing dict: dictionary with 'sum' 'mean' 'min' 'max' of column. Axes object: pie chart visualizing column data. Requirements: - pandas - numpy - matplotlib.pyplot Example: >>> data = [[25, 50000, 2], [30, 75000, 5], [35, 100000, 7], [40, 125000, 10], [45, 150000, 12]] >>> stats, ax = task_func('Salary', data) >>> stats {'sum': 500000, 'mean': 100000.0 'min': 50000 'max': 150000} >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if column not in ['Age', 'Salary', 'Experience']:\n        raise KeyError('Invalid column name')\n    \n    df = pd.DataFrame(data, columns=['Age', 'Salary', 'Experience'])\n    \n    if column == 'Age':\n        labels = df['Age'].unique()\n    elif column == 'Salary':\n        labels = df['Salary'].unique()\n    elif column == 'Experience':\n        labels = df['Experience'].unique()\n    \n    values = df[column].value_counts().values\n    total = df[column].sum()\n    mean = df[column].mean()\n    min_val = df[column].min()\n    max_val = df[column].max()\n    \n    stats = {'sum': total, 'mean': mean, 'min': min_val, 'max': max_val}\n    \n    ax = plt.pie(values, labels=labels, autopct='%1.1f%%')\n    \n    return stats, ax\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze list fitness data calculate sum mean minimum maximum of column draw line chart validate numeric values for steps calories burned distance walked non-negative. Parameters: column (str): column to analyze from data. allowed columns are 'Date', 'Steps' 'Calories Burned' 'Distance Walked'. data (list of list): list each inner list contains datetime object representing date followed by numeric values for steps calories burned distance walked in order. Each numeric value must be non-negative. not empty. Returns: tuple: tuple containing - dict: dictionary with sum, mean min max of column. - matplotlib.axes.Axes: Axes object of plotted line chart. line chart Date on x-axis column value on y-axis title Line Chart of (column). Requirements: - pandas - numpy - matplotlib.pyplot Raises: KeyError: If specified column not valid ValueError: If data list empty or numeric values for steps calories burned distance walked negative. Example: >>> data = [[datetime(2022, 1, 1), 5000, 200, 3.5], ... [datetime(2022, 1, 2), 5500, 220, 4.0], ... [datetime(2022, 1, 3), 6000, 240, 4.5]] >>> stats, ax = task_func('Steps', data) >>> type(ax) <class 'matplotlib.axes._axes.Axes'> >>> print(stats) {'sum': 16500, 'mean': 5500.0, 'min': 5000, 'max': 6000}\n    \"\"\"\n    if column not in ['Date', 'Steps', 'Calories Burned', 'Distance Walked']:\n        raise KeyError(\"Invalid column name\")\n    if not data:\n        raise ValueError(\"Data list is empty\")\n    for row in data:\n        for value in row[1:]:\n            if not isinstance(value, (int, float)) or value < 0:\n                raise ValueError(\"Numeric values for steps, calories burned, distance walked must be non-negative\")\n    df = pd.DataFrame(data, columns=['Date', 'Steps', 'Calories Burned', 'Distance Walked'])\n    df['Date'] = pd.to_datetime(df['Date'])\n    stats = {'sum': df[column].sum(), 'mean': df[column].mean(), 'min': df[column].min(), 'max': df[column].max()}\n    ax = df.plot(x='Date', y=column, title=f'Line Chart of {column}')\n    return stats, ax\n\nThe function `task_func` is designed to analyze a list of fitness data and calculate the sum, mean, minimum, and maximum of a specified column. It also validates the numeric values for steps, calories burned, and distance walked to ensure they are non-negative. The function uses the pandas library to create a DataFrame from the input data, and then calculates the required statistics. It also plots a line chart of the specified column against the date using matplotlib.\n\nHere is an example of how to use the function:\n\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    Read list dictionaries from JSON file calculate mean median for each key (ignoring non-numeric or missing values), convert results into Pandas DataFrame. Parameters: - input_file (str, optional): input JSON file name. Defaults to 'data.json'. file should contain list dictionaries If key missing in dictionary treated as NaN for record. Non-numeric values ignored for calculation mean median If all values for key non-numeric or missing, statistics for key be NaN. Returns: - df (pd.DataFrame): DataFrame indexed sorted by variable names (keys) from input data containing columns 'mean' and 'median'. Requirements: - numpy - collections - json - pandas Example: >>> df = task_func('data_1.json') a mean median b mean median c mean median\n    \"\"\"\n    with open(input_file, \"r\") as f:\n        data = json.load(f)\n    data = [d for d in data if isinstance(d, dict)]\n    keys = set().union(*data)\n    stats = defaultdict(list)\n    for key in keys:\n        values = [d.get(key) for d in data]\n        values = [v for v in values if isinstance(v, (int, float))]\n        if values:\n            stats[key].append(np.mean(values))\n            stats[key].append(np.median(values))\n        else:\n            stats[key].append(np.nan)\n            stats[key].append(np.nan)\n    df = pd.DataFrame(stats)\n    df = df.T\n    df = df.sort_index()\n    df.columns = [\"mean\", \"median\"]\n    return df\n\n"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(file_path):\n    \"\"\"\n    Identifies duplicate rows from CSV file csv library convert rows into pandas DataFrame plot using matplotlib. Parameters: file_path (str): path to CSV file. Returns dict: dictionary with duplicate rows as keys counts as values. Axes: matplotlib Axes object with bar chart of duplicate rows. Requirements: csv - collections.Counter - pandas - matplotlib.pyplot Example: >>> duplicates, ax = task_func(\"sample_data.csv\") >>> duplicates {('Alice', '25', 'New York'): 3, ('Bob', '30', 'London'): 2} >>> type(ax) <class 'matplotlib.axes._axes.Axes'> Ensure CSV file in proper format .csv extension Other file formats raise ValueError.\n    \"\"\"\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"File must be in .csv format\")\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n        duplicates = Counter(tuple(row) for row in rows)\n        duplicates = {k: v for k, v in duplicates.items() if v > 1}\n        df = pd.DataFrame(duplicates.items(), columns=['Row', 'Count'])\n        ax = df.plot(x='Row', y='Count', kind='bar')\n        return duplicates, ax\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    Identify duplicate entries in DataFrame record age distribution for duplicate names function takes DataFrame with 'name' 'age' columns If age provided as floats rounded down to nearest integer Age not negative otherwise function raises ValueError function identifies duplicate names records age distribution returns Counter object with age distribution histogram plot showing distribution ages for duplicate names age on x-axis count on y-axis. Bins calculated based on minimum maximum ages among duplicates adjusted by .5 to ensure integer ages fall squarely within bins. Parameters: df: pd.DataFrame - DataFrame with columns 'name' 'age' not be empty If empty function raises ValueError. Returns: Counter: Age distribution among duplicate names. plt.Axes or None: Histogram plot displaying age distribution or None if no duplicates. Requirements: - pandas - numpy - collections.Counter - seaborn - matplotlib.pyplot Raises - ValueError: If DataFrame empty or if age negative. Example: >>> df = pd.DataFrame({'name': ['Alice', 'Bob', 'Alice'], 'age': [25, 26, 25]}) >>> duplicates_counter, ax = task_func(df) >>> duplicates_counter Counter({25: 2}) >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if not all(df['age'].apply(lambda x: isinstance(x, int) or isinstance(x, float))):\n        raise ValueError(\"Age must be a number\")\n    if any(df['age'] < 0):\n        raise ValueError(\"Age must not be negative\")\n    duplicates = df[df.duplicated(subset=['name'], keep=False)]\n    if duplicates.empty:\n        return Counter(), None\n    min_age = duplicates['age'].min()\n    max_age = duplicates['age'].max()\n    bins = np.arange(min_age - 0.5, max_age + 1.5)\n    age_counts = duplicates['age'].value_counts().sort_index()\n    plt.figure(figsize=(10, 6))\n    ax = sns.histplot(age_counts, bins=bins, kde=False)\n    ax.set_xlabel('Age')\n    ax.set_ylabel('Count')\n    ax.set_title('Age distribution among duplicate names')\n    return age_counts, ax\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, bins=4):\n    \"\"\"\n    Identify count duplicate values in DataFrame's 'value' column function plots histogram for all values overlays normal distribution curve on histogram. Parameters: df (pd.DataFrame): DataFrame containing numeric 'value' column. If empty function return empty Counter empty plot. bins (int optional): Number of bins for histogram. Defaults to 4. Returns: tuple: tuple containing - Counter: Counter object with count of each duplicate value Axes: matplotlib.axes.Axes object represents plot histogram with 'value' column data. If applicable normal distribution curve fitted to data overlaid. histogram's bars green with 60% opacity normal distribution curve black linewidth 2. plot titled \"Distribution\", \"Value\" x-axis label \"Frequency\" y-axis label. Requirements: - collections.Counter - numpy - scipy.stats.norm - matplotlib.pyplot Example: >>> df = pd.DataFrame({'value': [1, 2, 2, 3, 3, 4, 3, 2, 1, 4, 4, 4, 2, 2, 3, 1, 1, 1, 3, 2]}) >>> counter, ax = task_func(df) >>> ax <Axes: title={'center': 'Distribution'}, xlabel='Value', ylabel='Frequency'> >>> counter Counter({2: 6, 1: 5, 3: 5, 4: 4})\n    \"\"\"\n    if df.empty:\n        return Counter(), plt.plot()\n\n    counter = Counter(df['value'])\n    values = np.array(list(counter.keys()))\n    frequencies = np.array(list(counter.values()))\n\n    fig, ax = plt.subplots()\n    ax.bar(values, frequencies, color='green', alpha=0.6)\n    ax.set_title(\"Distribution\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n\n    mean = np.mean(values)\n    std_dev = np.std(values)\n    x = np.linspace(min(values), max(values), 100)\n    y = norm.pdf(x, mean, std_dev)\n    ax.plot(x, y, color='black', linewidth=2)\n\n    return counter, ax\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\n\ndef task_func(a, b):\n    \"\"\"\n    Generate pandas DataFrame with random values based on lists 'a' and 'b' plot as bar chart List 'a' sets row indices length of list 'b' determines number of columns using predefined names from 'COLUMNS = ['A', 'B' 'C' 'D', 'E']' list. Parameters: - a (list): list define number of rows. - b (list): Another list define number of columns. actual column names predefined. Returns: - matplotlib.axes.Axes: Axes object of plotted bar chart. Requirements: - numpy - pandas - matplotlib Data Structure: Uses pandas DataFrame to structure data. Example: >>> ax = task_func([1, 2, 3], ['A', 'B', 'C', 'D', 'E'])\n    \"\"\"\n    # Create DataFrame\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=b)\n\n    # Plot bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n\n    return ax\n\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(data):\n    \"\"\"\n    function plots bar chart monthly data values single year 'month' x-axis 'value' y-axis. Parameters: data (str): data string format 'yyyy-mm-value'. Returns: Axes object: matplotlib.axes.Axes object representing plot title formatted 'Monthly Data for yyyy' x label 'Month' y label 'Value'. Requirements: - pandas - datetime - matplotlib.pyplot Example >>> data = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650' >>> ax = task_func(data)\n    \"\"\"\n    # Convert data string to pandas DataFrame\n    df = pd.DataFrame([x.split('-') for x in data.split(',')], columns=['year', 'month', 'value'])\n    df['month'] = pd.to_datetime(df['month'], format='%m')\n    df['value'] = pd.to_numeric(df['value'])\n\n    # Plot data\n    ax = df.plot(x='month', y='value', kind='bar', title=f'Monthly Data for {df[\"year\"].iloc[0]}', xlabel='Month', ylabel='Value')\n\n    return ax\n\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    function draws histogram visualize frequency distribution of numeric values string format with 'Value' x-axis 'Frequency' y-axis 'Histogram of Values' title. Parameters: data (str): data string format 'value-value-value-...'. Returns: ax (matplotlib.axes._axes.Axes): Axes object of created histogram. Requirements: - pandas - numpy - matplotlib.pyplot Notes histogram uses bins calculated as `np.arange(data.min(), data.max()+2) - 0.5`. Example: >>> data = '1-2-3-4-5-6-7-8-9-10' >>> ax = task_func(data)\n    \"\"\"\n    data = pd.Series(data.split('-')).astype(float)\n    bins = np.arange(data.min(), data.max()+2) - 0.5\n    ax = data.hist(bins=bins)\n    ax.set_title('Histogram of Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    return ax\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    Create noisy sine wave specified length adjusts curve using curve_fit from scipy.optimize to data. Parameters: array_length (int): Length sine wave array Defaults to 100. noise_level (float): Level noise added to sine wave Defaults to 0.2. Returns: Axes object: plot showing noisy sine wave adjusted curve. Requirements: - numpy - scipy.optimize - matplotlib.pyplot Example: >>> ax = task_func(100, 0.2)\n    \"\"\"\n    # Create noisy sine wave\n    x = np.linspace(0, 2*np.pi, array_length)\n    y = np.sin(x) + noise_level * np.random.normal(size=array_length)\n\n    # Fit curve to data\n    popt, pcov = curve_fit(lambda x, a, b, c: a * np.sin(b * x) + c, x, y)\n\n    # Plot data and fitted curve\n    fig, ax = plt.subplots()\n    ax.plot(x, y, 'o', label='Noisy data')\n    ax.plot(x, popt[0] * np.sin(popt[1] * x) + popt[2], label='Fitted curve')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Noisy sine wave adjusted curve')\n    ax.legend()\n\n    return ax\n\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    Reads CSV file normalizes text to ASCII counts words returns 10 most common words with frequencies as matplotlib bar plot and list of tuples. Parameters: csv_file (str): path to CSV file. Returns: tuple: tuple containing matplotlib.axes.Axes object for bar plot and list of 10 most common words with frequencies. Raises: FileNotFoundError: If CSV file be found at specified path. IOError: If error in reading file. Requirements: - unicodedata - csv - collections - matplotlib.pyplot Example: >>> create_dummy_csv_file('dummy.csv') >>> ax, most_common_words = task_func('dummy.csv') >>> os.remove('dummy.csv') >>> type(ax) <class 'matplotlib.axes._axes.Axes'> >>> type(most_common_words) <class 'list'> Note: function assumes CSV file contains text data file properly formatted.\n    \"\"\"\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            data = [row for row in reader]\n    except FileNotFoundError:\n        print(f\"File '{csv_file}' not found.\")\n        return None\n    except IOError:\n        print(f\"Error reading file '{csv_file}'.\")\n        return None\n\n    text = ' '.join(data)\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n    words = text.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(10)\n\n    fig, ax = plt.subplots()\n    ax.bar(range(len(most_common_words)), [count for word, count in most_common_words], tick_label=[word for word, count in most_common_words])\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('10 Most Common Words')\n\n    return ax, most_common_words\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef task_func(size=1000):\n    \"\"\"\n    Create list normally distributed random numbers plot histogram probability density function (PDF). Parameters: size (int): number of random numbers generate Default 1000. Requirements: - numpy - scipy.stats - matplotlib.pyplot Returns: matplotlib.figure.Figure: figure object containing histogram PDF plot. Example >>> import matplotlib >>> fig = task_func(size=500) >>> isinstance(fig, matplotlib.figure.Figure) Check if output is matplotlib figure object True >>> len(fig.axes[0].lines) == 1 Ensure one line plot on axes for PDF True >>> len(fig.axes[0].patches) > 10 Check if histogram bars (patches) present True\n    \"\"\"\n    # Generate normally distributed random numbers\n    random_numbers = np.random.normal(size=size)\n\n    # Create histogram\n    hist, bins = np.histogram(random_numbers, bins=30, density=True)\n\n    # Calculate bin centers\n    bin_centers = (bins[:-1] + bins[1:]) / 2\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot histogram PDF\n    ax.plot(bin_centers, hist, label='PDF')\n\n    # Set plot title and labels\n    ax.set_title('Histogram PDF')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Probability Density')\n\n    # Show legend\n    ax.legend()\n\n    # Return figure object\n    return fig\n\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    \"\"\"\n    Generates RSA public private key pair saves private key in file after encrypting with password using AES encryption. Returns public key filename of encrypted private key encryption details for testing. output file format \"private_key_<number>.txt\", <number> is hex representation of 8 randomly generated bytes. Returns: rsa.PublicKey: RSA public key. str: filename where encrypted private key stored. bytes: encryption password for testing decryption. bytes: encryption nonce for testing decryption. Requirements: - rsa - os - Crypto.Cipher.AES - Crypto.Random.get_random_bytes - base64.b64encode - base54.b64decode Examples: >>> pub_key, filename, password, nonce = task_func() >>> isinstance(pub_key, rsa.PublicKey) True >>> isinstance(filename, str) True >>> isinstance(password, bytes) True >>> isinstance(nonce, bytes) True\n    \"\"\"\n    # Generate RSA public private key pair\n    key = rsa.generate_key(2048)\n\n    # Generate random password for AES encryption\n    password = get_random_bytes(16)\n\n    # Generate random nonce for AES encryption\n    nonce = get_random_bytes(16)\n\n    # Encrypt private key with password using AES encryption\n    cipher = AES.new(password, AES.MODE_EAX, nonce)\n    encrypted_private_key = cipher.encrypt(key.export_key())\n\n    # Save encrypted private key to file\n    filename = f\"private_key_{b64encode(get_random_bytes(8)).decode()}.txt\"\n    with open(filename, \"wb\") as f:\n        f.write(cipher.nonce)\n        f.write(encrypted_private_key)\n\n    return key, filename, password, nonce\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    \"\"\"\n    Generates RSA public private keys encrypts file using AES encryption encrypts AES key with public RSA key. encrypted file and AES key saved in separate new files. method demonstrates hybrid encryption approach. Parameters: file_path (str): path to file to be encrypted. Returns: rsa.PublicKey: RSA public key. str: filename of encrypted file. str: filename file containing encrypted AES key. Requirements: - rsa - os - cryptography.hazmat.backends.default_backend - cryptography.hazmat.primitives.ciphers.Cipher -.ciphers.algorithms - cryptography.ciphers.modes - base64.b64encode Examples: >>> pub_key, encrypted_file, encrypted_key_file = task_func('my_file.txt') >>> isinstance(pub_key, rsa.PublicKey) True >>> encrypted_file.endswith('.encrypted') True >>> encrypted_key_file.endswith('.encrypted') True\n    \"\"\"\n    # Generate RSA public private keys\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n        backend=default_backend()\n    )\n    pub_key = key.public_key()\n\n    # Encrypt file using AES encryption\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Generate a random AES key\n    aes_key = os.urandom(32)\n\n    # Create AES cipher object\n    cipher = Cipher(algorithms.AES(aes_key), modes.CFB(b'0' * 16), backend=default_backend())\n    encryptor = cipher.encryptor()\n\n    # Pad the data\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n\n    # Encrypt the data\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Save the encrypted data to a new file\n    encrypted_file = file_path + '.encrypted'\n    with open(encrypted_file, 'wb') as f:\n        f.write(encrypted_data)\n\n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n\n    # Save the encrypted AES key to a new file\n    encrypted_key_file = file_path + '.key.encrypted'\n    with open(encrypted_key_file, 'wb') as f:\n        f.write(encrypted_aes_key)\n\n    return pub_key, encrypted_file, encrypted_key_file\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    \"\"\"\n    Extracts text href attributes of anchor tags from given URL's HTML content returns data in pandas DataFrame with time of data extraction. Parameters: url (str): URL from to fetch HTML content. Returns: pandas.DataFrame: DataFrame with columns 'text', 'href' 'fetch_time'. Each row corresponds to anchor tag in HTML 'text' and 'href' containing text and hyperlink reference of anchor tag,. 'fetch_time' contains timestamp of data fetched in format 'YYYY-MM-DD HH:MM:SS'. Raises: ValueError: If provided URL invalid or empty. URLError: If issue with network connectivity or server. Requirements: - urllib.request - pyquery - datime - pandas - urllib.error Example: >>> df = task_func('https://en.wikibooks.org/wiki/Main_Page') Note function requires internet connectivity to fetch HTML content.\n    \"\"\"\n    if not url:\n        raise ValueError(\"URL cannot be empty\")\n    try:\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n        doc = pq(html_content)\n        links = doc('a')\n        data = []\n        for link in links:\n            text = link.text\n            href = link.attrib.get('href')\n            data.append({'text': text, 'href': href})\n        df = pd.DataFrame(data)\n        df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        return df\n    except urllib.error.URLError as e:\n        raise e\n    except Exception as e:\n        raise e\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Create sensor data for specified number hours save in CSV file with coloumns 'Time' 'Temperature' 'Humidity' 'Pressure'. Parameters: - hours (int): number of hours for sensor data generated output_dir (str, optional): output file path Returns: - hours (int): Number of hours to generate data for. Requirements: - datetime - os - random - csv Example: >>> file_path = task_func(1) Generate data for 1 hour >>> os.path.exists(file_path) Check if file created True >>> isinstance(file_path, str) Validate return type is string True >>> 'sensor_data.csv' in file_path Ensure filename correct True\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate sensor data\n    sensor_data = []\n    for _ in range(hours * 60):\n        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        temperature = randint(20, 30)\n        humidity = randint(40, 60)\n        pressure = randint(900, 1100)\n        sensor_data.append([time, temperature, humidity, pressure])\n\n    # Save sensor data to CSV file\n    file_path = os.path.join(output_dir, 'sensor_data.csv')\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Temperature', 'Humidity', 'Pressure'])\n        writer.writerows(sensor_data)\n\n    return file_path\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates traffic data for vehicle types over specified hours saves data to CSV file with coloumns 'Time' 'Car' 'Bus' 'Truck' 'Bike' plots data in line chart with 'Time' x-axis 'Vehicle Count' y-axis. Parameters: hours (int): Number hours generate data output_dir (str optional): output file path Returns tuple: Path to CSV file matplotlib axes object line plot. Requirements: - pandas - os - csv - matplotlib.pyplot - random - datetime Example import matplotlib >>> file_path, ax = task_func(2) # Generate data for 2 hours >>> isinstance(file_path, str) True >>> 'traffic_data.csv' in file_path True >>> isinstance(ax, matplotlib.axes.Axes) True\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    for hour in range(hours):\n        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        car_count = randint(0, 100)\n        bus_count = randint(0, 50)\n        truck_count = randint(0, 30)\n        bike_count = randint(0, 200)\n        data.append([time, car_count, bus_count, truck_count, bike_count])\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Car', 'Bus', 'Truck', 'Bike'])\n        writer.writerows(data)\n\n    # Plot data\n    df = pd.read_csv(file_path)\n    ax = df.plot(x='Time', y=['Car', 'Bus', 'Truck', 'Bike'], kind='line')\n    ax.set_title('Traffic Data')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n\n    return file_path, ax\n\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generate weather data specified number hours save in CSV file with colomns 'Time' 'Condition' back up file to backup directory. Parameters: - hours (int): number of hours weather data generated output_dir (str, optional): output file path Returns: - str: path of generated CSV file. Requirements: - datetime - os - random - csv - shutil Example: >>> 'weather_data.csv' in task_func(24) True >>> 'weather_data.csv' in task_func(10) True\n    \"\"\"\n    # Create output directory if not exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    for i in range(hours):\n        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        weather_data.append([time, condition])\n\n    # Save weather data to CSV file\n    output_file = os.path.join(output_dir, 'weather_data.csv')\n    with open(output_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Time', 'Condition'])\n        writer.writerows(weather_data)\n\n    # Back up file to backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    backup_file = os.path.join(backup_dir, f'weather_data_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv')\n    shutil.copy(output_file, backup_file)\n\n    return output_file\n\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generate visualize Pandas DataFrame results football matches multiple teams 'Team' with random goals 'Goals' penalties 'Penalty Cost' Penalties converted into fines according to penalty costs. Parameters: goals (int): maximum number goals team score match. penalties (int): maximum number penalties team receive match. Returns: pd.DataFrame: dataframe containing match results. list: list containing two seaborn plot objects (Axes) for goals penalty costs. Requirements: - pandas - seaborn - matplotlib.pyplot - random Example: >>> df, plots = task_func(5, 3)\n    \"\"\"\n    # Generate random goals and penalties for each team\n    goals = [randint(0, goals) for _ in range(len(TEAMS))]\n    penalties = [randint(0, penalties) for _ in range(len(TEAMS))]\n\n    # Calculate penalty costs for each team\n    penalty_costs = [penalty * PENALTY_COST for penalty in penalties]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Team': TEAMS, 'Goals': goals, 'Penalty Cost': penalty_costs})\n\n    # Plot goals\n    goals_plot = sns.barplot(x='Team', y='Goals', data=df)\n    goals_plot.set_title('Goals per Team')\n\n    # Plot penalty costs\n    penalty_costs_plot = sns.barplot(x='Team', y='Penalty Cost', data=df)\n    penalty_costs_plot.set_title('Penalty Costs per Team')\n\n    return df, [goals_plot, penalty_costs_plot]\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\n\ndef task_func(rows):\n    \"\"\"\n    Create Pandas DataFrame with random integer values 0 9 for given number rows Count non-zero values in each column visualize information using bar plot. Parameters: rows (int): number of rows in DataFrame. Returns tuple: tuple containing elements - DataFrame: generated DataFrame with random integer values. - Axes: matplotlib Axes object containing bar plot. Requirements: - numpy - pandas - matplotlib.pyplot Example: >>> df, ax = task_func(10) >>> print(ax.title.get_text()) # return 'Non-Zero Value Counts' Non\n    \"\"\"\n    # Create DataFrame with random integer values\n    df = pd.DataFrame(np.random.randint(0, 10, size=(rows, len(COLUMNS))), columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.count()\n\n    # Create bar plot\n    ax = non_zero_counts.plot(kind='bar')\n\n    # Set title and labels\n    ax.set_title('Non-Zero Value Counts')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Count')\n\n    return df, ax\n\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\ndef task_func(num_students):\n    \"\"\"\n    Generate Pandas DataFrame displays grades randomly selected group students multiple courses Calculate average grade each course number students passing grade (>= 60), visualize information using bar plot title 'Course-wise Average and Passing Grade Counts'. Parameters: num_students (int): number of students in sample. Returns: Tuple[pd.DataFrame, plt.Axes]: tuple containing generated DataFrame bar plot's Axes object. Requirements: - pandas - numpy - matplotlib.pyplot - random - typing Example: >>> df, ax = task_func(50) >>> ax.get_title() 'Course-wise Average and Passing Grade Counts'\n    \"\"\"\n    # Generate random data\n    num_courses = 5\n    grades = np.random.randint(0, 101, size=(num_students, num_courses))\n    course_names = [f'Course {i+1}' for i in range(num_courses)]\n    student_names = [f'Student {i+1}' for i in range(num_students)]\n    df = pd.DataFrame(grades, columns=course_names, index=student_names)\n    \n    # Calculate average grade for each course\n    avg_grades = df.mean()\n    \n    # Calculate number of students passing grade (>= 60) for each course\n    passing_counts = (df >= 60).sum()\n    \n    # Create bar plot\n    fig, ax = plt.subplots()\n    ax.bar(avg_grades.index, avg_grades.values, label='Average Grade')\n    ax.bar(passing_counts.index, passing_counts.values, label='Passing Grade Count')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.set_xlabel('Course')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return df, ax\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fit exponential decay function to indices in array where first column matches target value Parameters: array (np.ndarray): numpy array first column searched for target value target_value (float or int): value first column filter data for fitting. Returns: tuple: optimized parameters of fitting function (popt) matplotlib Axes object. Requirements: - numpy - scipy.optimize - matplotlib.pyplot Example import numpy as np >>> array = np.array([[1, 2], [1, 3], [1, 4] [2, 5], [2, 6]]) >>> target = 1 >>> params, ax = task_func(array, target) >>> len(params) 3\n    \"\"\"\n    # Filter data for target value\n    filtered_data = array[array[:, 0] == target_value, 1]\n\n    # Define exponential decay function\n    def exp_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Initial guess for parameters\n    initial_guess = [1, 1, 1]\n\n    # Fit data to exponential decay function\n    popt, pcov = optimize.curve_fit(exp_decay, np.arange(len(filtered_data)), filtered_data, p0=initial_guess)\n\n    # Plot data and fitted function\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(len(filtered_data)), filtered_data, 'o', label='Data')\n    ax.plot(np.arange(len(filtered_data)), exp_decay(np.arange(len(filtered_data)), *popt), '-', label='Fit')\n    ax.legend()\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.set_title(f'Exponential Decay Fit for Target Value {target_value}')\n\n    return popt, ax\n\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    Performs topic extraction from text documents using Non-Negative Matrix Factorization (NMF). function preprocesses input texts by removing non-alphanumeric characters (excluding spaces), characters to lowercase removing stopwords vectorizes processed texts using TF-IDF applies NMF to extract specified number of topics. Each topic represented as list of significant words based on NMF component weights. Parameters: - texts (list of str): input text documents from to extract topics. num_topics (int): number of topics to extract. Returns: list of list of str: list where each element is list of words representing topic. Requirements: - re - nltk - sklearn.decomposition - sklearn.feature_extraction.text Example: texts = [ \"Data science involves study of data \"Machine learning provides systems learn from data \"Python is programming language in data science.\" ] >>> topics = task_func(texts, 2) >>> print(topics) [['data', 'science'] ['systems' 'provides']] exact output may vary depending on TF-IDF vectorization and NMF initialization.\n    \"\"\"\n    # Preprocess texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stopwords\n        text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n        processed_texts.append(text)\n\n    # Vectorize processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n\n    # Apply NMF to extract specified number of topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n\n    # Get the words for each topic\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n        topics.append(top_words)\n\n    return topics\n"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from texts using gensim Word2Vec model nltk.corpus.stopwords. texts cleaned removing non-alphanumeric characters except space, lowercased stop words removed. Parameters: texts (list): list of strings. stopwords (list optional): list stopwords removed If not provided nltk's stopwords used. Returns: Word2Vec: trained Word2Vec model. Requirements: - re - nltk - gensim Example: >>> texts = [\"Hello, World!\", \"Machine Learning is great\", \"Python favorite programming language\"] >>> model = task_func(texts) >>> vector = model.wv['python']\n    \"\"\"\n    # Remove non-alphanumeric characters except space\n    texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n    # Remove stopwords\n    if stopwords is None:\n        stopwords = nltk.corpus.stopwords.words('english')\n    texts = [' '.join([word for word in text.split() if word not in stopwords]) for text in texts]\n    # Train Word2Vec model\n    model = Word2Vec(texts, min_count=1)\n    return model\n\nimport re\nimport nltk\nfrom gensim.models import Word2Vec"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files in directory function reads each JSON file into DataFrame inserts \"Source\" column filename processed files moved to \"processed\" subdirectory path may not exist initially. Parameters: path (str): path directory containing JSON files. Returns df (pandas.DataFrame): DataFrame containing data from processed files. Requirements - pandas - json - os - shutil Example os.mkdir('data') open('data/a.json', 'w') f: f.write('[{\"a\": 1, \"b\": 2}\": 3, \"b\": 4}]') ... 36 open('data/b.json', 'w') f: f.write('[{\"a\": 5, \"b\": 6} {\"a\": 7, \"b\": 8}]') ... 36 >>> df = task_func('data') >>> print(df) a b source 0 5 6 b.json 1 7 8 b.json 0 1 2 a.json 1 3 4 a.json >>> shutil.rmtree('data')\n    \"\"\"\n    if not os.path.exists(path):\n        os.mkdir(path)\n    if not os.path.exists(os.path.join(path, 'processed')):\n        os.mkdir(os.path.join(path, 'processed'))\n    for filename in os.listdir(path):\n        if filename.endswith('.json'):\n            with open(os.path.join(path, filename), 'r') as f:\n                data = json.load(f)\n                df = pd.DataFrame(data)\n                df['source'] = filename\n                df.to_csv(os.path.join(path, 'processed', filename.replace('.json', '.csv')), index=False)\n                shutil.move(os.path.join(path, filename), os.path.join(path, 'processed', filename))\n    return df\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    \"\"\"\n    Create delete CSV file \"task_func_data/Output.txt\" with sensor data for temperature humidity data generated randomly written in append mode file deleted after use. Returns: Returns path to CSV file \"task_func_data/Output.txt before deletion. Requirements: - os - csv - random datatime Example: >>> task_func()\n    \"\"\"\n    # Create directory if it does not exist\n    if not os.path.exists(\"task_func_data\"):\n        os.makedirs(\"task_func_data\")\n\n    # Create CSV file\n    csv_file = \"task_func_data/Output.txt\"\n    with open(csv_file, \"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Time\", \"Temperature\", \"Humidity\"])\n\n    # Generate random sensor data and write to CSV file\n    for _ in range(10):\n        temperature = random.uniform(20, 30)\n        humidity = random.uniform(40, 60)\n        time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        with open(csv_file, \"a\", newline=\"\") as file:\n            writer = csv.writer(file)\n            writer.writerow([time, temperature, humidity])\n\n    # Delete CSV file after use\n    os.remove(csv_file)\n\n    return csv_file"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrape data from given URL save data to CSV file. Parameters: url (str): URL to scrape data. Returns: CSV_FILE_PATH (str): path of CSV file where scraped data saved. Requirements: - urllib - bs4 - csv - os Example: >>> task_func('http://www.example.com/') 'scraped_data.csv'\n    \"\"\"\n    # Send HTTP request to the URL\n    response = urllib.request.urlopen(url)\n    # Parse the HTML content\n    soup = BeautifulSoup(response, 'html.parser')\n    # Find all the links on the page\n    links = soup.find_all('a')\n    # Create a CSV file to save the data\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        # Write the header row\n        writer.writerow(['Link'])\n        # Write the data rows\n        for link in links:\n            writer.writerow([link.get('href')])\n\n    # Return the path of the CSV file\n    return CSV_FILE_PATH\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    \"\"\"\n    Train linear regression model return model score of test set DataFrame used as training data target_column as target in training model Before training data split into training and test set using test_size random_state parameters. Parameters: data (DataFrame): input data for training target_column (str): column to predict random_state (int): seed for train-test split Defaults to 0 test_size (float): fractional size of test set Defaults to 0.2 Returns float model's score. Raises ValueError: If data not DataFrame If data empty If target_column not column data data contains values not numeric random_state not integer test_size not between 0 and 1. Requirements: - pandas - sklearn.model_selection.train_test_split - sklearn.linear_model.LinearRegression - numpy Example: >>> rng = np.random.default_rng(seed=42) >>> data = pd.DataFrame({ ... 'x1': rng.random(100), 'x2': rng.random(100), 'y': rng.random(100) ... }) >>> result = task_func(data, 'y', random_state=2, test_size=0.3) >>> result -0.25486317198996633 >>> data = pd.DataFrame({ ... 'x1': rng.random(500), ... }) >>> data['y'] = data['x1'] * 2 + 1 >>> result = task_func(data 'y' random_state=9, test_size=0.1) >>> result 1.0\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a pandas DataFrame\")\n    if data.empty:\n        raise ValueError(\"data must not be empty\")\n    if not isinstance(target_column, str):\n        raise ValueError(\"target_column must be a string\")\n    if target_column not in data.columns:\n        raise ValueError(\"target_column must be a column in data\")\n    if not np.issubdtype(data[target_column].dtype, np.number):\n        raise ValueError(\"data[target_column] must contain numeric values\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"random_state must be an integer\")\n    if not (0 <= test_size <= 1):\n        raise ValueError(\"test_size must be between 0 and 1\")\n    \n    X_train, X_test, y_train, y_test = train_test_split(data.drop(target_column, axis=1), data[target_column], test_size=test_size, random_state=random_state)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    return model.score(X_test, y_test)\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \"\"\"\n    Creates random DataFrame with 100 records Each record consists ID (ranging from 1 to 100), Name (randomly selected from lists Latin and other names), Date of Birth (randomly generated dates between specified years), Email (constructed using name year of birth email domain). Improperly encoded Latin characters in names corrected during process. Parameters: start_year (int): starting year for birth years Defaults to 1980. end_year (int): ending year for birth years Defaults to 2000. email_domain (str): domain for email addresses Defaults to 'example.com'. latin_names (list str): list of Latin names in generation. Defaults to latin=['Sopet\u00f3n' 'M\u00e9ndez' 'G\u00f3mez' 'P\u00e9rez' 'Mu\u00f1oz'] other_names (list str): list other names in generation Defaults to other='Smith' 'Johnson 'Williams' 'Brown' 'Jones'] rng_seed (int): seed for rng. Returns DataFrame: pandas DataFrame containing generated user data. has columns: 'ID' 'Name' 'Date of Birth' 'Email'. Requirements: - pandas - numpy - codecs - re - datetime Examples: >>> df = task_func(rng_seed=1) >>> print(df) ID Name Date of Birth Email 0 1 Brown 1992-09-10 brown1992@example.com 1 2 Smith 1996-02-13 smith1996@example.com 2 3 Jones 1986-10-19 jones1986@example.com 3 4 G\u00f3mez 2000-12-11 g\u00f3mez2000@example.com 4 5 G\u00f3mez 1984-08-24 g\u00f3mez1984@example.95 96 Johnson 1990-09-17@example 97 Brown 1992-10-14 brown1992@example.com 98 Mu\u00f1oz 1998-05-04 mu\u00f1oz1998@example 99 Mu\u00f1oz 1982-01-01 mu\u00f1oz1982@example.com 100 Jones 1990-03-28 jones1990@example.com <BLANKLINE [100 rows 4 columns df task_func(start_year=0 end_year=1200 email_domain.at rng_seed=3) print(df) ID Name Date Birth Email Sopet\u00f3n 0952-09-01 00:00:00 sopet\u00f3n952@test.at Brown 0875-10-10 00:00:00 brown875@test.at Sopet\u00f3n 0605-08-15 00:00:00 sopet\u00f3n605@test.at G\u00f3mez 0337-11-23 00:00:00 g\u00f3mez337@test.at G\u00f3mez 0641-04-27 00:00:00 g\u00f3mez641@test.at 95 96 Brown 0044-05-17 00:00:00 brown44@test.at 96 97 Williams 0530-01-21 00:00:00 williams530@test.at 98 Johnson 1005-12-15 00:00:00 johnson1005@test.at 99 M\u00e9ndez 1134-07-19 00:00:00 m\u00e9ndez1134@test.at 100 Johnson 0696-08-22 00:00:00 johnson696@test.at <BLANKLINE [100 4 columns\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Create an empty DataFrame\n    df = pd.DataFrame(columns=['ID', 'Name', 'Date of Birth', 'Email'])\n    \n    # Generate random IDs\n    df['ID'] = np.arange(1, 101)\n    \n    # Generate random names\n    df['Name'] = np.random.choice(latin_names + other_names, size=100)\n    \n    # Generate random dates of birth\n    df['Date of Birth'] = pd.to_datetime(np.random.randint(start_year, end_year+1, size=100), format='%Y')\n    \n    # Generate random emails\n    df['Email'] = df['Name'] + str(df['Date of Birth'].dt.year) + '@' + email_domain\n    \n    # Correct improperly encoded Latin characters in names\n    df['Name'] = df['Name'].apply(lambda x: codecs.decode(x, 'unicode_escape'))\n    \n    # Return the DataFrame\n    return df\n"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    Read list dictionaries from JSON file calculate mean median for each key write results to CSV file. Parameters: input_file (str): input JSON file name. output_file (str): output CSV file name. Returns: - dict: dictionary where each key is field from input JSON each value is another dictionary with mean median of field. Requirements: - numpy - collections - json - csv Example: >>> task_func('data.json', 'stats.csv')\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize dictionary to store mean and median values\n    stats = defaultdict(lambda: {'mean': [], 'median': []})\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key-value pair in the dictionary\n        for key, value in d.items():\n            # Append the value to the list for the key\n            stats[key]['mean'].append(value)\n            stats[key]['median'].append(value)\n\n    # Calculate mean and median for each key\n    for key in stats:\n        stats[key]['mean'] = np.mean(stats[key]['mean'])\n        stats[key]['median'] = np.median(stats[key]['median'])\n\n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Field', 'Mean', 'Median'])\n        for key in stats:\n            writer.writerow([key, stats[key]['mean'], stats[key]['median']])\n\n    return stats\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from dictionary checking if file exists if has content copies to target directory. Parameters: - kwargs (dict): dictionary keys are full file paths values are file content target_dir (str, optional): directory files copied to. Defaults to 'non_none_files'. Returns: copied_files (list): list of full file paths copied. Requirements: - os - pathlib.Path - shutil Example: >>> files = {'/path/to/file1.txt': 'Hello',/file2.txt': None/file3.txt': 'World'} >>> task_func(files) >>> files = {'/path/to/file4.txt': 'Another', '/path/to/file5.txt': 'Example'} >>> task_func(files, target_dir=\"another_directory\")\n    \"\"\"\n    copied_files = []\n    for file_path, file_content in kwargs.items():\n        if file_content is not None:\n            if os.path.exists(file_path):\n                if os.path.getsize(file_path) > 0:\n                    target_path = Path(target_dir) / Path(file_path).name\n                    shutil.copy(file_path, target_path)\n                    copied_files.append(str(target_path))\n    return copied_files\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    \"\"\"\n    Processes CSV files in directory based specified pattern creates new files with altered names preserving content laid out solid foundation with initial tests. Parameters: - directory (str): path to directory containing CSV files to be processed pattern (str, optional): regular expression pattern filenames interest should match. Returns: - new_files (list): list of strings each string is filename of new CSV file created by function. Requirements: - re - os - pathlib - csv Example: >>> task_func(\"/mnt/data/test_data\") ['Shan.csv', 'Shannon.csv']\n    \"\"\"\n    new_files = []\n    for file in os.listdir(directory):\n        if re.match(pattern, file):\n            with open(os.path.join(directory, file), 'r') as f:\n                reader = csv.reader(f)\n                data = list(reader)\n            new_file_name = re.sub(r'-\\d+\\.csv$', '.csv', file)\n            with open(os.path.join(directory, new_file_name), 'w', newline='') as f:\n                writer = csv.writer(f)\n                writer.writerows(data)\n            new_files.append(new_file_name)\n    return new_files\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzip all zip files in directory name matches pattern by splitting filename last time \"-\" occurs using prefix part filename as directory to extract. Parameters: - directory (str): directory where zip files located pattern (str): Regex pattern match zip files. Returns: - list: list of directories where files extracted. Requirements: - os - re - zipfile Example: >>> task_func('/tmp/my_data') ('/tmp/backup/backup_20230827010101', [])\n    \"\"\"\n    # Get all zip files in directory\n    zip_files = [f for f in os.listdir(directory) if re.match(pattern, f)]\n    # Create list to store directories where files extracted\n    extracted_dirs = []\n    # Loop through zip files\n    for zip_file in zip_files:\n        # Get prefix part of filename\n        prefix = re.search(pattern, zip_file).group(1)\n        # Create directory to extract files\n        extract_dir = os.path.join(directory, prefix)\n        # Extract files\n        with zipfile.ZipFile(os.path.join(directory, zip_file), 'r') as zip_ref:\n            zip_ref.extractall(extract_dir)\n        # Add directory to list\n        extracted_dirs.append(extract_dir)\n    # Return list of directories where files extracted\n    return extracted_dirs\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archive all files match pattern delete original files. Parameters: - pattern (str): pattern to match files. Returns: - archive_file (str): archive file path. Requirements: - subprocess - os - glob Example: >>> task_func('*.txt') Note function return archive file path.\n    \"\"\"\n    # Create archive directory if not exists\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Get list of files matching pattern\n    files = glob.glob(pattern)\n\n    # Create archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar')\n\n    # Create archive\n    subprocess.run(['tar', '-cvf', archive_file] + files)\n\n    # Delete original files\n    for file in files:\n        os.remove(file)\n\n    return archive_file\n\nThe code you've provided is a Python function named `task_func` that takes a pattern as an argument and archives all files that match this pattern. It then deletes the original files. The function uses the `subprocess` module to run the `tar` command to create the archive and the `os` and `glob` modules to handle file operations.\n\nHere's a breakdown of the function:\n\n1. The function first checks if the archive directory exists. If it doesn't, it creates it using `os.makedirs(ARCHIVE_DIR)`.\n\n2. It then uses `glob.glob(pattern)` to get a list of all files that match the given pattern.\n\n3. The function creates an archive file name by joining the archive directory path with the string 'archive.tar' using `os.path.join(ARCHIVE_DIR, 'archive.tar')`.\n\n4. It uses `subprocess.run(['tar', '-cvf', archive_file] + files)` to create the archive. The `tar` command is used with the `-cvf` options, which stand for \"create, verbose, and file\". The `+ files` part adds the list of files to be archived to the command.\n\n5. The function then uses a `for` loop to delete the original files using `os.remove(file)`.\n\n6. Finally, the function returns the path to the archive file.\n\nThe function is designed to be used as follows:\n\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count total goals penalties from CSV file update with given goals penalties. Parameters: goals (dict): dictionary keys team names values numbers goals scored penalties (dict): dictionary keys team names values numbers penalties incurred. Returns: - count (Counter.collections): Counter object with total counts of goals penalties. Requirements: - csv - os - collections.Counter Example goals = {'Team A': 3 'Team B': 2 'Team C': 1 'Team D': 0 'Team E': 2} penalties = {'Team A': 1 'Team B': 0 'Team C': 2 'Team D': 3 'Team E': 1} counts = task_func(goals, penalties) >>> print(counts) Counter({'goals': 8, 'penalties': 7})\n    \"\"\"\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    # Update the data with the given goals and penalties\n    for i in range(1, len(data)):\n        team = data[i][0]\n        if team in goals:\n            data[i][1] = str(int(data[i][1]) + goals[team])\n        if team in penalties:\n            data[i][2] = str(int(data[i][2]) + penalties[team])\n\n    # Write the updated data back to the CSV file\n    with open(csv_file_path, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(data)\n\n    # Count the total goals and penalties\n    counts = Counter()\n    for row in data[1:]:\n        counts['goals'] += int(row[1])\n        counts['penalties'] += int(row[2])\n\n    return counts\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from source directory to target directory based on specified pattern. function iterates through all files in source directory if file's name matches specified pattern, moved to target directory. Parameters: - source_dir (str): path to source directory. - target_dir (str): path to target directory. - file_pattern (str, optional): regular expression pattern filenames must match to be moved. Default is r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b', matches filenames alphanumeric characters extensions txt, doc, or docx. Returns: - moved_files_count (int): number of files successfully moved from source directory to target directory. Requirements: - re - os - shutil Example: >>> task_func('/path/to/source', '/path/to/target') 3 example move 3 files from '/path/to/source' to '/path/to/target' if filenames match default pattern.\n    \"\"\"\n    moved_files_count = 0\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            source_file = os.path.join(source_dir, filename)\n            target_file = os.path.join(target_dir, filename)\n            shutil.move(source_file, target_file)\n            moved_files_count += 1\n    return moved_files_count\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate similarity values between two texts based on cosine similarity Levenshtein ratio texts cleaned by removing non-alphanumeric characters except spaces converted to lowercase. Cosine similarity computed based on term frequency in each text Levenshtein ratio computed using 'ratio' function from 'python-Levenshtein' library measures similarity of two strings number between 0 and 1. Parameters: text1 (str): first string to compare. text2 (str): second string to compare. Returns: tuple: tuple containing cosine similarity and Levenshtein ratio as floats. cosine similarity (float): similarity ranges from 0 to 1 1 means identical term frequency 0 indicates no common terms. levenshtein_ratio (float): Levenshtein ratio also ranges from 0 to 1 1 means strings identical 0 means completely different. Requirements: - re - numpy - collections - Levenshtein Example: >>> task_func(\"Hello, World!\", \"Hello World\") (0.9999999999999998, 0.9565217391304348)\n    \"\"\"\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Compute term frequency\n    tf1 = Counter(text1.split())\n    tf2 = Counter(text2.split())\n\n    # Compute cosine similarity\n    common_terms = set(tf1.keys()) & set(tf2.keys())\n    numerator = sum([tf1[t] * tf2[t] for t in common_terms])\n    denominator = np.sqrt(sum([tf1[t] ** 2 for t in tf1.keys()])) * np.sqrt(sum([tf2[t] ** 2 for t in tf2.keys()]))\n    cosine_similarity = numerator / denominator if denominator != 0 else 0\n\n    # Compute Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    Generate permutations given list numbers calculate sum factorials each number empty list given function returns empty lists. Parameters: numbers (list of int): list integers permute calculate factorial sums Returns: list of int: list containing sums factorials each number permutation. list list int: list containing all permutations numbers. Raises TypeError: If numbers not list integers ValueError: If input numbers negative. Requirements: - functools.reduce - itertools.permutations math.factorial Example >>> fac, perm = task_func([1, 2, 3]) >>> print(fac) [9, 9, 9, 9, 9, 9] >>> print(perm) [(1, 2, 3), (1, 3, 2), (2, 1, 3) (2, 3, 1), (3, 1, 2), (3, 2, 1)] >>> fac, perm = task_func([0, 4]) >>> print(fac) [25, 25] >>> print(perm) [(0, 4), (4, 0)]\n    \"\"\"\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input numbers not list\")\n    if not all(isinstance(i, int) for i in numbers):\n        raise TypeError(\"Input numbers not list integers\")\n    if any(i < 0 for i in numbers):\n        raise ValueError(\"Input numbers negative\")\n    if len(numbers) == 0:\n        return [], []\n    perm = list(permutations(numbers))\n    fac = [reduce(lambda x, y: x * y, [math.factorial(i) for i in p]) for p in perm]\n    return fac, perm\n"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory (SOURCE_DIR) to another (DEST_DIR) based on specified file extensions issues warnings for files not transferred due to issues. Parameters: SOURCE_DIR (str): source directory path files transferred DEST_DIR (str): destination directory path files transferred EXTENSIONS (list): list of file extensions consider for transferring. Example: ['.txt', '.csv', '.xlsx'] Returns transferred_files (list): list containing names of files successfully transferred. Requirements: - warnings - os - glob - shutil - time Example >>> task_func('/path/to/source', '/path/to/destination', ['.txt', '.csv']) ['file1.txt', 'file2.csv'] >>> task_func('/path/to/source', '/path/to/destination', ['.jpg']) []\n    \"\"\"\n    transferred_files = []\n    for extension in EXTENSIONS:\n        source_files = glob.glob(os.path.join(SOURCE_DIR, f'*{extension}'))\n        for file in source_files:\n            try:\n                shutil.move(file, DEST_DIR)\n                transferred_files.append(os.path.basename(file))\n                time.sleep(0.1)  # Add a small delay to avoid potential issues with file access\n            except Exception as e:\n                warnings.warn(f\"Failed to transfer file {file}: {e}\")\n    return transferred_files\n"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef task_func(data):\n    \"\"\"\n    function takes list tuples containing elements counts weights normalizes counts z-score normalization weights min-max scaling returns pandas DataFrame with items counts weights. Parameters: data (list tuples): list each tuple contains element count weight (float). Example: [('A', 100, 0.5), ('B', 200,)] Returns DataFrame pandas DataFrame with three columns 'Item', 'Normalized Count' 'Normalized Weight' Each corresponds to entry input data. Requirements - pandas - numpy - scipy.stats.zscore - sklearn.preprocessing.MinMaxScaler Example data = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7)] report = task_func(data) >>> print(report) Item Normalized Count Weight 0 A -1.224745 0.0 1 B 1.224745 0.5 2 C 0.000000 1.0 data = [('Andrew', 5743, 0.925), ('Elizabeth', 4655, 1.0875) ('Susan', 4716, 0.65)'Christopher', 2100, 0.05)('Timothy', 3943, 0.175)] report = task_func(data) >>> print(report) Item Normalized Count Weight 0 Andrew 1.248851 0.843373 1 Elizabeth 0.349969 1.000000 2 Susan 0.400366 0.578313 3 Christopher -1.760916 0.000000 4 Timothy -0.238270 0.120482\n    \"\"\"\n    # Create a DataFrame from the input data\n    df = pd.DataFrame(data, columns=['Item', 'Count', 'Weight'])\n\n    # Normalize the counts using z-score normalization\n    df['Normalized Count'] = zscore(df['Count'])\n\n    # Normalize the weights using min-max scaling\n    scaler = MinMaxScaler()\n    df['Normalized Weight'] = scaler.fit_transform(df[['Weight']])\n\n    return df\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    Calculate mean numerical values each position across tuples list Non-numeric values ignored means computed from available data missing data some tuples ignored. DataFrame with one columns named 'Mean Value' contains mean values all tuple positions index 'Position i' i current position. empty list passed empty DataFrame returned. Parameters: data_list (list tuples): list containing tuples mixed data types (string, int float Defaults to [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)] Returns DataFrame: pandas DataFrame with mean values numerical data each position. Requirements: - pandas - numpy - itertools Example: >>> df = task_func() >>> print(df) Mean Value Position 0 NaN Position 1 3.0 Position 2 4.3 >>> data = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)] >>> df = task_func() >>> print(df) Mean Value Position 0 NaN Position 1 3.0 Position 2 4.3\n    \"\"\"\n    if not data_list:\n        return pd.DataFrame()\n\n    # Extract numerical values from each position across tuples\n    numerical_values = []\n    for i in range(len(data_list[0])):\n        numerical_values.append([tuple_[i] for tuple_ in data_list if isinstance(tuple_[i], (int, float))])\n\n    # Calculate mean values for each position\n    mean_values = [np.mean(values) if values else np.nan for values in numerical_values]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Mean Value': mean_values}, index=['Position ' + str(i) for i in range(len(mean_values))])\n\n    return df\n\n"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    Perform chi-square test of independence of variables in contingency table function takes DataFrame containing categorical data two column names constructs contingency table from two categorical columns performs chi-square test independence returns p-value test indicates probability of observing data if null hypothesis (independence variables true. Parameters: data (pd.DataFrame): DataFrame containing categorical variables. col1 (str): name first categorical column. col2 (str): name second categorical column. Returns float: p-value of chi-square test independence. Raises ValueError: If 'data' empty 'col1' or 'col2' not in 'data' or columns have multiple categories some categories less than 5 observations (violating chi-square test assumptions). TypeError: If one or columns contain non-categorical data. Requirements: numpy pandas scipy.stats.chi2_contingency Examples: >>> data = pd.DataFrame({ ... 'Var1': ['A'] * 40 + ['B'] * 60, ... 'Var2': ['X'] * 25 + ['Y'] * 25 + ['X'] * 25 + ['Y'] * 25 ... }) >>> task_func(data, 'Var1', 'Var2') 0.06619257972219346 >>> np.random.seed(42) >>> data = pd.DataFrame({ ... 'a': np.random.choice(['A', 'B'], size=100), ... 'b': np.random.choice(['X', 'Y'], size=100) ... }) >>> task_func(data, 'a', 'b') 1.0\n    \"\"\"\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"Column '{col1}' or '{col2}' not in data\")\n    if not data[col1].dtype == 'object' or not data[col2].dtype == 'object':\n        raise TypeError(f\"Column '{col1}' or '{col2}' is not categorical\")\n    if len(data[col1].unique()) > 2 or len(data[col2].unique()) > 2:\n        raise ValueError(f\"Column '{col1}' or '{col2}' has more than 2 categories\")\n    if data[col1].value_counts().min() < 5 or data[col2].value_counts().min() < 5:\n        raise ValueError(f\"Some categories in column '{col1}' or '{col2}' have less than 5 observations\")\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    return p\n\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate number dice rolls calculate frequency each result return frequency array histogram results. dice rolls have 6 possible outcomes. title histogram is \"Histogram of Dice Rolls\". x-axis labeled \"Dice Value\" y-axis \"Frequency\". Parameters: rolls (int): number of dice rolls. Returns: tuple: tuple containing np.array: numpy array with frequency each outcome matplotlib.Axes: Axes object representing histogram. Requirements: - numpy - matplotlib.pyplot - random Examples: >>> import random >>> random.seed(0) >>> outcomes, ax = task_func(10000) >>> print(outcomes) [1656 1690 1696 1657 1632 1669] >>> plt.show() >>> random.seed(10) >>> outcomes, ax = task_func(100) >>> print(outcomes) [15 21 17 22 16 9] >>> plt.show()\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    outcomes = np.zeros(len(NUMBERS))\n    for _ in range(rolls):\n        roll = random.choice(NUMBERS)\n        outcomes[roll - 1] += 1\n    ax = plt.hist(outcomes, bins=len(NUMBERS), edgecolor='black', align='left')\n    plt.title(\"Histogram of Dice Rolls\")\n    plt.xlabel(\"Dice Value\")\n    plt.ylabel(\"Frequency\")\n    return outcomes, ax\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives processed files from source directory to target directory. function identifies processed files by '_processed' suffix in filename. Parameters: source_dir (str): directory containing files archived. target_dir (str): directory archive saved. archive_name (str): name of archive file. Default is 'archive.zip'. Returns: str: path to created archive. Requirements: - os - re - shutil - zipfile Example: >>> task_func('./data/',./data_processed/') './data_processed/archive.zip' >>> task_func('./data/',/data_processed/ 'my_archive.zip') './data_processed/my_archive.zip'\n    \"\"\"\n    # Create target directory if it does not exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Find all files with '_processed' suffix in source directory\n    processed_files = [f for f in os.listdir(source_dir) if f.endswith('_processed')]\n\n    # Create archive\n    with zipfile.ZipFile(os.path.join(target_dir, archive_name), 'w') as archive:\n        for file in processed_files:\n            # Add file to archive\n            archive.write(os.path.join(source_dir, file), file)\n\n    # Return path to created archive\n    return os.path.join(target_dir, archive_name)\n\nThe function `task_func` is designed to archive files with a '_processed' suffix in the source directory to a specified target directory. The function takes three parameters: `source_dir`, `target_dir`, and `archive_name`. The `source_dir` parameter is the directory containing the files to be archived, the `target_dir` parameter is the directory where the archive will be saved, and the `archive_name` parameter is the name of the archive file. The function returns the path to the created archive.\n\nThe function first checks if the target directory exists. If it does not, it creates the directory. Then, it finds all files in the source directory that have a '_processed' suffix. It creates a new archive file in the target directory and adds each processed file to the archive. Finally, it returns the path to the created archive.\n\nThe function uses the `os` module to check if the target directory exists and to create the directory if it does not. It uses the `re` module to find files with a '_processed' suffix. It uses the `shutil` module to add files to the archive. It uses the `zipfile` module to create the archive file.\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Predicts stock closing prices next 7 days linear regression plots data. Parameters: df (DataFrame): input dataframe columns 'date' 'closing_price' 'date' datetime format. Returns tuple tuple containing list: list predicted prices next 7 days Axes: matplotlib Axes object containing plot. Requirements: - pandas - numpy - matplotlib.pyplot - sklearn.linear_model.LinearRegression Constants function uses constant time step 24*60*60 seconds generate future timestamps. Example >>> df = pd.DataFrame({ 'date': pd.date_range(start='1/1/2021' end='1/7/2021'), 'closing_price': [100, 101, 102, 103, 104, 105, 106] ... }) >>> pred_prices, plot = task_func(df) >>> print(pred_prices) [107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0]\n    \"\"\"\n    # Convert date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Sort dataframe by date\n    df = df.sort_values('date')\n    \n    # Create a new column 'timestamp' with datetime64[ns] type\n    df['timestamp'] = df['date'].astype(np.int64) // 10**9\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp'].shift(-7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(-7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(-7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(-7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(-7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(-7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns] type\n    df['timestamp_next'] = df['timestamp_next'].shift(7)\n    \n    # Create a new column 'timestamp_next' with datetime64[ns]"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies plots outliers 'closing_price' column DataFrame using Z-Score method. Parameters: df (pandas.DataFrame): input DataFrame contain column named 'closing_price' with numerical values. z_threshold (float optional): absolute Z-Score threshold for identifying outliers. Default 2. Returns tuple: tuple containing elements pandas.DataFrame: DataFrame containing outliers 'closing_price' column matplotlib.axes._axes.Axes: plot object displaying outliers x-axis label 'Index' y-axis label 'Closing Price' title 'Outliers in Closing Prices'. Requirements: - numpy - matplotlib.pyplot - scipy.stats.zscore Constants Z-Score threshold identifying outliers customizable via 'z_threshold' parameter. Examples import pandas as pd >>> df1 = pd.DataFrame({ 'closing_price': [100, 101 102 103 104, 150] ... >>> outliers1, plot1 = task_func(df1) >>> df2 = pd.DataFrame({ 'closing_price': [10, 20, 30, 40, 50, 100] ...) >>> outliers2, plot2 = task_func(df2, z_threshold=1.5)\n    \"\"\"\n    # Calculate Z-Score for 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n\n    # Identify outliers based on Z-Score threshold\n    outliers = df[df['z_score'].abs() > z_threshold]\n\n    # Plot outliers\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['closing_price'], label='Closing Price')\n    ax.scatter(outliers.index, outliers['closing_price'], color='r', label='Outliers')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n\n    return outliers, ax\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize distribution stock closing prices using box plot and histogram within single figure function designed understand spread, central tendency distribution shape stock. Note: tile of box plot set to 'Box Plot of Closing Prices' title histogram set to 'Histogram of Closing Prices'. Requirements: - pandas - matplotlib.pyplot - seaborn Parameters: df (DataFrame): pandas DataFrame containing one column named 'closing_price' with stock closing prices. Returns: tuple: tuple containing two matplotlib.axes._axes.Axes objects first for boxplot second for histogram. Example: >>> df = pd.DataFrame({ ... 'closing_price': [100, 101, 102, 103, 104, 150] ... }) >>> boxplot_ax, histplot_ax = task_func(df) >>> print(boxplot_ax.get_title()) Box Plot of Closing Prices >>> print(histplot_ax.get_title()) Histogram of Closing Prices\n    \"\"\"\n    # Create a figure and a set of subplots\n    fig, (boxplot_ax, histplot_ax) = plt.subplots(2, 1, figsize=(10, 8))\n\n    # Plot the boxplot\n    sns.boxplot(data=df, x='closing_price', ax=boxplot_ax)\n    boxplot_ax.set_title('Box Plot of Closing Prices')\n\n    # Plot the histogram\n    sns.histplot(data=df, x='closing_price', kde=True, ax=histplot_ax)\n    histplot_ax.set_title('Histogram of Closing Prices')\n\n    # Return the axes\n    return boxplot_ax, histplot_ax\nThe function `task_func` takes a pandas DataFrame as input and returns a tuple containing two matplotlib.axes._axes.Axes objects. The function creates a figure and a set of subplots using `plt.subplots()`. It then plots a boxplot of the 'closing_price' column using `sns.boxplot()` and sets the title of the boxplot to 'Box Plot of Closing Prices'. It also plots a histogram of the 'closing_price' column using `sns.histplot()` and sets the title of the histogram to 'Histogram of Closing Prices'. Finally, it returns the axes of the boxplot and histogram.\n"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts share closing prices next 7 days ARIMA model plots forecast. Parameters: df (pd.DataFrame): input dataframe columns 'date' 'closing_price' 'date' datetime dtype 'closing_price' float. Returns: Tuple[List[float], Axes]: tuple containing list forecasted prices next 7 days matplotlib Axes object subplot. Requirements: - pandas - numpy - matplotlib.pyplot - statsmodels.tsa.arima.model.ARIMA Example df = pd.DataFrame({ 'date': pd.date_range(start='1/1/2021' end='1/7/2021'), 'closing_price': [100, 101, 102 103, 104, 105, 106] ... }) >>> forecast, ax = task_func(df) >>> print(forecast) [106.99999813460752, 107.99999998338443, 108.99999547091295, 109.99999867405204, 110.99999292499156, 111.99999573455818, 112.9999903188028]\n    \"\"\"\n    # Extract the closing prices and dates\n    closing_prices = df['closing_price'].values\n    dates = df['date'].values\n\n    # Fit the ARIMA model\n    model = ARIMA(closing_prices, order=(1, 1, 1))\n    model_fit = model.fit()\n\n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)[0]\n\n    # Create a subplot\n    fig, ax = plt.subplots()\n\n    # Plot the original data\n    ax.plot(dates, closing_prices, label='Original')\n\n    # Plot the forecasted data\n    ax.plot(dates[-7:], forecast, label='Forecast')\n\n    # Set the labels and title\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('ARIMA Forecast')\n\n    # Add a legend\n    ax.legend()\n\n    return forecast, ax\n\nThe function `task_func` is designed to forecast the closing prices of a stock for the next 7 days using an ARIMA model. It takes a pandas DataFrame as input, which should contain two columns: 'date' and 'closing_price'. The 'date' column should contain datetime objects, and the 'closing_price' column should contain float values representing the closing prices of the stock.\n\nThe function first extracts the closing prices and dates from the DataFrame. It then fits an ARIMA model to the closing prices using the `ARIMA` class from the `statsmodels.tsa.arima.model` module. The order of the ARIMA model is set to (1, 1, 1), which means it uses one lag, one difference, and one moving average.\n\nAfter fitting the model, the function uses the `forecast` method to predict the closing prices for the next 7 days. The forecasted prices are returned as a list.\n\nThe function also creates a matplotlib subplot and plots the original closing prices and the forecasted prices on the same plot. The x-axis represents the dates, and the y-axis represents the closing prices. The original data is plotted as a line, and the forecasted data is plotted as a line with a different color. The subplot is returned along with the forecasted prices.\n\nHere is an example usage of the function:\n\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create dictionary containing two-letter combinations of lowercase English alphabets. dictionary values represent frequency combinations in given word. If combination appear in word, value 0. Requirements: - collections.Counter - itertools - string Parameters: - word (str): input string containing alphabetic characters. Returns: - dict: dictionary with keys as two-letter alphabet combinations values as counts in word. Requirements function uses `collections.Counter` library to count occurrences of two-letter combinations. function uses `itertools.permutations` method to generate all two-letter combinations of alphabets. function uses `string` library to get string of lowercase alphabets. Example: >>> list(task_func('abcdef').items())[:5] [('ab', 1), ('ac', 0), ('ad', 0), ('ae', 0), ('af', 0)]\n    \"\"\"\n    alphabet = string.ascii_lowercase\n    combinations = list(itertools.permutations(alphabet, 2))\n    counter = Counter()\n    for i in range(len(word) - 1):\n        counter[word[i:i+2]] += 1\n    return dict(counter)\n\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Create visualize sales report for different categories over period time. Parameters start_date (str): start date report 'YYYY-MM-DD' format Default '2016-01-01'. periods (int): number of periods report Default 13. freq (str): frequency of dates generated Default 'WOM-2FRI' (WeekOfMonth-2nd Friday). categories (list): List categories report Default ['Electronics' 'Fashion' 'Home & Kitchen' 'Automotive' 'Sports']. Returns Returns DataFrame sales data columns 'Date' 'Category' 'Sales'. Returns Matplotlib Axes object for plot. Requirements pandas - matplotlib.pyplot - numpy Example >>> df, ax = task_func(start_date='2020-01-01' periods=5 freq='W-MON' categories=['Electronics', 'Fashion']) >>> df Date Category Sales 0 2020-01-06 Electronics 272 1 Fashion 147 2 2020-01-13 Electronics 217 3 2020-01-13 Fashion 292 4 2020-01-20 Electronics 423 5 2020-01-20 Fashion 351 6 2020-01-27 Electronics 295 7 2020-01-27 Fashion 459 8 2020-02-03 Electronics 109 9 2020-02-03 Fashion 311\n    \"\"\"\n    # Create date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Date': date_range, 'Category': [], 'Sales': []})\n    \n    # Fill DataFrame with random sales data\n    for category in categories:\n        df.loc[df['Category'] == category, 'Sales'] = np.random.randint(100, 500, size=len(df))\n    \n    # Plot sales data\n    ax = df.plot(x='Date', y='Sales', kind='bar', figsize=(10, 6), title='Sales Report')\n    \n    # Set x-axis labels\n    ax.set_xticklabels(df['Date'].dt.strftime('%Y-%m-%d'), rotation=45)\n    \n    # Set y-axis label\n    ax.set_ylabel('Sales')\n    \n    # Set title\n    ax.set_title('Sales Report')\n    \n    # Show plot\n    plt.show()\n    \n    return df, ax\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates time series sales data from specified date linear regression forecast future sales based provided sales data. Parameters: start_date (str): start date sales data in YYYY-MM-DD format Default '2016-01-01'. periods (int): number of periods sales data available Default is 13. freq (str): frequency sales data 'WOM-2FRI' second Friday each month Default 'WOM-2FRI'. sales_data (array-like optional): array containing actual sales data If not provided random data generated. Returns numpy array containing forecasted future sales same number periods as input data. Requirements: - numpy - pandas - sklearn.linear_model.LinearRegression Examples >>> np.random.seed(42) For consistent random data generation examples >>> task_func('2016-01-01', 13, 'WOM-2FRI') array([313.65384615, 318.56043956, 323.46703297, 328.37362637, 333.28021978, 338.18681319, 343.09340659, 348. , 352.90659341, 357.81318681, 362.71978022, 367.62637363, 372.53296703]) >>> task_func('2020-01-01', 5, 'M', [200, 300, 400, 500, 600]) array([238.9, 226. , 213.1, 200.2, 187.3])\n    \"\"\"\n    if sales_data is None:\n        sales_data = np.random.rand(periods)\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    df = pd.DataFrame(sales_data, index=dates, columns=['sales'])\n    model = LinearRegression()\n    model.fit(df.index.values.reshape(-1, 1), df['sales'].values)\n    future_dates = pd.date_range(start=dates[-1], periods=periods, freq=freq)\n    forecast = model.predict(future_dates.values.reshape(-1, 1))\n    return forecast\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns specified number tasks to employees with due date current day returns DataFrame with assignments. Parameters: task_list (list str): List tasks assigned. n_tasks (int): Number of tasks assigned number not negative can be larger than number tasks in task_list. employees (list str optional): List employee names tasks assigned. If not provided defaults to ['John Doe' 'Jane Smith' 'James Brown' 'Mary Johnson' 'Robert Davis']. seed (int optional): Seed for random number generator ensure reproducibility. Defaults to None (not set). Returns: - pd.DataFrame: Contains columns 'Task Name', 'Assigned To' 'Due Date' each row representing assigned task. Raises ValueError: If n_tasks negative. Note Task names sanitized by replacing spaces with underscores Due dates set to current system date. Requirements: - pandas - random - datetime Examples: >>> df = task_func(['Clean Office', 'Prepare Report' 'Client Meeting'], 2, seed=42) >>> df Task Name Assigned To Due Date 0 Client_Meeting John Doe 2024-04-13 1 Clean_Office James Brown 2024-04-13 >>> type(df) <class 'pandas.core.frame.DataFrame'>\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"Number of tasks cannot be negative\")\n    if seed is not None:\n        random.seed(seed)\n    if n_tasks > len(task_list):\n        print(\n            f\"Warning: Number of tasks requested ({n_tasks}) is greater than number of tasks available ({len(task_list)}).\"\n        )\n    task_list = [task.replace(\" \", \"_\") for task in task_list]\n    assignments = []\n    for i in range(n_tasks):\n        task = random.choice(task_list)\n        employee = random.choice(employees)\n        due_date = datetime.now().strftime(\"%Y-%m-%d\")\n        assignments.append([task, employee, due_date])\n    return pd.DataFrame(assignments, columns=[\"Task Name\", \"Assigned To\", \"Due Date\"])\n\n"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n\ndef task_func(mystrings, text):\n    \"\"\"\n    Replace spaces in words with underscores plots frequency of each unique word. Parameters: mystrings (list of str): List of words/phrases where spaces replaced with underscores text (str): text modifications applied word frequencies calculated. not be empty. Returns: matplotlib.axes.Axes: Axes object of plot. Raises: ValueError: If input text empty. Requirements: - numpy - matplotlib - re - collections Notes All operations case-insensitive frequency plot displays each unique word on x-axis in order appear after modification with corresponding frequency on y-axis. Examples: >>> ax = task_func(['Lorem ipsum', 'consectetur adipiscing'], 'Lorem ipsum dolor sit amet lorem Ipsum') >>> type(ax) <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n    # Replace spaces with underscores\n    modified_text = re.sub(r'\\s+', '_', text)\n    # Split modified text into words\n    words = modified_text.split('_')\n    # Count frequency of each unique word\n    word_counts = Counter(words)\n    # Sort words by frequency\n    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n    # Extract words and frequencies\n    words, frequencies = zip(*sorted_words)\n    # Plot frequency of each unique word\n    ax = plt.figure().add_subplot(111)\n    ax.bar(words, frequencies)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Frequency of each unique word')\n    return ax\n\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from source directory save as zip file to target directory. Parameters: source_directory (str): source directory containing files to be zipped target_directory (str): destination directory of zip file created. If not exist function will create it zip_name (str): name of zip file to create (without extension; '.zip' added automatically). Returns: - str: full path to created zip file in format \"/path/to/target_directory/zip_name.zip\". Raises OSError: If source_directory exist. Requirements: - os - glob - pathlib - zipfile Note valid extensions are ['.txt', '.docx' '.xlsx' '.csv']. Example: >>> path = task_func('/path/to/source_directory', '/path/to/target_directory', 'zipped_files') >>> type(path) <class 'str'> >>> path '/path/to/target_directory/zipped_files.zip'\n    \"\"\"\n    # Check if source directory exist\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory {source_directory} does not exist.\")\n    # Check if target directory exist, if not create it\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    # Get all files with valid extensions from source directory\n    files = glob.glob(os.path.join(source_directory, '*'))\n    files = [file for file in files if os.path.splitext(file)[1] in ['.txt', '.docx', '.xlsx', '.csv']]\n    # Create zip file\n    with zipfile.ZipFile(os.path.join(target_directory, f'{zip_name}.zip'), 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    # Return full path to created zip file\n    return os.path.join(target_directory, f'{zip_name}.zip')\n\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt .docx .xlsx .csv) from source directory to CSV files saves in target directory. Parameters: - source_directory (str): path to source directory containing files to be converted. target_directory (str): path to target directory where converted CSV files saved. If not exist function will create it. Returns: - int: number of files successfully converted to CSV. Raises: - FileNotFoundError: If source directory exist. Requirements: - os - pathlib - pandas - python-docx - openpyxl Notes: Each file's text content captured stored in CSV with single 'Text' column no row indices. function overwrite existing files in target directory if same names as converted files. Example: >>> task_func('/Users/test/Documents', '/Users/test/Documents/csv_files') 4 >>> task_func('/path/to/source', '/path/to/target') 2\n    \"\"\"\n    # Check if source directory exist\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' not found.\")\n    # Check if target directory exist, if not create it\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    # Get list of files in source directory\n    files = os.listdir(source_directory)\n    # Initialize counter for number of files converted\n    count = 0\n    # Loop through files in source directory\n    for file in files:\n        # Get file path\n        file_path = os.path.join(source_directory, file)\n        # Get file extension\n        file_extension = os.path.splitext(file_path)[1]\n        # Check if file extension is one of the supported types\n        if file_extension in ['.txt', '.docx', '.xlsx', '.csv']:\n            # Read file content\n            if file_extension == '.txt':\n                with open(file_path, 'r') as f:\n                    content = f.read()\n            elif file_extension == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n            elif file_extension == '.xlsx':\n                df = pd.read_excel(file_path)\n                content = df.to_string(index=False)\n            elif file_extension == '.csv':\n                df = pd.read_csv(file_path)\n                content = df.to_string(index=False)\n            # Create new file name for CSV file\n            csv_file_name = os.path.splitext(file)[0] + '.csv'\n            # Create new file path for CSV file\n            csv_file_path = os.path.join(target_directory, csv_file_name)\n            # Write content to CSV file\n            with open(csv_file_path, 'w') as f:\n                f.write(content)\n            # Increment counter\n            count += 1\n    # Return number of files converted\n    return count\n\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes MinMax-normalized cumulative sum each numeric column DataFrame. Parameters: - df (pandas.DataFrame): input DataFrame containing numerical values. Returns pd.DataFrame: DataFrame each column contains normalized cumulative sum column input DataFrame retaining original column names. Raises TypeError: DataFrame contains non-numeric data types ValueError: If DataFrame empty or contains NaN values. Requirements - pandas - numpy - sklearn Example input_df = pd.DataFrame({'A': [1, 2, 3], 'B': [3, 2, 1]}) >>> output_df = task_func(input_df) type(output_df) <class 'pandas.core.frame.DataFrame'> >>> output_df A B 0 0.0 0.000000 1 0.666667 2 1.0 1.000000\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n    if not all(df.dtypes.apply(pd.api.types.is_numeric_dtype)):\n        raise TypeError(\"DataFrame contains non-numeric data types\")\n    scaler = MinMaxScaler()\n    df_normalized = scaler.fit_transform(df)\n    df_cumsum = np.cumsum(df_normalized, axis=0)\n    df_cumsum = pd.DataFrame(df_cumsum, columns=df.columns)\n    return df_cumsum\n"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes directory listing each file size, creation time last modification time without recursing into subdirectories. Parameters: - directory_path (str): path to directory to analyzed. If empty function returns empty list. Returns: - list of tuples: Each tuple contains (file name, file size in bytes creation time ISO format modification time in ISO format). Raises ValueError: If directory not exist. Requirements: - os - pathlib - datetime Notes function assumes directory exists contains only files (no subdirectories processed). Times reported in system time, UTC creation modification times platform dependent some systems creation time might not be available be replaced by last metadata change time. Examples: >>> result = task_func('/path/to/directory') >>> print(result) [('example.txt', 1024, '2023-04-01T14:30:00Z', '2023-04-02T15:00:00Z'), ...] >>> result = task_func('/path/to/empty_directory') >>> print(result) []\n    \"\"\"\n    if not os.path.exists(directory_path):\n        raise ValueError(\"Directory not exist\")\n\n    result = []\n    for file_name in os.listdir(directory_path):\n        file_path = Path(directory_path) / file_name\n        if file_path.is_file():\n            file_size = file_path.stat().st_size\n            creation_time = datetime.fromtimestamp(file_path.stat().st_ctime, timezone.utc).isoformat()\n            modification_time = datetime.fromtimestamp(file_path.stat().st_mtime, timezone.utc).isoformat()\n            result.append((file_name, file_size, creation_time, modification_time))\n\n    return result\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates population report DataFrame CSV file based on JSON data. Parameters: json_data (str): Nested JSON string containing country names (str) as keys populations (int) as values parent key expected \"Countries\". Example format: '{\"Countries\": {\"Country A\": 331002651, \"Country B\": 67886011}}'. output_dir (str): Directory path CSV report saved. Defaults to current directory function create if. file_name (str): Name of CSV report. Defaults to \"country_population_report.csv\". Returns: str: file path of generated CSV report. pd.DataFrame: country-population data loaded from input JSON with columns: \"Country\", \"Population\". Raises ValueError: If JSON data malformed, empty contains non-string country names non-numeric or negative populations. IOError: If file be written to specified directory. Requirements: json - os - pandas - math Notes Output DataFrame has no extra index column. If function encounters float population valid round it down to nearest integer. Example: >>> json_str = '{\"Countries\": {\"Country A\": 331002651, \"Country B\": 67886011}}' >>> csv_file_path, df = task_func(json_str) >>> print(csv_file_path)./country_population_report.csv >>> df Country Population 0 Country A 331002651 1 Country B 67886011\n    \"\"\"\n    try:\n        data = json.loads(json_data)\n        if \"Countries\" not in data:\n            raise ValueError(\"JSON data malformed, missing 'Countries' key.\")\n        countries = data[\"Countries\"]\n        if not isinstance(countries, dict):\n            raise ValueError(\"JSON data malformed, 'Countries' value not a dictionary.\")\n        country_population = []\n        for country, population in countries.items():\n            if not isinstance(country, str):\n                raise ValueError(\"JSON data malformed, country name not a string.\")\n            if not isinstance(population, int) and not isinstance(population, float):\n                raise ValueError(\"JSON data malformed, population not a number.\")\n            if population < 0:\n                raise ValueError(\"JSON data malformed, population negative.\")\n            country_population.append([country, math.floor(population)])\n        df = pd.DataFrame(country_population, columns=[\"Country\", \"Population\"])\n        csv_file_path = os.path.join(output_dir, file_name)\n        df.to_csv(csv_file_path, index=False)\n        return csv_file_path, df\n    except json.JSONDecodeError:\n        raise ValueError(\"JSON data malformed.\")\n    except IOError:\n        raise IOError(\"File could not be written to specified directory.\")\n\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files directories) in directory against specified conditions. Parameters: dir_path (str): path to directory evaluated. Must exist. predicates (list of strings): Names of conditions to check for Must contain valid conditions Invalid conditions ignored. Supported conditions: 1. 'is_file': item is file 2. 'is_dir': item directory 3. 'has_special_chars': item name contains character not letter, digit underscore ignoring file extensions 4. 'has_numbers': item name contains number Returns: - dict: dictionary with directory items as keys results of condition checks as values. Raises ValueError: If no valid predicates provided FileNotFoundError: If specified directory not exist or not directory. function evaluates file/directory names full path Predicates deduplicated. Requirements: - os - re - pathlib Examples: task_func('/path/to/dir', ['is_file', 'has_numbers']) {'file.txt': {'is_file': True, 'has_numbers': False} 'file2.txt': {'is_file': True, 'has_numbers': True}} >>> task_func('/path/to/dir', ['is_dir', 'has_special_chars']) {'my_folder': {'is_dir': True, 'has_special_chars': False} 'a_@Folder': {'is_dir': True, 'has_special_chars': True}}\n    \"\"\"\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"Directory {dir_path} does not exist or is not a directory\")\n\n    if not predicates:\n        raise ValueError(\"No valid predicates provided\")\n\n    predicates = list(set(predicates))  # deduplicate predicates\n\n    results = {}\n\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        item_results = {}\n\n        for predicate in predicates:\n            if predicate == 'is_file':\n                item_results[predicate] = os.path.isfile(item_path)\n            elif predicate == 'is_dir':\n                item_results[predicate] = os.path.isdir(item_path)\n            elif predicate == 'has_special_chars':\n                item_results[predicate] = bool(re.search(r'[^a-zA-Z0-9_]', item))\n            elif predicate == 'has_numbers':\n                item_results[predicate] = bool(re.search(r'\\d', item))\n            else:\n                continue\n\n        results[item] = item_results\n\n    return results\n\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\n\ndef task_func(hex_string):\n    \"\"\"\n    Convert hexadecimal string to various encodings. function takes hexadecimal string as input performs encoding operations decodes hexadecimal string to bytes converts bytes into UTF-8 string. UTF-8 string encoded into different formats: hexadecimal, base64, UTF-8, UTF-16, UTF-32 ASCII (if possible), URL encoding ROT13. if ASCII not possible, returns 'Not representable in ASCII'. Parameters: - hex_string (str): input string in hexadecimal format. Returns: - dict: dictionary containing input string encoded in various formats. dictionary's keys are encoding types ('hex', 'base64' 'utf-8' 'utf-16' 'utf-32' 'ASCII' 'URL' 'ROT13'), values are corresponding encoded strings. If string be represented in ASCII, 'ASCII' key maps to 'Not representable in ASCII'.Requirements binascii base64 codecs Example task_func(\"4a4b4c\") {'hex' '4a4b4c' 'base64 'SktM' 'utf-8 'JKL' 'utf-16 'JKL' 'utf-32 'JKL' 'ASCII' 'JKL' 'URL' 'JKL' 'ROT13' 'WXY' task_func(\"68656c6c6f\") {'hex' '68656c6c6f' 'base64 'aGVsbG8=' 'utf-8' 'hello' 'utf-16' 'hello' 'utf-32' 'hello' 'ASCII' 'hello' 'URL' 'hello' 'ROT13' 'uryyb'\n    \"\"\"\n    # Convert hexadecimal string to bytes\n    bytes_data = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_string = bytes_data.decode('utf-8')\n\n    # Encode UTF-8 string into different formats\n    encoded_dict = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_string.encode('ascii').decode('ascii') if utf8_string.isascii() else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n\n    return encoded_dict\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    \"\"\"\n    Downloads tar.gz file from specified URL, validates MD5 checksum against predefined expected value. If checksum matches, extracts contents of tar.gz file. Otherwise, deletes downloaded file. Parameters: url (str): URL from which to download tar.gz file. Returns: bool: Returns True if file successfully downloaded, MD5 checksum matches expected value, and extracted. Returns False if checksum not match expected value or if download fails. Requirements: - urllib.request - hashlib - tarfile - os Example: >>> task_func('http://example.com/files.tar.gz') True\n    \"\"\"\n    # Download tar.gz file\n    try:\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n        return False\n\n    # Validate MD5 checksum\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        md5 = hashlib.md5(f.read()).hexdigest()\n    if md5 != EXPECTED_MD5_CHECKSUM:\n        print(f\"MD5 checksum does not match expected value: {md5}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Error extracting file: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Delete downloaded file\n    os.remove(TARGET_TAR_FILE)\n    return True\n\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\n\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    Download CSV file from given URL save it to specified path count occurrences of each value in particular column. function handles various scenarios including missing columns and file download errors. Parameters: url (str): URL of CSV file to be downloaded. Must be valid and accessible URL. column_name (str): name of column in CSV file whose values to be counted. function raise ValueError if this column not found. csv_file_path (str): file path where downloaded CSV file saved. If file already exists at this path, it will be overwritten. Returns: dict: dictionary mapping values from specified column to corresponding occurrence counts. Raises: ValueError: If specified column_name not exist in CSV file, function will delete downloaded file raise ValueError with message stating \"The provided column_name '{column_name}' not exist in CSV file.\" Requirements: - urllib - os - csv - collections Example: >>> task_func('http://example.com/data.csv', 'category', 'downloaded_data.csv') {'cat1': 5, 'cat2': 3, 'cat3': 8} # hypothetical output; actual output depend on CSV data. Notes: - downloaded CSV file deleted after contents processed. function only counts values in specified column ignores other data.\n    \"\"\"\n    try:\n        # Download CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        # Handle file download errors\n        print(f\"Error downloading file: {e}\")\n        return None\n\n    # Read CSV file\n    try:\n        with open(csv_file_path, 'r') as file:\n            reader = csv.DictReader(file)\n            # Check if specified column_name exist in CSV file\n            if column_name not in reader.fieldnames:\n                raise ValueError(f\"The provided column_name '{column_name}' not exist in CSV file.\")\n            # Count occurrences of each value in specified column\n            column_values = [row[column_name] for row in reader]\n            value_counts = collections.Counter(column_values)\n    except Exception as e:\n        # Handle CSV file reading errors\n        print(f\"Error reading CSV file: {e}\")\n        return None\n\n    # Delete downloaded CSV file\n    try:\n        os.remove(csv_file_path)\n    except Exception as e:\n        # Handle file deletion errors\n        print(f\"Error deleting file: {e}\")\n        return None\n\n    return value_counts\n\n"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\n\ndef task_func(url):\n    \"\"\"\n    Fetches parses XML file from specified URL converts into Pandas DataFrame. Parameters: url (str): URL of CSV file to be downloaded. Must be valid accessible URL. Returns: pandas.DataFrame DataFrame constructed from parsed XML data. Each row DataFrame corresponds to 'item' element in XML file child elements of 'item' becoming columns. Raises: ValueError error raised in several scenarios: 1. If URL invalid or XML file cannot be fetched URL. 2. XML file has invalid syntax. 3. XML structure not conform to expected format. Requirements: - urllib - lxml - pandas Examples: # Example with valid XML structure >>> df = task_func('http://example.com/sample_data.xml') >>> print(df) name age 0 John 25 1 Jane 30 Example with invalid XML structure >>> df = task_func('http://example.com/invalid_structure.xml') ValueError: XML structure not match expected format.\n    \"\"\"\n    try:\n        # Fetch XML file from URL\n        response = urllib.request.urlopen(url)\n        xml_data = response.read()\n\n        # Parse XML data\n        root = etree.fromstring(xml_data)\n\n        # Check if XML structure matches expected format\n        if root.tag != 'root' or len(root) == 0:\n            raise ValueError('XML structure not match expected format.')\n\n        # Create DataFrame from XML data\n        df = pd.DataFrame(columns=[child.tag for child in root[0]])\n\n        for child in root:\n            row_data = {}\n            for grandchild in child:\n                row_data[grandchild.tag] = grandchild.text\n            df = df.append(row_data, ignore_index=True)\n\n        return df\n\n    except urllib.error.URLError as e:\n        raise ValueError(f'URL invalid or XML file cannot be fetched: {e}')\n\n    except etree.XMLSyntaxError as e:\n        raise ValueError(f'XML file has invalid syntax: {e}')\n\n    except Exception as e:\n        raise ValueError(f'Unexpected error: {e}')"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(url):\n    \"\"\"\n    Downloads text file from specified URL processes text to count frequency of each word plots bar chart showing ten most frequently occurring words. Parameters: url (str): URL from text file downloaded. URL should point directly to text file. Returns: tuple: tuple containing two elements: - Counter: Counter object from collections module containing word frequencies in text. - Axes: matplotlib Axes object represents plotted bar chart of ten most common words. Note function assumes URL points to plain text file may not handle binary files or non-text content correctly. Words identified using basic regular expression case-sensitive. function not remove common stopwords; all words counted as is. Requires internet access to download file from URL. Example: >>> word_freq, ax = task_func('http://www.example.com/data.txt') >>> print(word_freq.most_common(5)) [('the', 102), ('of', 76), ('and', 64), ('to', 52), ('in', 41)] Requirements: - urllib - re - collections - matplotlib\n    \"\"\"\n    # Download the text file from the URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Use regular expression to find all words in the text\n    words = re.findall(r'\\w+', text)\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Plot a bar chart of the ten most common words\n    ax = word_freq.most_common(10)\n    ax = plt.bar(range(10), [x[1] for x in ax], align='center')\n    plt.xticks(range(10), [x[0] for x in ax])\n    plt.ylabel('Frequency')\n    plt.title('Top 10 most common words')\n\n    return word_freq, ax\n\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    Downloads extracts ZIP file from specified URL to given directory. Parameters: - url (str): URL from to download ZIP file. should be valid accessible URL. download_path (str): directory path where ZIP file downloaded extracted. Defaults to \"mnt/data/downloads/\". Returns: - str: Path to directory containing extracted contents. If error occurs descriptive message returned. message starts with \"Error: \". If specific descrption is either \"The URL not point to ZIP file.\", or \"The downloaded file not a valid ZIP file.\", or \"Unable to download file from provided URL.\". Raises: - Network Issues or Invalid URL: Returns \"Error: Unable to download file from provided URL.\" if issues in reaching URL or downloading file. Incorrect File Type: Returns \"Error: URL not point to ZIP file.\" if downloaded file's content type not 'application/zip'. Corrupt ZIP File: Returns \"Error: downloaded file not a valid ZIP file.\" if downloaded file is ZIP file but corrupt or cannot be extracted. General Exceptions: Catches reports other exceptions (like runtime errors) during process with specific error message, formatted as \"Error: [exception message]\". Requirements: - requests - os - zipfile Example: >>> task_func('https://example.com/file.zip') 'mnt/data/downloads/file'\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url, stream=True)\n        # Check if the request was successful\n        if response.status_code == 200:\n            # Check if the content type is 'application/zip'\n            if response.headers['content-type'] == 'application/zip':\n                # Create a unique filename for the downloaded file\n                filename = os.path.join(download_path, url.split('/')[-1])\n                # Open the file in write-binary mode\n                with open(filename, 'wb') as file:\n                    # Write the content of the response to the file\n                    for chunk in response.iter_content(chunk_size=8192):\n                        file.write(chunk)\n                # Try to extract the ZIP file\n                try:\n                    with ZipFile(filename, 'r') as zip_ref:\n                        # Create a unique directory name for the extracted contents\n                        extracted_dir = os.path.join(download_path, os.path.splitext(url.split('/')[-1])[0])\n                        # Extract the contents of the ZIP file to the directory\n                        zip_ref.extractall(extracted_dir)\n                    # Return the path to the directory containing the extracted contents\n                    return extracted_dir\n                # If the ZIP file is corrupt or cannot be extracted, raise an exception\n                except BadZipFile:\n                    return \"Error: downloaded file not a valid ZIP file.\"\n            else:\n                return \"Error: URL not point to ZIP file.\"\n        else:\n            return \"Error: Unable to download file from provided URL.\"\n    # If any other exception occurs, catch it and return a descriptive error message\n    except Exception as e:\n        return f\"Error: {str(e)}\""}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\n\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts converts data from specified HTML table based on given 'table_id' on webpage into Pandas DataFrame. If table present but contains no data rows (i no <tr> tags), function returns empty DataFrame. Parameters: - url (str): URL of webpage from to extract table. table_id (str): 'id' attribute of HTML table to be extracted. Returns: - df (pd.DataFrame): DataFrame containing data extracted from specified HTML table. If table found but no rows (<tr> elements), empty DataFrame returned. Raises: - requests.exceptions.HTTPError: If HTTP request fails (e due to connection issues or non-successful status code like 404 or 500). - ValueError: If no table with specified 'table_id' found on webpage. error message \"Table with specified ID not found.\" Requirements: - requests - bs4.BeautifulSoup - pandas - io Notes: function raises HTTPError for unsuccessful HTTP requests, includes scenarios like network problems or non-2xx HTTP responses. ValueError raised when HTML table with specified ID not present in webpage's content, indicating incorrect ID or absence of table. If located table has no rows, by absence of <tr> tags, empty DataFrame is returned. useful for handling tables structurally present in HTML but devoid of data. Example: >>> task_func('https://example.com/data.html', 'table1') DataFrame: Name Age 0 Alice 25 1 Bob 30 Example of ValueError: >>> task_func('https://example.com/data.html', 'nonexistent_table') ValueError: Table with specified ID not found.Example empty table >>> task_func('https://example/emptytable.html'_table') DataFrame: Empty Columns: [] Index: []\n    \"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as e:\n        raise e\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if table is None:\n        raise ValueError(\"Table with specified ID not found.\")\n\n    rows = table.find_all('tr')\n    if len(rows) == 0:\n        return pd.DataFrame()\n\n    headers = [header.text.strip() for header in rows[0].find_all('th')]\n    data = [[cell.text.strip() for cell in row.find_all('td')] for row in rows[1:]]\n\n    df = pd.DataFrame(data, columns=headers)\n    return df\n\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads extracts zip file from specified URL. Parameters: url (str): URL of zip file to download. filename (str): filename under downloaded zip file saved. Returns: tuple: tuple containing status message list of filenames in unzipped directory, or empty list if extraction fails. status message contain \"Error\" when - Network-related exceptions raised if download fails. File-related exceptions raised if issue with file handling or extraction. Requirements: - requests - pathlib.Path - zipfile Example: >>> task_func('http://example.com/myfile.zip', 'myfile.zip') ('Download and extraction successful', ['file1.txt', 'file2.txt'])\n    \"\"\"\n    try:\n        # Create download directory if it doesn't exist\n        DOWNLOAD_DIR.mkdir(exist_ok=True)\n        # Create unzipped files directory if it doesn't exist\n        ZIP_DIR.mkdir(exist_ok=True)\n        # Download the zip file\n        response = requests.get(url)\n        response.raise_for_status()\n        # Save the zip file\n        zip_file_path = DOWNLOAD_DIR / filename\n        with open(zip_file_path, \"wb\") as file:\n            file.write(response.content)\n        # Extract the zip file\n        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        # Get list of filenames in unzipped directory\n        unzipped_files = list(ZIP_DIR.iterdir())\n        filenames = [file.name for file in unzipped_files]\n        return (\"Download and extraction successful\", filenames)\n    except Exception as e:\n        return (f\"Error: {str(e)}\", [])\n\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    function scrapes webpage for hyperlinks saves as absolute URLs to CSV file. Parameters: url (str): relative URL webpage to scrape. base_url (str optional): base URL website to prepend to relative links. Defaults to 'https://www.example.com'. csv_file (str optional): filename for CSV file where links saved Defaults to 'scraped_data.csv'. Returns: int: number of unique absolute links scraped from webpage. Requirements: requests - urllib.parse.urljoin - bs4.BeautifulSoup - csv Examples: >>> task_func('/mywebpage') 5 >>> task_func('/anotherpage' base_url='https://www.different.com' csv_file='other_links.csv') 8\n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the webpage: {response.status_code}\")\n        return 0\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    # Find all the <a> tags in the HTML\n    links = soup.find_all(\"a\")\n\n    # Create a set to store unique absolute URLs\n    unique_links = set()\n\n    # Iterate over the <a> tags\n    for link in links:\n        # Get the href attribute of the <a> tag\n        href = link.get(\"href\")\n\n        # Check if the href attribute is not None\n        if href:\n            # Create an absolute URL from the href attribute\n            absolute_url = urljoin(base_url, href)\n\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Open the CSV file in write mode\n    with open(csv_file, \"w\", newline=\"\") as file:\n        # Create a CSV writer object\n        writer = csv.writer(file)\n\n        # Write the unique absolute URLs to the CSV file\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(unique_links)\n\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    function parses HTML table data from specified URL or local file stores it into SQLite database. function handles different scenarios for fetching, processing storing data. Parameters: - webpage_url (str): URL of webpage or local file path prefixed with \"file://\". - database_name (str): name of SQLite database file where data stored. Defaults to \"my_database.db\". Returns: - int: number of rows in parsed HTML table. Raises: - requests.RequestException: exception raised if network issue in accessing URL. includes scenarios like connection errors, timeouts HTTP errors. - sqlite3.DatabaseError: exception raised in issues connecting to or writing to SQLite database. includes issues like invalid database names, write permissions SQL execution errors. Notes: - function designed to replace table \"my_table\" in specified SQLite database with new data each time called. If HTML content not contain table or if table empty, function return 0, indicating no rows were parsed and stored. function relies on 'requests', 'lxml', 'pandas', 'sqlite3' libraries for operations. Requirements: - requests - lxml - pandas - sqlite3 Example: >>> num_rows = task_func(\"http://example.com/tabledata\") >>> print(f\"Number of rows parsed: {num_rows}\") Number of rows parsed: 5\n    \"\"\"\n    try:\n        # Fetch HTML content from URL or local file\n        if webpage_url.startswith(\"file://\"):\n            with open(webpage_url[7:], \"r\") as file:\n                html_content = file.read()\n        else:\n            response = requests.get(webpage_url)\n            response.raise_for_status()\n            html_content = response.text\n\n        # Parse HTML content to extract table data\n        tree = html.fromstring(html_content)\n        table = tree.xpath(\"//table\")[0]\n        rows = table.xpath(\".//tr\")\n        headers = [header.text_content().strip() for header in rows[0].xpath(\".//th\")]\n        data = []\n        for row in rows[1:]:\n            row_data = [cell.text_content().strip() for cell in row.xpath(\".//td\")]\n            data.append(row_data)\n\n        # Create DataFrame from parsed data\n        df = pd.DataFrame(data, columns=headers)\n\n        # Connect to SQLite database and replace table with new data\n        conn = sqlite3.connect(database_name)\n        df.to_sql(\"my_table\", conn, if_exists=\"replace\", index=False)\n\n        # Return number of rows in parsed HTML table\n        return len(df)\n\n    except requests.RequestException as e:\n        print(f\"Error fetching data from {webpage_url}: {e}\")\n        return 0\n\n    except sqlite3.DatabaseError as e:\n        print(f\"Error connecting to or writing to database {database_name}: {e}\")\n        return 0\n\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return 0\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\n\nIMAGE_PATH = \"image.png\"\n\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    Opens image file extracts text using OCR converts text encoding fallback to image comment processing. Raises ValueError: UnicodeDecodeError or LookupError occurs during conversion Parameters: filename (str): path to image file. Defaults to global variable 'IMAGE_PATH'. from_encoding (str): original encoding of extracted text or image comment. Default is 'cp1251'. to_encoding (str): target encoding for converted text or comment. Default is 'utf8'. Returns comment (str): text extracted from image or image comment converted to target encoding. If OCR extraction and comment processing fail returns empty string. Raises ValueError: incorrect encodings for text or comment conversion. Requirements: - codecs - PIL - pytesseract Example Assuming 'image.png' contains text '\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440' in Russian (encoded in cp1251) text successfully extracted by OCR. >>> text = task_func('image.png', 'cp1251', 'utf8') >>> print(text) '\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440' output is utf-8 encoded version of extracted text.\n    \"\"\"\n    try:\n        # Open image file\n        image = Image.open(filename)\n\n        # Extract text using OCR\n        text = pytesseract.image_to_string(image, lang='rus')\n\n        # Convert text encoding\n        text = text.encode(from_encoding).decode(to_encoding)\n\n        # Get image comment\n        comment = image.info.get('comment', '')\n\n        # Convert comment encoding\n        comment = comment.encode(from_encoding).decode(to_encoding)\n\n        # Return comment\n        return comment\n\n    except (UnicodeDecodeError, LookupError) as e:\n        raise ValueError(f\"Incorrect encodings for text or comment conversion: {e}\")\n\n    except Exception as e:\n        raise ValueError(f\"Error occurred during text extraction or comment processing: {e}\")\n\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    Fetches data from REST API URL processes for JSON parsing. process involves decoding and re-encoding data handling different encoding scenarios. function initiates HTTP GET request to specified URL with 5-second timeout retrieves response content in raw bytes. Parameters: - url (str): URL of REST API. Default is 'http://api.example.com/data'. - from_encoding (str, optional): original encoding of data. If None, encoding auto-detected. Default is None. to_encoding (str): target encoding format for data. Default is 'utf8'. Returns: - dict: JSON-parsed data after re-encoding. Returns empty dictionary if content empty. Raises ValueError: \"Unable to detect encoding for non-empty content\", if fails to detect non-empty response content. Requirements: - json - requests - chardet Example: >>> data = task_func('http://api.example.com/data') >>> print(data) {'key': 'value'} Example of expected output function sets timeout of 5 seconds for API request handles cases with empty content and undetectable encoding by returning empty dictionary or raising exception. decoding and re-encoding steps ensure compatibility with various data formats final JSON parsing.\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error fetching data from {url}: {e}\")\n        return {}\n\n    content = response.content\n    if not content:\n        return {}\n\n    if from_encoding is None:\n        encoding = chardet.detect(content)[\"encoding\"]\n        if encoding is None:\n            raise ValueError(\"Unable to detect encoding for non-empty content\")\n    else:\n        encoding = from_encoding\n\n    try:\n        decoded_content = content.decode(encoding)\n    except UnicodeDecodeError as e:\n        print(f\"Error decoding content with {encoding}: {e}\")\n        return {}\n\n    try:\n        data = json.loads(decoded_content)\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON content: {e}\")\n        return {}\n\n    try:\n        encoded_data = json.dumps(data, ensure_ascii=False).encode(to_encoding)\n    except UnicodeEncodeError as e:\n        print(f\"Error encoding data with {to_encoding}: {e}\")\n        return {}\n\n    try:\n        final_data = json.loads(encoded_data.decode(to_encoding))\n    except UnicodeDecodeError as e:\n        print(f\"Error decoding data with {to_encoding}: {e}\")\n        return {}\n\n    return final_data\n"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads CSV file processes date-related data. function performs key tasks checking for file's existence validating presence specified date column converting date values to datetime objects filtering rows based on current date sorting resulting data. function handles special cases like empty CSV file returning empty DataFrame raises exceptions for specific error scenarios like missing files or columns. Parameters: - csv_file_path (str): path to CSV file. FileNotFoundError raised if path invalid. - column_name (str): name of column containing date values. ValueError raised if column missing in CSV file. date_format (str, optional): format of date values in specified column. Defaults to '%Y-%m-%d'. Returns: - pandas - os - datetime.datetime - pandas.errors.EmptyDataError Raises: FileNotFoundError: If specified CSV file not found at given path. ValueError: If specified column not present in CSV file. Requirements: - pandas - os - datetime Example: >>> task_func('path/to/csvfile.csv', 'DateColumn') Date Value 0 2023-12-10 100 1 2023-12-11 150\n    \"\"\"\n    try:\n        # Check if file exists\n        if not os.path.exists(csv_file_path):\n            raise FileNotFoundError(f\"File not found: {csv_file_path}\")\n\n        # Read CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Check if specified column exists\n        if column_name not in df.columns:\n            raise ValueError(f\"Column '{column_name}' not found in CSV file.\")\n\n        # Convert date values to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n        # Filter rows based on current date\n        current_date = datetime.now().date()\n        df = df[df[column_name].dt.date == current_date]\n\n        # Sort resulting data\n        df = df.sort_values(by=column_name)\n\n        return df\n\n    except FileNotFoundError as e:\n        print(e)\n        return None\n\n    except ValueError as e:\n        print(e)\n        return None\n\n    except EmptyDataError as e:\n        print(\"CSV file is empty.\")\n        return None\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    function secures client socket using SSL/TLS sends back SHA256 hash of file requested by client. Parameters: - client_socket (socket.socket): client socket wrapped with SSL/TLS for secure communication cert_file (str): file path to SSL certificate for secure connection. key_file (str): file path to SSL key corresponding to certificate. buffer_size (int, optional): size of buffer used to receive data from client. Defaults to 1024 bytes. Returns: - str: SHA256 hash of requested file. If requested file not exist, returns 'File not found'. exception during processing error message returned. Requirements: - ssl - os - hashlib Note: function assumes client requests file by sending path function not handle opening or closing of client_socket Error handling basic might need to be expanded based on specific use cases. Example: >>> # Server setup >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) >>> server_socket.bind(('localhost', 443)) >>> server_socket.listen(5) >>> cert_file = \"path/to/certificate.crt\" >>> key_file = \"path/to/private.key\" >>> # Accept client connection >>> client_socket, addr = server_socket.accept() >>> Use task_func function to handle client request >>> file_hash = task_func(client_socket, cert_file, key_file) >>> print(\"Sent file hash:\", file_hash) >>> server_socket.close()\n    \"\"\"\n    try:\n        # Wrap client socket with SSL/TLS\n        ssl_socket = ssl.wrap_socket(client_socket, server_side=True, certfile=cert_file, keyfile=key_file)\n\n        # Receive file path from client\n        file_path = ssl_socket.recv(buffer_size).decode()\n\n        # Check if file exists\n        if not os.path.isfile(file_path):\n            return 'File not found'\n\n        # Calculate SHA256 hash of file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            for byte_block in iter(lambda: file.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n\n        # Send SHA256 hash back to client\n        ssl_socket.send(sha256_hash.hexdigest().encode())\n\n        # Close SSL/TLS connection\n        ssl_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return str(e)\n\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Run non-blocking echo server appends server current time to received data sends back to client handling exceptional conditions for each socket. Parameters: server_address (str): address for server listen on Default is 'localhost'. server_port (int): port for server listen on Default is 12345. buffer_size (int): buffer size for data reception. Default is 1024 bytes. run_duration (int): duration (in seconds) for server run. Default is 5 seconds. Returns: str: status message indicating server's operation run duration. Requirements: - socket - select - queue - datetime Example: >>> print(task_func()) 'Server started on localhost:12345. Ran for 5 seconds'\n    \"\"\"\n    # Create a TCP/IP socket\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n    # Bind the socket to the server address and port\n    server_socket.bind((server_address, server_port))\n\n    # Listen for incoming connections\n    server_socket.listen(5)\n\n    # Create a list to keep track of all the sockets\n    sockets_list = [server_socket]\n\n    # Create a dictionary to keep track of all the sockets' data\n    sockets_data = {}\n\n    # Create a dictionary to keep track of all the sockets' exceptional conditions\n    sockets_exceptions = {}\n\n    # Create a dictionary to keep track of all the sockets' last received data\n    sockets_last_received_data = {}\n\n    # Create a dictionary to keep track of all the sockets' last received time\n    sockets_last_received_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent data\n    sockets_last_sent_data = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets' last sent time\n    sockets_last_sent_time = {}\n\n    # Create a dictionary to keep track of all the sockets'"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\n\ndef task_func(client_socket):\n    \"\"\"\n    Receive message from client socket send as email via SMTP server. Parameters: client_socket (socket.socket): client socket message received. Returns: - None Requires working internet connection access to SMTP server function asks for sender's email recipient's email sender's email password for authentication. Requirements: - smtplib - email.message.EmailMessage - getpass Example >>> import socket >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT)) >>> server_socket.listen(5) >>> client_socket, addr = server_socket.accept() >>> task_func(client_socket)\n    \"\"\"\n    # Receive message from client socket\n    message = client_socket.recv(BUFFER_SIZE).decode()\n    # Send message as email via SMTP server\n    send_email(message)\n    # Close client socket\n    client_socket.close()\n\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes a CSV file containing text data generates histogram of ten most common words. function reads CSV file, expected to contain single column of text data. splits text into words creates histogram of frequency of top ten most common words, excluding predefined set of stopwords. resulting histogram can be either displayed on screen or saved to file. CSV file should have single column with header 'Text'. Each row under column should contain a text string. If CSV file not header, first column assumed to be text data. Parameters: - file_path (str): path to input CSV file. - save_path (str, optional): path where histogram plot will be saved. If not provided, plot displayed on screen. Returns: - matplotlib.axes.Axes: Axes object of plot if save_path not provided. Useful for further customization or display in notebooks - None: If save_path provided, plot saved to specified path, function returns None. Raises: - FileNotFoundError: If specified file_path not exist. raises FileNotFoundError with message indicating file path not found - Exception: For any other errors occur during function execution. In this case, error is printed to console, None returned. Requirements: - pandas - scikit-learn - matplotlib Notes: - function uses pandas for data manipulation, sklearn's CountVectorizer for text vectorization, matplotlib for plotting. - predefined list of stopwords used to filter out common but insignificant words from histogram. Examples: >>> ax = task_func('text_data.csv print(ax) Axes(0.125,0.11;0.775x0.77) result = task_func('text_data.csv 'output_plot.png print(result) None\n    \"\"\"\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n\n        # Check if 'Text' column exists\n        if 'Text' not in df.columns:\n            raise ValueError(\"CSV file should have 'Text' column\")\n\n        # Initialize CountVectorizer\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n\n        # Fit and transform text data\n        X = vectorizer.fit_transform(df['Text'])\n\n        # Get word counts\n        word_counts = X.sum(axis=0).tolist()[0]\n\n        # Get top ten words\n        top_words = sorted(zip(vectorizer.get_feature_names_out(), word_counts), key=lambda x: x[1], reverse=True)[:10]\n\n        # Create histogram\n        plt.figure(figsize=(10, 6))\n        plt.bar([word for word, count in top_words], [count for word, count in top_words])\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n\n        # Save or display plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.show()\n            return plt.gca()\n\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found\")\n        return None\n    except Exception as e:\n        print(f\"Error occurred: {e}\")\n        return None\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\n\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    Create DataFrame with combinations animals foods in 'animal:food' format. Parameters: animals (list str optional): list animal names. If not provided defaults to predefined list common animals including 'Dog' 'Cat' 'Elephant' 'Tiger' 'Lion' 'Zebra' 'Giraffe 'Bear 'Monkey 'Kangaroo'. foods (list str optional): list of food names. If not provided defaults to predefined list common foods including 'Meat' 'Fish' 'Grass' 'Fruits' 'Insects' 'Seeds' 'Leaves'. Returns: df (pandas.DataFrame): DataFrame each row represents unique animal from 'animals' list each column represents food item from 'foods' list. Each cell contains string in format 'animal:food'. Handling Special Cases: If 'animals' and 'foods' lists empty or not provided function returns empty DataFrame. If either 'animals or 'foods' list empty or not provided function uses predefined list for missing parameter. Requirements: - pandas - numpy - itertools Example: >>> animal_food_pairs = task_func(['Dog', 'Cat'], ['Meat', 'Fish']) >>> print(animal_food_pairs) Meat Fish 0 Dog:Meat Dog:Fish 1 Cat:Meat Cat:Fish function generates all possible combinations of provided 'animals' 'foods' using itertools.product resulting pairs shuffled randomly to ensure variety in DataFrame layout.\n    \"\"\"\n    if animals is None:\n        animals = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\n    if foods is None:\n        foods = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n    if not animals or not foods:\n        return pd.DataFrame()\n    combinations = list(itertools.product(animals, foods))\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    df['Animal:Food'] = df['Animal'] + ':' + df['Food']\n    df = df[['Animal:Food']]\n    return df\n"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates average time difference in seconds between each consecutive pair timestamps in given list after converting to specified timezone. Parameters: - time_strings (list of str): list of timestamp strings in format 'dd/mm/yy HH:MM:SS.fff'. timezone (str): timezone to timestamp strings should be converted. should be valid timezone string, e.g., 'America/New_York'. Returns: - float: mean (average) time difference in seconds between each consecutive timestamps. If less than two timestamps in list, function returns 0.0. Requirements: - datetime - pytz - numpy Notes function first converts each timestamp in list to specified timezone then calculates absolute time difference in seconds between each timestamps. If list contains less than two timestamps, function returns 0.0, no pairs to compare. If no time differences (e single timestamp after timezone conversion), also returns 0.0. function uses numpy's mean function to calculate average time difference. Example: >>> time_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123'] >>> mean_diff = task_func(time_strings, 'America/New_York') >>> print(mean_diff) 61.0\n    \"\"\"\n    # Convert time strings to datetime objects in specified timezone\n    timezones = pytz.timezone(timezone)\n    time_objects = [datetime.strptime(time, '%d/%m/%y %H:%M:%S.%f') for time in time_strings]\n    time_objects = [timezones.localize(time) for time in time_objects]\n\n    # Calculate time differences in seconds\n    time_diffs = np.diff(time_objects).astype('timedelta64[s]').astype(int)\n\n    # Calculate mean time difference\n    mean_diff = np.mean(time_diffs)\n\n    return mean_diff\n\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    Analyzes frequency of words in text after lowercasing removing punctuation splitting words plots top 10 common words. Parameters: text (str): input text analyzed. Returns: list: list tuples containing 10 common words counts Axes: matplotlib Axes object of bar chart. Requirements: - re - collections.Counter - matplotlib.pyplot Example: >>> common_words, ax = task_func(\"This sample text. text contains sample words like 'text' 'sample' 'words'.\") >>> print(common_words) [('sample', 3), ('text', 3), ('this', 2), ('words', 2), ('is', 1), ('a', 1) ('contains', 1) ('like', 1) ('and', 1)]\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    words = text.split()\n    word_counts = Counter(words)\n    common_words = word_counts.most_common(10)\n    ax = plt.bar([word for word, count in common_words], [count for word, count in common_words])\n    plt.xticks(rotation=45)\n    plt.show()\n    return common_words, ax\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n\ndef task_func(myString):\n    \"\"\"\n    Extracts URL from given string retrieves title web page from URL. If no valid URL found or URL not result in successful web page fetch, returns error message. Parameters: myString (str): string from to extract URL. Returns: str: title webpage at extracted URL if successful, otherwise error messages: - \"No valid URL found in provided string.\" \"Unable to fetch content of URL: {url}\" \"No title tag found in webpage.\" Requirements: - re - urllib.parse.urlparse - bs4.BeautifulSoup - requests Example: >>> task_func('Check this: https://www.google.com') 'Google' >>> task_func('No URL here') 'No valid URL found in provided string.' >>> task_func('Check this broken link: https://www.thisdoesnotexist12345.com') 'Unable to fetch content of URL: https://www.thisdoesnotexist12345.com'\n    \"\"\"\n    # Extract URL from string\n    url = re.search(r'(https?://[^\\s]+)', myString)\n    if not url:\n        return 'No valid URL found in provided string.'\n    \n    url = url.group(1)\n    \n    # Check if URL is valid\n    parsed_url = urlparse(url)\n    if not all([parsed_url.scheme, parsed_url.netloc]):\n        return 'No valid URL found in provided string.'\n    \n    # Fetch content of URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException:\n        return f'Unable to fetch content of URL: {url}'\n    \n    # Parse content of URL\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Extract title of webpage\n    title_tag = soup.find('title')\n    if not title_tag:\n        return 'No title tag found in webpage.'\n    \n    return title_tag.text.strip()\n\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses JSON string to find URL associated with specified key downloads file from URL saves with timestamped filename. filename format is '{unknown_key}_{timestamp}.txt' 'timestamp' formatted as '%Y%m%d%H%M%S%f' include date and time down to microseconds. file saved in specified directory or current working directory by default. Parameters: - json_data (str): JSON data as string expected to contain key directly linked to URL unknown_key (str): key used to extract URL from JSON data. save_dir (str, optional): directory to save downloaded file. If not specified file saved in current working directory. Defaults to None. Returns: str: absolute path of downloaded file reflecting where saved. Requirements: - json - requests - os - datetime.datetime Example: >>> json_str = '{\"unknown\": \"https://example.com/file.txt\"}' >>> file_path = task_func(json_str, 'unknown') >>> print(f\"Downloaded file saved at: {file_path}\")\n    \"\"\"\n    # Parse JSON string to dictionary\n    data = json.loads(json_data)\n\n    # Extract URL from JSON data using unknown_key\n    url = data.get(unknown_key)\n\n    # Check if URL exists\n    if url is None:\n        raise ValueError(f\"Key '{unknown_key}' not found in JSON data.\")\n\n    # Get current timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')[:-3]\n\n    # Create filename with timestamp\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n\n    # Create save directory if not specified\n    if save_dir is None:\n        save_dir = os.getcwd()\n\n    # Create absolute path for downloaded file\n    file_path = os.path.join(save_dir, filename)\n\n    # Download file from URL\n    response = requests.get(url)\n\n    # Check if file downloaded successfully\n    if response.status_code == 200:\n        # Save file to disk\n        with open(file_path, 'wb') as file:\n            file.write(response.content)\n        return file_path\n    else:\n        raise Exception(f\"Failed to download file from URL: {url}\")\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from URL or local file saves to specified file in JSON format. Parameters url (str): URL webpage scrape or local file path prefixed with 'file://' output_path (str): path where extracted phone numbers saved in JSON format. Returns list: list of strings each representing found phone number. Requirements: - bs4 - requests - re - json Example task_func('file:///path/to/local/file.txt', 'output.json') ['+1 (234) 567 8901', '+44 1234 567890']\n    \"\"\"\n    if url.startswith('file://'):\n        with open(url[7:], 'r') as f:\n            content = f.read()\n    else:\n        response = requests.get(url)\n        content = response.text\n\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    text = soup.get_text()\n\n    phone_numbers = re.findall(r'\\+?\\d{1,4}[-.\\s]?\\(?\\d{1,3}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}', text)\n\n    with open(output_path, 'w') as f:\n        json.dump(phone_numbers, f)\n\n    return phone_numbers\n"}
