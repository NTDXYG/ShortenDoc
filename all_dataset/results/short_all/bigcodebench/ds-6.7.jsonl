{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    \n    \"\"\"\n    ftp = ftplib.FTP(timeout=10)\n    ftp.connect(ftp_server, 21)\n    ftp.login(ftp_user, ftp_password)\n    ftp.cwd(ftp_dir)\n\n    files = ftp.nlst()\n    for file in files:\n        with open(file, 'wb') as f:\n            ftp.retrbinary('RETR '+ file, f.write)\n\n    ftp.quit()\n\n    return files\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    with open(commands_file_path, 'r') as commands_file:\n        reader = csv.reader(commands_file)\n        for row in reader:\n            command = row[0]\n            output_file_path = os.path.join(output_dir_path, row[1])\n            with open(output_file_path, 'w') as output_file:\n                subprocess.run(command, shell=True, stdout=output_file)\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    \n    \"\"\"\n    while True:\n        for proc in psutil.process_iter(['pid', 'name']):\n            if proc.info['name'] == process_name:\n                return f\"Process {process_name} is running with PID {proc.info['pid']}\"\n        time.sleep(1)\n"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\n    files = glob.glob(directory + '/*')\n\n    # \u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u30eb\u30fc\u30d7\u51e6\u7406\n    for file in files:\n        # \u30d5\u30a1\u30a4\u30eb\u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5834\u5408\u306f\u30b9\u30ad\u30c3\u30d7\n        if os.path.isdir(file):\n            continue\n\n        # \u30d5\u30a1\u30a4\u30eb\u540d\u304b\u3089\u62e1\u5f35\u5b50\u3092\u53d6\u5f97\n        _, ext = os.path.splitext(file)\n\n        # \u62e1\u5f35\u5b50\u304c.zip\u306e\u5834\u5408\u306f\u30b9\u30ad\u30c3\u30d7\n        if ext == '.zip':\n            continue\n\n        # \u30d5\u30a1\u30a4\u30eb\u540d\u306b.zip\u3092\u8ffd\u52a0\u3057\u3066zip\u30d5\u30a1\u30a4\u30eb\u540d\u3092\u4f5c\u6210\n        zip_file = file + '.zip'\n\n        # zip\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\n        with zipfile.ZipFile(zip_file, 'w') as zipf:\n            zipf.write(file)\n\n        # \u5143\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u524a\u9664\n        os.remove(file)\n"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u53bb\u9664\u6807\u70b9\u7b26\u53f7\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # 2. \u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5c0f\u5199\n    text = text.lower()\n\n    # 3. \u751f\u6210\u8bcd\u4e91\n    wordcloud = WordCloud(width=800, height=400, max_font_size=200, min_font_size=50).generate(text)\n\n    # 4. \u663e\u793a\u8bcd\u4e91\u56fe\u50cf\n    plt.figure(figsize=(10, 7))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, target_column):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    # 2. \u0421\u043e\u0437\u0434\u0430\u0439\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c RandomForestClassifier\n    model = RandomForestClassifier(n_estimators=100)\n\n    # 3. \u041e\u0431\u0443\u0447\u0438\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\n    model.fit(X, y)\n\n    # 4. \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    feature_importances = pd.DataFrame(model.feature_importances_,\n                                       index = X.columns,\n                                       columns=['importance']).sort_values('importance', ascending=False)\n\n    # 5. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    sns.barplot(x=feature_importances.index, y=feature_importances['importance'])\n    plt.xlabel('Features')\n    plt.ylabel('Importance')\n    plt.title('Feature importance from RandomForest')\n    plt.show()\n\n    return feature_importances\n\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n\n    class User(UserMixin):\n        def __init__(self, username, password):\n            self.id = username\n            self.password = password\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    users = [User('admin', generate_password_hash('password'))]\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        for user in users:\n            if user.id == user_id:\n                return user\n        return None\n\n    @app.route('/', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            for user in users:\n                if user.id == form.username.data:\n                    if user.check_password(form.password.data):\n                        login_user(user)\n                        return redirect(url_for('dashboard'))\n            return 'Invalid username or password'\n        return render_template('login.html', form"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    \n    \"\"\"\n    # Standardize the data\n    scaler = StandardScaler()\n    data_std = scaler.fit_transform(data[[column]])\n\n    # Calculate z-scores\n    z_scores = stats.zscore(data_std)\n\n    # Identify outliers\n    outliers = np.where(np.abs(z_scores) > outlier_z_score)\n\n    # Remove outliers\n    data_no_outliers = data.drop(data.index[outliers[0]])\n\n    return data_no_outliers\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    \n    \"\"\"\n    # 1. Load the data\n    df = pd.read_csv(data)\n\n    # 2. Preprocess the data\n    df = df.dropna()\n\n    # 3. Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(df)\n\n    # 4. Visualize the clusters\n    plt.scatter(df['x'], df['y'], c=kmeans.labels_)\n    plt.show()\n\n    # 5. Return the labels\n    return kmeans.labels_\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u6570\u636e\u9884\u5904\u7406\n    # 1.1 \u5220\u9664\u7f3a\u5931\u503c\n    data.dropna(inplace=True)\n\n    # 1.2 \u6570\u636e\u6807\u51c6\u5316\n    data_std = (data - data.min()) / (data.max() - data.min())\n\n    # 2. PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(data_std)\n\n    # 3. \u7ed8\u5236PCA\u7ed3\u679c\n    plt.figure(figsize=(8, 6))\n    plt.scatter(pca.components_[0], pca.components_[1], c='r')\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.title('PCA')\n    plt.grid(True)\n    plt.show()\n\n    # 4. \u8fd4\u56dePCA\u7ed3\u679c\n    return pca.components_\n"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n\n    # \uac01 \ud488\uc885\ubcc4\ub85c \uc0b0\uc810\ub3c4 \uadf8\ub9ac\uae30\n    sns.pairplot(df, hue='target')\n    plt.show()\n\n    # \uac01 \ud488\uc885\ubcc4\ub85c \uc0c1\uad00\uacc4\uc218 \uad6c\ud558\uae30\n    print(df.corr())\n\n    # \uac01 \ud488\uc885\ubcc4\ub85c \uc0c1\uad00\uad00\uacc4 \uc2dc\uac01\ud654\n    sns.heatmap(df.corr(), annot=True)\n    plt.show()\n"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n\n    # 1. Create a DataFrame with 1000 rows and 3 columns: 'A', 'B', 'C'\n    data = {\n        'A': [random.randint(0,100) for _ in range(1000)],\n        'B': [random.randint(0,100) for _ in range(1000)],\n        'C': [random.randint(0,100) for _ in range(1000)]\n    }\n    df = pd.DataFrame(data)\n\n    # 2. Create a new column 'D' where each element is the sum of 'A', 'B', and 'C'\n    df['D'] = df['A'] + df['B'] + df['C']\n\n    # 3. Create a new column 'E' where each element is the datetime of the row\n    df['E'] = [datetime.now() for _ in range(1000)]\n\n    # 4. Save the DataFrame to a CSV file\n    df.to_csv('data.csv', index=False)\n\n    # 5. Plot a histogram of 'D'\n    plt.hist(df['D'], bins=20)\n    plt.title('Histogram of D')\n    plt.xlabel('D')\n    plt.ylabel('Frequency')\n    plt.savefig('histogram.png')\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\n    # Task 1\n    print(data.head())\n    print(data.info())\n\n    # Task 2\n    print(data.describe())\n\n    # Task 3\n    correlation_matrix = data.corr().round(2)\n    sns.heatmap(data=correlation_matrix, annot=True)\n    plt.show()\n\n    # Task 4\n    sns.pairplot(data)\n    plt.show()\n\n    # Task 5\n    from sklearn.model_selection import train_test_split\n    X = data.drop('MEDV', axis=1)\n    y = data['MEDV']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n    print(X_train.shape)\n    print(X_test.shape)\n    print(y_train.shape)\n    print(y_test.shape)\n\n    # Task 6\n    from sklearn.linear_model import LinearRegression\n    lin_model = LinearRegression()\n    lin_model.fit(X_train, y_train)\n\n    # Task 7\n    from sklearn.metrics import mean_squared_error\n    y_train_predict = lin_model.predict(X_train)\n    rmse ="}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    \n    \"\"\"\n    # Set the index to a datetime object\n    df.index = pd.to_datetime(df.index)\n\n    # Decompose the time series\n    decomposition = seasonal_decompose(df, model=decomposition_model, freq=freq)\n\n    # Plot the original time series, the trend, the seasonality, and the residuals\n    plt.figure(figsize=(12,8))\n    decomposition.observed.plot()\n    plt.title('Observed')\n    plt.show()\n\n    plt.figure(figsize=(12,8))\n    decomposition.trend.plot()\n    plt.title('Trend')\n    plt.show()\n\n    plt.figure(figsize=(12,8))\n    decomposition.seasonal.plot()\n    plt.title('Seasonality')\n    plt.show()\n\n    plt.figure(figsize=(12,8))\n    decomposition.resid.plot()\n    plt.title('Residuals')\n    plt.show()\n\n    # Print the first few rows of the observed, trend, seasonality, and residuals\n    print(\"Observed:\")\n    print(decomposition.observed.head())\n    print(\"\\nTrend:\")\n    print(decomposition.trend.head())\n    print(\"\\nSeasonality:\")\n    print(decomposition.seasonal.head())\n    print(\"\\nResiduals:\")\n    print(decomposition.resid.head())\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    data = {\n        'date': date_range,\n        'value': [randint(0, 100) for _ in range(len(date_range))]\n    }\n    df = pd.DataFrame(data)\n    df.set_index('date', inplace=True)\n    return df\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    \n    \"\"\"\n    all_files = glob.glob(os.path.join(file_dir, '*'+file_ext))\n    li = []\n\n    for filename in all_files:\n        df = pd.read_csv(filename, index_col=None, header=0)\n        li.append(df)\n\n    frame = pd.concat(li, axis=0, ignore_index=True)\n    \n    return frame\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    for i in range(size):\n        my_list.append(randint(1,100))\n    return my_list\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the website\n    r = requests.get(url)\n\n    # Parse the content of the request with BeautifulSoup\n    soup = BeautifulSoup(r.content, 'html.parser')\n\n    # Find the table in the website\n    table = soup.find_all('table')[0] \n\n    # Parse the table into a pandas DataFrame\n    df = pd.read_html(str(table))[0]\n\n    return df\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # 2.\n    df['Month'] = df.index.month\n    df['Year'] = df.index.year\n\n    # 3.\n    df['Month-Year'] = df['Month'].astype(str) + '-' + df['Year'].astype(str)\n\n    # 4.\n    df_grouped = df.groupby('Month-Year').mean()\n\n    # 5.\n    df_grouped.plot(kind='bar', figsize=(15, 7))\n    plt.title('Average Monthly Sales')\n    plt.xlabel('Month-Year')\n    plt.ylabel('Average Sales')\n    plt.show()\n\n    # 6.\n    df_grouped_std = df.groupby('Month-Year').std()\n\n    # 7.\n    df_grouped_std.plot(kind='bar', figsize=(15, 7))\n    plt.title('Standard Deviation of Monthly Sales')\n    plt.xlabel('Month-Year')\n    plt.ylabel('Standard Deviation')\n    plt.show()\n\n    # 8.\n    df_grouped_max = df.groupby('Month-Year').max()\n\n    # 9.\n    df_grouped_max.plot(kind='bar', figsize=(15, 7))\n    plt.title('Maximum Monthly Sales')\n    plt.xlabel('Month-Year')\n    plt.ylabel('Maximum Sales')\n    plt.show()\n\n    # 10.\n    df_grouped_min = df.groupby('Month-Year').min()\n\n    # 11.\n    df_grouped_min.plot(kind='bar', fig"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    \n    \"\"\"\n    for ip in ip_range:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(1)\n        try:\n            s.connect((str(ip), port))\n            print(f\"{ip} is open\")\n        except:\n            print(f\"{ip} is closed\")\n        finally:\n            s.close()\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(log_file):\n    \"\"\"\n    \n    \"\"\"\n    # \u6b63\u5219\u8868\u8fbe\u5f0f\u6a21\u5f0f\n    pattern = r'(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}) - - \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\d+)'\n\n    # \u8bfb\u53d6\u65e5\u5fd7\u6587\u4ef6\n    with open(log_file, 'r') as file:\n        log_data = file.readlines()\n\n    # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u6570\u636e\n    data = [re.findall(pattern, line)[0] for line in log_data]\n\n    # \u521b\u5efa\u4e00\u4e2apandas DataFrame\n    df = pd.DataFrame(data, columns=['ip', 'date_time', 'request', 'status', 'size'])\n\n    # \u5c06\u65e5\u671f\u65f6\u95f4\u8f6c\u6362\u4e3adatetime\u5bf9\u8c61\n    df['date_time'] = df['date_time'].apply(lambda x: datetime.strptime(x, '%d/%b/%Y:%H:%M:%S %z'))\n\n    # \u5c06\u5927\u5c0f\u8f6c\u6362\u4e3a\u6574\u6570\n    df['size'] = df['size'].astype(int)\n\n    # \u5c06\u72b6\u6001\u8f6c\u6362\u4e3a\u6574\u6570\n    df['status'] = df['status'].astype(int)\n\n    return df\n\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    \n    \"\"\"\n    # \u7edf\u8ba1\u5b57\u7b26\u51fa\u73b0\u6b21\u6570\n    char_count = {}\n    for char in text:\n        if char in char_count:\n            char_count[char] += 1\n        else:\n            char_count[char] = 1\n\n    # \u6309\u51fa\u73b0\u6b21\u6570\u6392\u5e8f\n    sorted_char_count = sorted(char_count.items(), key=lambda x: x[1], reverse=True)\n\n    # \u7ed8\u5236\u67f1\u72b6\u56fe\n    labels, counts = zip(*sorted_char_count)\n    indexes = np.arange(len(labels))\n    plt.bar(indexes, counts, rwidth)\n    plt.xticks(indexes + rwidth / 2, labels)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\n\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # 1. Lowercase all the text\n    df['text'] = df['text'].apply(lambda x: x.lower())\n\n    # 2. Remove punctuation\n    df['text'] = df['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n\n    # 3. Remove numbers\n    df['text'] = df['text'].str.replace('\\d+', '')\n\n    # 4. Remove stopwords\n    stop = nltk.corpus.stopwords.words('english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n    # 5. Stemming\n    stemmer = nltk.PorterStemmer()\n    df['text'] = df['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n\n    return df\n\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    \n    \"\"\"\n    # Data Preprocessing\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'\\W', ' ', str(x)))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'\\s+', ' ', x))\n\n    # Vectorization\n    vectorizer = CountVectorizer(stop_words=STOPWORDS)\n    X = vector"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    \n    \"\"\"\n    # Create a GeoDataFrame\n    gdf = gpd.GeFrame(geometry=gpd.points_from_xy(x=np.random.uniform(low=dic['Lon'][0], high=dic['Lon'][1], size=len(cities)),\n                                                 y=np.random.uniform(low=dic['Lat'][0], high=dic['Lat'][1], size=len(cities))))\n    gdf['City'] = cities\n    \n    return gdf\n"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    \n    \"\"\"\n    set_seed(seed)\n\n    data = []\n    for city in cities:\n        for _ in range(10):\n            local_datetime = utc_datetime.astimezone(pytz.timezone(timezones[city]))\n            data.append({\n                'city': city,\n                'date': local_datetime.date(),\n                'time': local_datetime.time(),\n                'weather_condition': weather_conditions[randint(0, len(weather_conditions) - 1)]\n            })\n\n    df = pd.DataFrame(data)\n    df.set_index(['city', 'date', 'time'], inplace=True)\n\n    return df\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    df = pd.DataFrame(np.random.randint(0,100,size=(elements, 4)), columns=list('ABCD'))\n    df['E'] = df['A'] + df['B'] + df['C'] + df['D']\n    df = df.sort_values(by='E', ascending=False)\n    df.reset_index(drop=True, inplace=True)\n    return df\n\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    response = requests.get(url, headers=headers, stream=True)\n\n    if response.status_code == 200:\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(destination_directory, file_name)\n\n        with open(file_path, 'wb') as f:\n            f.write(response.content)\n\n        return file_path\n    else:\n        return None\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    \n    \"\"\"\n    # Set the seed\n    random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(low=range_low, high=range_high, size=image_size)\n\n    # Save the image\n    cv2.imwrite('random_image.jpg', image)\n\n    # Display the image\n    plt.imshow(image)\n    plt.show()\n\n    return image\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    \n    \"\"\"\n    # Load audio file\n    signal, sr = librosa.load(audio_file)\n\n    # Compute Short-Time Fourier Transform (STFT)\n    stft = librosa.stft(signal)\n\n    # Compute spectrogram\n    spectrogram = np.abs(stft) ** 2\n\n    # Compute Mel-Frequency Cepstral Coefficients (MFCCs)\n    mfccs = librosa.feature.mfcc(S=spectrogram, sr=sr, n_mfcc=M)\n\n    # Compute Chroma STFT\n    chroma_stft = librosa.feature.chroma_stft(S=spectrogram, sr=sr)\n\n    # Compute spectral contrast\n    spectral_contrast = librosa.feature.spectral_contrast(S=spectrogram, sr=sr)\n\n    # Compute Tonnetz\n    tonnetz = librosa.feature.tonnetz(S=spectrogram)\n\n    # Concatenate all features\n    features = np.concatenate((mfccs, chroma_stft, spectral_contrast, tonnetz), axis=0)\n\n    # Compute L-norm\n    l_norm = np.linalg.norm(features, ord=L)\n\n    # Compute N-norm\n    n_norm = np.linalg.norm(features, ord=N)\n\n    # Compute mean of features\n    mean_features = np.mean(features, axis=1)\n\n    # Compute standard deviation of features\n    std_features = np.std(features, axis=1)\n\n    # Compute median of features\n    median_features = np.median(features, axis=1)\n\n    # Compute N-th percentile of features\n    percentile_features = np.percentile(features, q=N, axis=1"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    print(f\"Original array:\\n{original}\")\n\n    # 2.\n    print(f\"Shape of the original array: {original.shape}\")\n\n    # 3.\n    print(f\"Data type of the original array: {original.dtype}\")\n\n    # 4.\n    print(f\"Mean of the original array: {np.mean(original)}\")\n\n    # 5.\n    print(f\"Standard deviation of the original array: {np.std(original)}\")\n\n    # 6.\n    print(f\"Minimum value in the original array: {np.min(original)}\")\n\n    # 7.\n    print(f\"Maximum value in the original array: {np.max(original)}\")\n\n    # 8.\n    print(f\"25th percentile of the original array: {np.percentile(original, 25)}\")\n\n    # 9.\n    print(f\"75th percentile of the original array: {np.percentile(original, 75)}\")\n\n    # 10.\n    plt.hist(original, bins='auto', alpha=0.7, rwidth=0.85)\n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of the original array')\n    plt.show()\n\n    # 11.\n    print(f\"Sorted array:\\n{np.sort(original)}\")\n\n    # 12.\n    print(f\"Transpose of the original array:\\n{original.T}\")\n\n    # 13.\n    print(f\"Flattened original array:\\n{original.flatten()}\")\n\n    # 14.\n    print(f\"Reshaped original array to 2x2:\\n{original.resh"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    # \u5bf9\u6570\u636e\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\n    min_max_scaler = preprocessing.MinMaxScaler()\n    original_scaled = min_max_scaler.fit_transform(original)\n\n    # \u8ba1\u7b97\u5747\u503c\n    mean_val = np.mean(original_scaled)\n    # \u8ba1\u7b97\u6807\u51c6\u5dee\n    std_val = np.std(original_scaled)\n\n    # \u8ba1\u7b97z-score\n    z_scores = [(x - mean_val) / std_val for x in original_scaled]\n\n    return z_scores\n\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    \n    \"\"\"\n    # \u5085\u91cc\u53f6\u53d8\u6362\n    fft_data = fftpack.fft(data)\n    fft_data = np.abs(fft_data)\n\n    # \u83b7\u53d6\u9891\u7387\n    freqs = fftpack.fftfreq(len(data), 1/sample_rate)\n\n    # \u7ed8\u5236\u9891\u8c31\u56fe\n    plt.figure(figsize=(10, 5))\n    plt.plot(freqs, fft_data)\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    pass\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bbe\u7f6eSMTP\u670d\u52a1\u5668\n    server = smtplib.SMTP(smtp_server, smtp_port)\n    server.starttls()\n    server.login(smtp_username, smtp_password)\n\n    # \u53d1\u9001\u90ae\u4ef6\n    msg = MIMEText('Hello, world')\n    msg['Subject'] = 'Hello'\n    msg['From'] = smtp_username\n    msg['To'] = 'recipient@example.com'\n    server.send_message(msg)\n    server.quit()\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u8bfb\u53d6\u6587\u4ef6\n    with open(os.path.join(directory, filename), 'r') as f:\n        text = f.read()\n\n    # 2. \u7edf\u8ba1\u8bcd\u9891\n    words = text.split()\n    word_counts = Counter(words)\n\n    # 3. \u5c06\u7ed3\u679c\u5199\u5165json\u6587\u4ef6\n    with open(os.path.join(directory, 'word_counts.json'), 'w') as f:\n        json.dump(word_counts, f)\n\n    return 'word_counts.json'\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    \n    \"\"\"\n    # Convert date column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set date as index\n    df.set_index('Date', inplace=True)\n\n    # Sort by date\n    df.sort_index(inplace=True)\n\n    # Calculate monthly averages\n    df_monthly = df.resample('M').mean()\n\n    # Calculate yearly averages\n    df_yearly = df.resample('Y').mean()\n\n    if plot:\n        # Plotting\n        fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n\n        # Plot monthly averages\n        ax[0].plot(df_monthly)\n        ax[0].set_title('Monthly Averages')\n\n        # Plot yearly averages\n        ax[1].plot(df_yearly)\n        ax[1].set_title('Yearly Averages')\n\n        plt.tight_layout()\n        plt.show()\n\n    return df_monthly, df_yearly\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    \n    \"\"\"\n    # Adding additional fields if any\n    fields = FIELDS + additional_fields\n\n    # Generating random marks for each student in each subject\n    data = {field: {student: random.randint(1, 100) for student in STUDENTS} for field in fields}\n\n    # Converting the dictionary to a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Adding a new column for the average marks of each student\n    df['Average'] = df.mean(axis=1)\n\n    # Sorting the DataFrame by the average marks in descending order\n    df.sort_values(by='Average', ascending=False, inplace=True)\n\n    # Resetting the index\n    df.reset_index(drop=True, inplace=True)\n\n    return df\n"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of dictionaries\n    people = [\n        {\n            'Name': f'Person{i}',\n            'Age': random.randint(18, 80),\n            'Height': random.randint(140, 200),\n            'Weight': random.randint(40, 150),\n        }\n        for i in range(PEOPLE_COUNT)\n    ]\n\n    # Write to CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=COLUMNS)\n        writer.writeheader()\n        for person in people:\n            writer.writerow(person)\n\n    # Read from CSV\n    with open(filename, 'r') as csvfile:\n        reader = csv.DictReader(csvfile)\n        ages = [int(row['Age']) for row in reader]\n        heights = [int(row['Height']) for row in reader]\n        weights = [int(row['Weight']) for row in reader]\n\n    # Print statistics\n    print(f'Average age: {mean(ages)}')\n    print(f'Average height: {mean(heights)}')\n    print(f'Average weight: {mean(weights)}')\n\n    # Remove file\n    os.remove(filename)\n\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\n    files = os.listdir(directory)\n\n    # \u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u65e5\u671f\u683c\u5f0f\n    date_pattern = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n\n    # \u904d\u5386\u6240\u6709\u6587\u4ef6\n    for file in files:\n        # \u5982\u679c\u6587\u4ef6\u540d\u5339\u914d\u65e5\u671f\u683c\u5f0f\n        if date_pattern.match(file):\n            # \u83b7\u53d6\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n            file_path = os.path.join(directory, file)\n            # \u5982\u679c\u6587\u4ef6\u662f\u6587\u4ef6\u5939\n            if os.path.isdir(file_path):\n                # \u83b7\u53d6\u6587\u4ef6\u5939\u7684\u521b\u5efa\u65f6\u95f4\n                ctime = os.path.getctime(file_path)\n                # \u5c06\u521b\u5efa\u65f6\u95f4\u8f6c\u6362\u4e3a\u65e5\u671f\u5bf9\u8c61\n                ctime = datetime.fromtimestamp(ctime)\n                # \u83b7\u53d6\u6587\u4ef6\u5939\u7684\u540d\u79f0\n                folder_name = file\n                # \u521b\u5efa\u65b0\u7684\u6587\u4ef6\u5939\u540d\u79f0\n                new_folder_name = f'{ctime.year}-{ctime.month:02d}-{ctime.day:02d}-{folder_name}'\n                # \u83b7\u53d6\u65b0\u7684\u6587\u4ef6\u5939\u7684\u7edd\u5bf9\u8def\u5f84\n                new_folder_path = os.path.join(directory, new_folder_name)\n                # \u91cd\u547d\u540d\u6587\u4ef6\u5939\n                os.rename(file_path, new_folder_path)\n\n"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\n\ndef task_func(file_list):\n    \"\"\"\n    \n    \"\"\"\n    for file in file_list:\n        print(f\"Processing {file}\")\n        subprocess.run([\"python\", \"main.py\", file])\n\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    \n    \"\"\"\n    # \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\n    file_list = glob.glob(directory_path + \"/*\")\n\n    # \u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u30eb\u30fc\u30d7\n    for file_path in file_list:\n        # \u30d5\u30a1\u30a4\u30eb\u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3063\u305f\u3089\u518d\u5e30\u7684\u306b\u51e6\u7406\n        if os.path.isdir(file_path):\n            task_func(file_path)\n        # \u30d5\u30a1\u30a4\u30eb\u304c\u30c6\u30ad\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb\u3060\u3063\u305f\u3089\u51e6\u7406\n        elif file_path.endswith(\".txt\"):\n            # \u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3044\u3066\u5185\u5bb9\u3092\u8868\u793a\n            with open(file_path, \"r\") as f:\n                print(f.read())\n"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u8ba1\u7b97\u6bcf\u5217\u7684\u5747\u503c\u3001\u4e2d\u4f4d\u6570\u3001\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c\n    mean_val = df[col].mean()\n    median_val = df[col].median()\n    min_val = df[col].min()\n    max_val = df[col].max()\n\n    # 2. \u6253\u5370\u7ed3\u679c\n    print(f\"{col}\u7684\u5747\u503c\u4e3a{mean_val}\")\n    print(f\"{col}\u7684\u4e2d\u4f4d\u6570\u4e3a{median_val}\")\n    print(f\"{col}\u7684\u6700\u5c0f\u503c\u4e3a{min_val}\")\n    print(f\"{col}\u7684\u6700\u5927\u503c\u4e3a{max_val}\")\n\n    # 3. \u7ed8\u5236\u76f4\u65b9\u56fe\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[col], kde=True, color='blue')\n    plt.title(f\"{col}\u7684\u76f4\u65b9\u56fe\")\n    plt.show()\n\n    # 4. \u7ed8\u5236\u7bb1\u7ebf\u56fe\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(df[col], color='blue')\n    plt.title(f\"{col}\u7684\u7bb1\u7ebf\u56fe\")\n    plt.show()\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(script_path):\n        print(f\"Script {script_path} does not exist.\")\n        return\n\n    if wait:\n        subprocess.call([sys.executable, script_path] + list(args))\n    else:\n        subprocess.Popen([sys.executable, script_path] + list(args), shell=True)\n\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n\n    # Clean the data\n    df = df.dropna(subset=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # Calculate the moving averages\n    df['SMA_50'] = df['Close'].rolling(window=50).mean()\n    df['SMA_200'] = df['Close'].rolling(window=200).mean()\n\n    # Create a column for the trading signals\n    df['Signal'] = 0.0  \n    df['Signal'][df['SMA_50'] > df['SMA_200']] = 1.0\n    df['Position'] = df['Signal'].diff()\n\n    # Plotting\n    fig, ax1 = plt.subplots()\n\n    color = 'tab:red'\n    ax1.set_xlabel('Date')\n    ax1.set_ylabel('Price', color=color)\n    ax1.plot(df['Close'], color=color)\n    ax1.tick_params(axis='y', labelcolor=color)\n\n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('Volume', color=color) \n    ax2.plot(df['Volume'], color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n\n    buy = df.loc[df['Position'] == 1.0]['Close']\n    sell = df.loc[df['Position'] == -1.0]['Close']\n    ax1.scatter(buy.index, buy, marker='^', color='g')"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef task_func(activities):\n    \"\"\"\n    \n    \"\"\"\n    # \u5c06\u65e5\u671f\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3adatetime\u5bf9\u8c61\n    activities = [(datetime.strptime(date, \"%Y-%m-%d\"), activity) for date, activity in activities]\n    # \u6309\u65e5\u671f\u6392\u5e8f\n    activities.sort()\n\n    # \u7edf\u8ba1\u6bcf\u5929\u7684\u6d3b\u52a8\u6b21\u6570\n    count_dict = defaultdict(int)\n    for date, activity in activities:\n        count_dict[activity] += 1\n\n    # \u7ed8\u5236\u56fe\u8868\n    plt.bar(count_dict.keys(), count_dict.values())\n    plt.xlabel('Activity')\n    plt.ylabel('Count')\n    plt.title('Daily Activity Count')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    files = os.listdir(src_dir)\n    random.shuffle(files)\n\n    for file in files:\n        if os.path.isfile(os.path.join(src_dir, file)):\n            shutil.copy(os.path.join(src_dir, file), dest_dir)\n\n    return \"Files have been shuffled and copied to the destination directory.\"\n\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    \n    \"\"\"\n    # \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\n    files = glob.glob(directory_path + '/*.xlsx')\n\n    # \u30d5\u30a1\u30a4\u30eb\u3054\u3068\u306b\u51e6\u7406\n    for file in files:\n        # \u30d5\u30a1\u30a4\u30eb\u540d\u304b\u3089\u62e1\u5f35\u5b50\u3092\u9664\u3044\u305f\u30d9\u30fc\u30b9\u540d\u3092\u53d6\u5f97\n        base_name = os.path.splitext(os.path.basename(file))[0]\n\n        # \u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\n        wb = load_workbook(filename=file)\n\n        # \u30b7\u30fc\u30c8\u540d\u3092\u53d6\u5f97\n        sheet_names = wb.sheetnames\n\n        # \u30b7\u30fc\u30c8\u3054\u3068\u306b\u51e6\u7406\n        for sheet_name in sheet_names:\n            # \u30b7\u30fc\u30c8\u3092\u30a2\u30af\u30c6\u30a3\u30d6\u306b\u3059\u308b\n            ws = wb[sheet_name]\n\n            # \u30bb\u30eb\u3054\u3068\u306b\u51e6\u7406\n            for row in ws.iter_rows():\n                for cell in row:\n                    # \u30bb\u30eb\u306e\u5024\u304c\u6587\u5b57\u5217\u3067\u3042\u308b\u3053\u3068\u3092\u78ba\u8a8d\n                    if isinstance(cell.value, str):\n                        # \u30bb\u30eb\u306e\u5024\u304b\u3089\u30e1\u30fc\u30eb\u30a2\u30c9\u30ec\u30b9\u3092\u62bd\u51fa\n                        email_list = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', cell.value)\n\n                        # "}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    \n    \"\"\"\n    # Generate time array\n    time = np.linspace(0, 1, sample_size)\n\n    # Generate signal\n    signal = np.sin(2 * np.pi * frequency * time)\n\n    return time, signal\n\n"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = 'smtp.gmail.com'\n    app.config['MAIL_PORT'] = 465\n    app.config['MAIL_USERNAME'] = 'your-email@gmail.com'\n    app.config['MAIL_PASSWORD'] = 'your-password'\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USE_SSL'] = True\n    mail = Mail(app)\n\n    return mail\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bfb\u53d6excel\u6587\u4ef6\n    df = pd.read_excel(os.path.join(excel_file_path, file_name))\n    \n    # \u68c0\u67e5\u5217\u662f\u5426\u5b58\u5728\n    if column_name not in df.columns:\n        return f\"\u5217 {column_name} \u4e0d\u5b58\u5728\"\n    \n    # \u8ba1\u7b97\u5217\u7684\u5747\u503c\n    mean_value = df[column_name].mean()\n    \n    # \u8ba1\u7b97\u5217\u7684\u6807\u51c6\u5dee\n    std_value = df[column_name].std()\n    \n    # \u8ba1\u7b97\u5217\u7684\u6700\u5c0f\u503c\n    min_value = df[column_name].min()\n    \n    # \u8ba1\u7b97\u5217\u7684\u6700\u5927\u503c\n    max_value = df[column_name].max()\n    \n    # \u8ba1\u7b97\u5217\u7684\u7a7a\u503c\u6570\u91cf\n    null_count = df[column_name].isnull().sum()\n    \n    # \u8ba1\u7b97\u5217\u7684\u552f\u4e00\u503c\u6570\u91cf\n    unique_count = df[column_name].nunique()\n    \n    # \u8fd4\u56de\u7ed3\u679c\n    return {\n        \"\u5747\u503c\": mean_value,\n        \"\u6807\u51c6\u5dee\": std_value,\n        \"\u6700\u5c0f\u503c\": min_value,\n        \"\u6700\u5927\u503c\": max_value,\n        \"\u7a7a\u503c\u6570\u91cf\": null_count,\n        \"\u552f\u4e00\u503c\u6570\u91cf\": unique_count\n    }\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Define the model\n    model = Sequential()\n    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=0.01)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=10)\n\n    # Plotting the loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.show()\n\n    # Plotting the accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='lower right')\n    plt.show()\n\n    return model\n"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Define the model\n    model = keras.Sequential([\n        keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n    # Evaluate the model\n    test_loss, test_acc = model.evaluate(X_test, y_test)\n    print('Test accuracy:', test_acc)\n\n    # Plot the training and validation accuracy\n    plt.plot(history.history['accuracy'], label='Training accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    # Plot the ROC curve\n    y_pred = model.predict(X_test)\n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='ROC"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load image\n    image = cv2.imread(image_path)\n    # Reshape image to 2D array\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(image)\n    # Get the cluster centers\n    cluster_centers = kmeans.cluster_centers_\n    # Replace each pixel with the color of the closest cluster center\n    image = np.array([cluster_centers[label] for label in kmeans.labels_]).astype('uint8')\n    # Reshape image back to original dimensions\n    image = image.reshape((image.shape[0] // n_clusters, n_clusters, 3))\n    # Save the image\n    cv2.imwrite('image_clustered.jpg', image)\n"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # 1.\n    data = np.column_stack((P, T))\n\n    # 2.\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(data)\n\n    # 3.\n    labels = kmeans.labels_\n\n    # 4.\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(P, T, c=labels)\n    ax.set_xlabel(\"P\")\n    ax.set_ylabel(\"T\")\n    plt.colorbar(scatter, ax=ax)\n\n    return labels, ax\n\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\n\ndef task_func(points, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    vor = Voronoi(points)\n\n    fig = voronoi_plot_2d(vor)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    for file in glob.glob(src_dir + '/*.' + ext):\n        shutil.copy(file, dest_dir)\n\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\n\ndef task_func(json_str):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u5c06json\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3aPython\u5b57\u5178\n    json_dict = json.loads(json_str)\n\n    # 2. \u4f7f\u7528pandas\u5c06\u5b57\u5178\u8f6c\u6362\u4e3aDataFrame\n    df = pd.DataFrame(json_dict)\n\n    # 3. \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6DataFrame\u4e2d\u7684\u6240\u6709\u7535\u8bdd\u53f7\u7801\n    phone_numbers = df.applymap(lambda x: re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', str(x)))\n\n    # 4. \u5c06\u63d0\u53d6\u7684\u7535\u8bdd\u53f7\u7801\u8f6c\u6362\u4e3a\u4e00\u4e2a\u5217\u8868\n    phone_numbers_list = phone_numbers.values.tolist()\n\n    # 5. \u5c06\u5217\u8868\u8f6c\u6362\u4e3a\u4e00\u4e2a\u5355\u4e00\u7684\u5217\u8868\n    flat_list = [item for sublist in phone_numbers_list for item in sublist]\n\n    return flat_list\n\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    try:\n        start_time = time.time()\n        process = subprocess.Popen([\"python\", script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        while True:\n            if time.time() - start_time > timeout:\n                os.kill(process.pid, 9)\n                return {\"status\": \"timeout\", \"output\": None}\n            if process.poll() is not None:\n                break\n            time.sleep(0.1)\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n            return {\"status\": \"error\", \"output\": stderr.decode()}\n        else:\n            return {\"status\": \"success\", \"output\": stdout.decode()}\n    except Exception as e:\n        return {\"status\": \"error\", \"output\": str(e)}\n\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n\n    # Generate random data\n    data = pd.DataFrame({\n        \"Category\": np.random.choice(CATEGORIES, N),\n        \"Value\": np.random.normal(10, 2, N)\n    })\n\n    return data\n\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    time_index = pd.date_range(start=start_time, end=end_time, freq=step)\n    data = trend + np.random.normal(size=len(time_index))\n    df = pd.DataFrame(data, index=time_index, columns=['value'])\n    return df\n\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(random_seed)\n\n    # Create a dataframe with the given epoch_milliseconds\n    df = pd.DataFrame(\n        {\n            \"timestamp\": [\n                datetime.fromtimestamp(epoch_milliseconds // 1000)\n                for _ in range(epoch_milliseconds)\n            ],\n            \"product\": [random.choice(products) for _ in range(epoch_milliseconds)],\n            \"quantity\": [random.randint(1, 100) for _ in range(epoch_milliseconds)],\n        }\n    )\n\n    # Group by product and sum the quantity\n    df_grouped = df.groupby(\"product\").sum().reset_index()\n\n    return df_grouped\n\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    \n    \"\"\"\n    # \u5c06json\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3apandas DataFrame\n    df = pd.read_json(json_str)\n\n    # \u521b\u5efa\u4e00\u4e2a\u65b0\u7684ExcelWriter\u5bf9\u8c61\n    writer = pd.ExcelWriter(filename, engine='xlwt')\n\n    # \u5c06DataFrame\u5199\u5165Excel\u6587\u4ef6\n    df.to_excel(writer, sheet_name=sheet_name)\n\n    # \u4fdd\u5b58\u5e76\u5173\u95ed\u6587\u4ef6\n    writer.save()\n    writer.close()\n\n    return os.path.exists(filename)\n\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(random_seed)\n\n    # Generate a date range\n    date_range = pd.date_range(start=datetime.now() - timedelta(days=days_in_past), end=datetime.now())\n\n    # Generate random data\n    data = {\n        'date': date_range,\n        'value': [random.randint(0, 100) for _ in range(days_in_past)]\n    }\n\n    # Create a dataframe\n    df = pd.DataFrame(data)\n\n    # Plot the data\n    sns.lineplot(x='date', y='value', data=df)\n\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(random_seed)\n\n    # Generate dates\n    today = datetime.now()\n    dates = pd.date_range(today - pd.DateOffset(days=days_in_past), periods=days_in_past)\n\n    # Generate stock data\n    stock_data = pd.DataFrame(index=dates)\n    for stock in stock_names:\n        # Generate random stock prices\n        prices = np.random.rand(days_in_past) * 100\n        stock_data[stock] = prices\n\n    return stock_data\n\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    \n    \"\"\"\n    df1 = pd.read_csv(file_path1, delimiter=delimiter, quotechar=quotechar)\n    df2 = pd.read_csv(file_path2, delimiter=delimiter, quotechar=quotechar)\n\n    df1.sort_values(by=list(df1.columns), inplace=True)\n    df2.sort_values(by=list(df2.columns), inplace=True)\n\n    df1.reset_index(drop=True, inplace=True)\n    df2.reset_index(drop=True, inplace=True)\n\n    df1.to_csv('sorted_file1.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n    df2.to_csv('sorted_file2.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n\n    with open('sorted_file1.csv', 'r') as file1, open('sorted_file2.csv', 'r') as file2:\n        diff = list(ndiff(file1.readlines(), file2.readlines()))\n\n    diff_lines = [i for i in range(len(diff)) if diff[i].startswith('- ') or diff[i].startswith('+ ')]\n\n    return diff_lines\n\n"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    print(f\"\\n{column.capitalize()}:\\n\")\n    print(f\"Mean: {data[column].mean()}\")\n    print(f\"Median: {data[column].median()}\")\n    print(f\"Mode: {data[column].mode()[0]}\")\n    print(f\"Standard Deviation: {data[column].std()}\")\n    print(f\"Variance: {data[column].var()}\")\n\n    # 2.\n    plt.figure(figsize=(10, 6))\n    plt.hist(data[column], bins=30, color='c', edgecolor='black', alpha=0.7)\n    plt.title(f\"Histogram of {column.capitalize()}\")\n    plt.xlabel(column.capitalize())\n    plt.ylabel('Frequency')\n    plt.show()\n\n    # 3.\n    plt.figure(figsize=(10, 6))\n    plt.boxplot(data[column])\n    plt.title(f\"Boxplot of {column.capitalize()}\")\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    print(f\"\\n{column.capitalize()}:\\n\")\n    print(f\"Mean: {data[column].mean()}\")\n    print(f\"Median: {data[column].median()}\")\n    print(f\"Mode: {data[column].mode()[0]}\")\n    print(f\"Standard Deviation: {data[column].std()}\")\n    print(f\"Variance: {data[column].var()}\")\n\n    # 2.\n    plt.figure(figsize=(10, 6))\n    plt.hist(data[column], bins=30, color='c', edgecolor='black', alpha=0.7)\n    plt.title(f\"Histogram of {column.capitalize()}\")\n    plt.xlabel(column.capitalize())\n    plt.ylabel('Frequency')\n    plt.show()\n\n    # 3.\n    plt.figure(figsize=(10, 6))\n    plt.boxplot(data[column])\n    plt.title(f\"Boxplot of {column.capitalize()}\")\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    \n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    df = pd.DataFrame(data)\n\n    # 1. How many unique users are there?\n    unique_users = df['user_id'].nunique()\n    print(f\"There are {unique_users} unique users.\")\n\n    # 2. What is the average session duration?\n    avg_session_duration = df['session_duration'].mean()\n    print(f\"The average session duration is {avg_session_duration}.\")\n\n    # 3. What is the most common browser used?\n    most_common_browser = df['browser'].mode()[0]\n    print(f\"The most common browser used is {most_common_browser}.\")\n\n    # 4. What is the average number of searches per user?\n    avg_searches_per_user = df['searches'].mean()\n    print(f\"The average number of searches per user is {avg_searches_per_user}.\")\n\n    # 5. What is the most common operating system used?\n    most_common_os = df['os'].mode()[0]\n    print(f\"The most common operating system used is {most_common_os}.\")\n\n    # 6. What is the average number of searches per session?\n    df['searches_per_session'] = df['searches'] / df['session_duration']\n    avg_searches_per_session = df['searches_per_session'].mean()\n    print(f\"The average number of searches per session is {avg_searches_per_session}.\")\n\n    # 7. What is the most common country of origin?\n    most_common_country = df['country'].mode()[0]\n    print(f\"The most common country of origin is {most_common_country}.\")\n\n    # 8. What is the average number of searches per country?\n   "}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Load data\n    df = pd.read_csv(file_path)\n\n    # Count the number of unique values in each column\n    unique_counts = df.nunique()\n\n    # Print the counts\n    print(unique_counts)\n\n    # Count the number of missing values in each column\n    missing_counts = df.isnull().sum()\n\n    # Print the counts\n    print(missing_counts)\n\n    # Plot the counts of unique values in each column\n    unique_counts.plot(kind='bar')\n    plt.ylabel('Count')\n    plt.title('Count of Unique Values in Each Column')\n    plt.show()\n\n    # Plot the counts of missing values in each column\n    missing_counts.plot(kind='bar')\n    plt.ylabel('Count')\n    plt.title('Count of Missing Values in Each Column')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    \n    # 2.\n    df_monthly = df.resample('M').sum()\n    \n    # 3.\n    df_monthly['year'] = df_monthly.index.year\n    df_monthly['month'] = df_monthly.index.month\n    df_monthly.set_index(['year', 'month'], inplace=True)\n    \n    # 4.\n    df_monthly_pivot = df_monthly.pivot_table(values='value', index='year', columns='month', aggfunc='sum')\n    \n    # 5.\n    heatmap_ax = sns.heatmap(df_monthly_pivot)\n    \n    # 6.\n    df_yearly = df.resample('Y').sum()\n    df_yearly['year'] = df_yearly.index.year\n    df_yearly.set_index('year', inplace=True)\n    \n    # 7.\n    counter = Counter(df_yearly['value'])\n    \n    return counter, heatmap_ax\n\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, bins=4):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u0420\u0430\u0441\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0435.\n    mean_std_df = df.agg(['mean', 'std'])\n\n    # 2. \u0420\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043d\u0430 4 \u0440\u0430\u0432\u043d\u044b\u0435 \u0447\u0430\u0441\u0442\u0438 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0443 'A'.\n    df_parts = np.array_split(df, bins)\n\n    # 3. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0435 'B'.\n    unique_values_counts = [df_part['B'].nunique() for df_part in df_parts]\n\n    # 4. \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439.\n    mean_unique_values = np.mean(unique_values_counts)\n    std_unique_values = np.std(unique_values_counts)\n\n    # 5. \u0421\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c 1000 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b \u0438\u0437 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0441 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438, \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c\u0438 \u043d\u0430 \u0448\u0430\u0433\u0435 4.\n    random_numbers = norm.rvs(mean_unique_values, std_unique_values, 1000)\n\n    # 6. \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b.\n    plt.hist(random_numbers, bins=30, edgecolor='black')\n    plt.show()\n\n    return mean_std_df, random_numbers\n\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\n\ndef task_func(a, b):\n    \"\"\"\n    \n    \"\"\"\n    return a + b\n\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u5c06\u65e5\u671f\u5217\u8f6c\u6362\u4e3adatetime\u683c\u5f0f\n    data['date'] = pd.to_datetime(data['date'])\n\n    # 2. \u5c06\u65e5\u671f\u5217\u8bbe\u7f6e\u4e3a\u7d22\u5f15\n    data.set_index('date', inplace=True)\n\n    # 3. \u6309\u6708\u5bf9\u6570\u636e\u8fdb\u884c\u6c47\u603b\uff0c\u5e76\u8ba1\u7b97\u6bcf\u4e2a\u6708\u7684\u5e73\u5747\u503c\n    monthly_data = data.resample('M').mean()\n\n    # 4. \u7ed8\u5236\u6bcf\u4e2a\u6708\u7684\u5e73\u5747\u503c\n    monthly_data.plot(kind='line', figsize=(10, 5))\n\n    # 5. \u6dfb\u52a0\u6807\u9898\u548c\u6807\u7b7e\n    plt.title('\u6bcf\u6708\u5e73\u5747\u503c')\n    plt.xlabel('\u65e5\u671f')\n    plt.ylabel('\u5e73\u5747\u503c')\n\n    # 6. \u663e\u793a\u56fe\u5f62\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    data['Date'] = pd.to_datetime(data['Date'])\n    data.set_index('Date', inplace=True)\n\n    # 2.\n    data['Month'] = data.index.month\n    data['Year'] = data.index.year\n\n    # 3.\n    data['Month-Year'] = data['Year'].astype(str) + '-' + data['Month'].astype(str)\n\n    # 4.\n    data['Month-Year'] = data['Month-Year'].astype('category')\n\n    # 5.\n    data['Month-Year'] = data['Month-Year'].cat.set_categories(['2018-1', '2018-2', '2018-3', '2018-4', '2018-5', '2018-6', '2018-7', '2018-8', '2018-9', '2018-10', '2018-11', '2018-12'], ordered=True)\n    data.sort_values(by='Month-Year', inplace=True)\n\n    # 6.\n    data['Sales_Last_Month'] = data['Sales'].shift(1)\n\n    # 7.\n    data['Sales_Last_Month'].fillna(0, inplace=True)\n\n    # 8.\n    data['Sales_Last_Month'] = data['Sales_Last_Month'].astype(int)\n\n    # 9.\n    data['Sales_Last_Month_Category'] = pd.qcut(data['Sales_Last_Month'], q=3, labels=['Low', 'Medium', 'High'])\n\n    # 10.\n    data['Sales_Last_Month_Category'] = data['Sales_Last_Month_Category'].astype('category')\n\n    # 1"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    \n    \"\"\"\n    x = np.linspace(0, 10, array_length)\n    y = 3*np.sin(x) + 0.5*x**2 + noise_level*np.random.normal(size=array_length)\n    return x, y\n\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    \n    \"\"\"\n    with open(csv_file, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        next(reader)  # Skip the header\n        data = [row for row in reader]\n\n    # Flatten the list of lists\n    flattened_data = [item for sublist in data for item in sublist]\n\n    # Normalize the data\n    normalized_data = [unicodedata.normalize('NFKD', item).encode('ascii', 'ignore').decode() for item in flattened_data]\n\n    # Count the occurrences of each item\n    counter = Counter(normalized_data)\n\n    # Get the 10 most common items\n    most_common = counter.most_common(10)\n\n    # Create a bar chart\n    labels, values = zip(*most_common)\n    plt.bar(labels, values)\n    plt.xlabel('Items')\n    plt.ylabel('Count')\n    plt.title('10 Most Common Items')\n    plt.show()\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef task_func(size=1000):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u0430\u0441\u0441\u0438\u0432\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b\n    np.random.seed(0)\n    data = np.random.normal(size=size)\n\n    # 2. \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    std = np.std(data)\n    var = np.var(data)\n\n    # 3. \u0412\u044b\u0432\u043e\u0434 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\n    print(f\"Mean: {mean}\")\n    print(f\"Median: {median}\")\n    print(f\"Mode: {mode}\")\n    print(f\"Standard deviation: {std}\")\n    print(f\"Variance: {var}\")\n\n    # 4. \u0413\u0440\u0430\u0444\u0438\u043a \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b\n    plt.hist(data, bins=30, density=True)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # Generate AES key\n    aes_key = get_random_bytes(16)\n\n    # Encrypt AES key with RSA\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Encrypt some data with AES\n    cipher = AES.new(aes_key, AES.MODE_EAX)\n    data = \"Hello, World!\"\n    ciphertext, tag = cipher.encrypt_and_digest(data.encode())\n\n    # Store encrypted AES key and ciphertext\n    with open(\"encrypted_aes_key.bin\", \"wb\") as file:\n        file.write(encrypted_aes_key)\n\n    with open(\"ciphertext.bin\", \"wb\") as file:\n        [file.write(x) for x in (cipher.nonce, tag, ciphertext)]\n\n    # Decryption\n    with open(\"encrypted_aes_key.bin\", \"rb\") as file:\n        encrypted_aes_key = file.read()\n\n    with open(\"ciphertext.bin\", \"rb\") as file:\n        nonce, tag, ciphertext = [file.read(x) for x in (16, 16, -1)]\n\n    # Decrypt AES key with RSA\n    aes_key = rsa.decrypt(encrypted_aes_key, privkey)\n\n    # Decrypt data with AES\n    cipher = AES.new(aes_key, AES.MODE_EAX, nonce=nonce)\n    data = cipher.decrypt_and_verify(ciphertext, tag)\n\n    print(data.decode())\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\n    with open(file_path, 'r') as f:\n        content = f.read()\n\n    # \u751f\u6210RSA\u5bc6\u94a5\u5bf9\n    (pubkey, privkey) = rsa.newkeys(512)\n\n    # \u4f7f\u7528RSA\u52a0\u5bc6\u5185\u5bb9\n    encrypted_content = rsa.encrypt(content.encode(), pubkey)\n\n    # \u751f\u6210AES\u5bc6\u94a5\n    aes_key = os.urandom(32)\n\n    # \u4f7f\u7528AES\u52a0\u5bc6\u5185\u5bb9\n    cipher = Cipher(algorithms.AES(aes_key), modes.ECB())\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_content = padder.update(encrypted_content) + padder.finalize()\n    encrypted_content = encryptor.update(padded_content) + encryptor.finalize()\n\n    # \u4f7f\u7528RSA\u52a0\u5bc6AES\u5bc6\u94a5\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # \u5c06\u52a0\u5bc6\u5185\u5bb9\u548c\u52a0\u5bc6\u7684AES\u5bc6\u94a5\u8f6c\u6362\u4e3abase64\u5b57\u7b26\u4e32\n    encrypted_content_b64 = b64encode(encrypted_content).decode()\n    encrypted_aes_key_b64 = b64encode(encrypted_aes_key).decode()\n\n    return encrypted_content_b64, encrypted_aes_key_b64\n\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf-8')\n    doc = pq(html)\n\n    # \u63d0\u53d6\u6570\u636e\n    items = doc('.table-striped tr').items()\n    data_list = []\n    for item in items:\n        data = {\n            'date': item('td:nth-child(1)').text(),\n            'open': item('td:nth-child(2)').text(),\n            'high': item('td:nth-child(3)').text(),\n            'low': item('td:nth-child(4)').text(),\n            'close': item('td:nth-child(5)').text(),\n            'volume': item('td:nth-child(6)').text(),\n            'amount': item('td:nth-child(7)').text(),\n        }\n        data_list.append(data)\n\n    # \u8f6c\u6362\u4e3a DataFrame\n    df = pd.DataFrame(data_list)\n\n    # \u5c06\u65e5\u671f\u8f6c\u6362\u4e3a\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\n    df['date'] = pd.to_datetime(df['date'])\n\n    # \u5c06\u5f00\u76d8\u4ef7\u3001\u6700\u9ad8\u4ef7\u3001\u6700\u4f4e\u4ef7\u3001\u6536\u76d8\u4ef7\u3001\u6210\u4ea4\u91cf\u3001\u6210\u4ea4\u91d1\u989d\u8f6c\u6362\u4e3a\u6d6e\u70b9\u6570\n    df[['open', 'high', 'low', 'close', 'volume', 'amount']] = df[['open', 'high', 'low', 'close', 'volume', 'amount']].apply(pd.to_numeric, errors='coerce')\n\n    return df\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for i in range(hours):\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        data = {sensor: randint(1, 100) for sensor in SENSORS}\n        data['Timestamp'] = timestamp\n\n        with open(f'{output_dir}/data_{i}.csv', 'w', newline='') as file:\n            writer = csv.DictWriter(file, fieldnames=data.keys())\n            writer.writeheader()\n            writer.writerow(data)\n\n        print(f'Wrote data_{i}.csv')\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if not exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = generate_data(hours)\n\n    # Save data to csv\n    save_to_csv(data, output_dir)\n\n    # Load data from csv\n    data_loaded = load_from_csv(output_dir)\n\n    # Plot data\n    plot_data(data_loaded)\n\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = generate_data(hours)\n\n    # Write data to CSV\n    write_to_csv(data, output_dir)\n\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    \n    \"\"\"\n    return (goals * 100) - (penalties * PENALTY_COST)\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\n\ndef task_func(rows):\n    \"\"\"\n    \n    \"\"\"\n    df = pd.DataFrame(rows, columns=COLUMNS)\n    df['Column3'] = df['Column3'].apply(lambda x: x*2)\n    df['Column4'] = df['Column4'].apply(lambda x: x*3)\n    df['Column5'] = df['Column5'].apply(lambda x: x*4)\n    return df\n\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\ndef task_func(num_students):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random scores for each student\n    scores = sample(range(0, 100), num_students)\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame(scores, columns=['Scores'])\n\n    # Calculate the mean score\n    mean_score = df['Scores'].mean()\n\n    # Calculate the median score\n    median_score = df['Scores'].median()\n\n    # Calculate the mode score\n    mode_score = df['Scores'].mode()[0]\n\n    # Calculate the standard deviation\n    std_dev = df['Scores'].std()\n\n    # Calculate the variance\n    variance = df['Scores'].var()\n\n    # Return the results\n    return mean_score, median_score, mode_score, std_dev, variance\n\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\n\ndef task_func(array, target_value):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    array = np.array(array)\n    # 2. \n    array_sorted = np.sort(array)\n    # 3. \n    array_sorted_reversed = array_sorted[::-1]\n    # 4. \n    array_sorted_reversed_target = array_sorted_reversed - target_value\n    # 5. \n    array_sorted_reversed_target_abs = np.abs(array_sorted_reversed_target)\n    # 6. \n    min_index = np.argmin(array_sorted_reversed_target_abs)\n    # 7. \n    closest_value = array_sorted_reversed[min_index]\n    # 8. \n    closest_value_index = np.where(array == closest_value)\n    # 9. \n    return closest_value, closest_value_index\n\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    \n    \"\"\"\n    # Vectorize the texts\n    vectorizer = TfidfVectorizer(\n        preprocessor=lambda x: ALPHANUMERIC.sub(' ', x),\n        stop_words=STOPWORDS,\n        token_pattern='\\S+',\n        max_df=0.8,\n        min_df=3,\n    )\n    tfidf = vectorizer.fit_transform(texts)\n\n    # Perform NMF\n    nmf = NMF(n_components=num_topics, random_state=42, alpha=.1, l1_ratio=.5).fit(tfidf)\n\n    # Extract and return the topics\n    feature_names = vectorizer.get_feature_names_out()\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        topics.append(\" \".join([feature_names[i] for i in topic.argsort()[:-num_topics - 1:-1]]))\n    return topics\n\n"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    \n    \"\"\"\n    # Lowercase, replace non-alphanumeric characters, and tokenize\n    tokenized_texts = [nltk.word_tokenize(text.lower()) for text in texts]\n\n    # Remove stopwords\n    if stopwords is not None:\n        tokenized_texts = [[token for token in text if token not in stopwords] for text in tokenized_texts]\n\n    return tokenized_texts\n\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    with open(path, 'r') as f:\n        data = json.load(f)\n\n    # Convert to pandas dataframe\n    df = pd.DataFrame(data)\n\n    # Save the dataframe to a csv file\n    df.to_csv(os.path.splitext(path)[0] + '.csv', index=False)\n\n    # Remove the json file\n    os.remove(path)\n\n    # Move the csv file to a new directory\n    shutil.move(os.path.splitext(path)[0] + '.csv', 'csv_files')\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # \u5b9a\u4e49\u6587\u4ef6\u8def\u5f84\n    file_path = os.path.join(os.getcwd(), 'data.csv')\n\n    # \u5b9a\u4e49\u6570\u636e\n    data = [\n        ['\u59d3\u540d', '\u5e74\u9f84', '\u751f\u65e5'],\n        ['\u5f20\u4e09', '25', '1996-01-01'],\n        ['\u674e\u56db', '30', '1991-02-02'],\n        ['\u738b\u4e94', '35', '1986-03-03'],\n        ['\u8d75\u516d', '40', '1981-04-04'],\n    ]\n\n    # \u5199\u5165\u6570\u636e\n    with open(file_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        for row in data:\n            writer.writerow(row)\n\n    # \u8bfb\u53d6\u6570\u636e\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            print(row)\n\n    # \u8ffd\u52a0\u6570\u636e\n    new_data = [\n        ['\u59d3\u540d', '\u5e74\u9f84', '\u751f\u65e5'],\n        ['\u8d75\u4e03', '45', '1976-05-05'],\n        ['\u94b1\u516b', '50', '1971-06-06'],\n        ['\u5b59\u4e5d', '55', '1966-07-07'],\n        ['\u5468\u5341', '60', '1961-08-08'],\n    ]\n    with open(file_path, 'a', newline='') as f:\n        writer = csv.writer(f)\n        for row in new_data:\n            writer.writerow(row)\n\n    # \u8bfb\u53d6\u6570\u636e\n    with open(file_path, 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            print(row)\n"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Open the URL\n    html = urllib.request.urlopen(url)\n\n    # Parse the HTML\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find the table\n    table = soup.find('table', {'class': 'wikitable'})\n\n    # Find all rows in the table\n    rows = table.find_all('tr')\n\n    # Open the CSV file\n    with open(CSV_FILE_PATH, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file)\n\n        # Loop through each row\n        for row in rows:\n            # Find all columns in the row\n            cols = row.find_all('td')\n\n            # If there are columns, write them to the CSV file\n            if cols:\n                cols = [col.text.strip() for col in cols]\n                writer.writerow(cols)\n\n    print(f'Scraped data saved to {CSV_FILE_PATH}')\n\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into train and test sets\n    X = data.drop(columns=[target_column])\n    y = data[target_column]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    predictions = model.predict(X_test)\n\n    # Calculate the mean squared error\n    mse = np.mean((y_test - predictions)**2)\n\n    return mse\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \"\"\"\n    \n    \"\"\"\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate random dates\n    start_date = datetime.datetime(start_year, 1, 1)\n    end_date = datetime.datetime(end_year, 1, 1)\n    time_between_dates = end_date - start_date\n    days_between_dates = time_between_dates.days\n    random_dates = np.array([start_date + datetime.timedelta(days=np.random.randint(days_between_dates)) for _ in range(1000)])\n\n    # Generate random names\n    random_latin_names = np.random.choice(latin_names, size=1000)\n    random_other_names = np.random.choice(other_names, size=1000)\n    random_names = np.where(np.random.randint(2, size=1000)==0, random_latin_names, random_other_names)\n\n    # Generate random emails\n    random_emails = np.array([f\"{name}@{email_domain}\" for name in random_names])\n\n    # Generate random phone numbers\n    random_phone_numbers = np.array([f\"+1{str(np.random.randint(100000000, 1000000000))}\" for _ in range(1000)])\n\n    # Generate random addresses\n    random_addresses = np.array([f"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    \n    \"\"\"\n    # Load data\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Process data\n    result = process_data(data)\n\n    # Save data\n    with open(output_file, 'w') as f:\n        json.dump(result, f)\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    \n    \"\"\"\n    # \u521b\u5efa\u76ee\u6807\u76ee\u5f55\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # \u904d\u5386\u6e90\u76ee\u5f55\n    for key, value in kwargs.items():\n        if value is not None:\n            # \u6e90\u6587\u4ef6\u8def\u5f84\n            source_file = Path(key)\n            # \u76ee\u6807\u6587\u4ef6\u8def\u5f84\n            target_file = Path(target_dir, source_file.name)\n            # \u590d\u5236\u6587\u4ef6\n            shutil.copy2(source_file, target_file)\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    \"\"\"\n    \n    \"\"\"\n\n    # Check if directory exists\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory {directory} does not exist.\")\n\n    # Get all files in directory\n    files = os.listdir(directory)\n\n    # Filter files by pattern\n    files = [file for file in files if re.match(pattern, file)]\n\n    # Sort files\n    files.sort()\n\n    # Initialize list to store data\n    data = []\n\n    # Loop over files\n    for file in files:\n\n        # Get file path\n        file_path = os.path.join(directory, file)\n\n        # Open file\n        with open(file_path, 'r') as f:\n\n            # Read file\n            reader = csv.reader(f)\n\n            # Loop over rows\n            for i, row in enumerate(reader):\n\n                # Skip header\n                if i == 0:\n                    continue\n\n                # Append row to data\n                data.append(row)\n\n    return data\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    \n    \"\"\"\n    regex = re.compile(pattern)\n    for filename in os.listdir(directory):\n        match = regex.match(filename)\n        if match:\n            base_name = match.group(1)\n            zip_files = [filename for filename in os.listdir(directory) if filename.startswith(base_name)]\n            with zipfile.ZipFile(os.path.join(directory, base_name + '.zip'), 'w') as zipf:\n                for f in zip_files:\n                    zipf.write(os.path.join(directory, f), f)\n            for f in zip_files:\n                os.remove(os.path.join(directory, f))\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    \n    \"\"\"\n    # Get list of files\n    files = glob.glob(pattern)\n\n    # Create archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Move each file to archive directory\n    for file in files:\n        try:\n            subprocess.run(['mv', file, ARCHIVE_DIR], check=True)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to move file {file} to archive directory: {e}\")\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the CSV file exists\n    if not os.path.isfile(csv_file_path):\n        return \"CSV file not found\"\n\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip the header\n        data = list(reader)\n\n    # Calculate the total points for each team\n    team_points = {}\n    for row in data:\n        team, team_goals, team_penalties = row\n        team_goals = int(team_goals)\n        team_penalties = int(team_penalties)\n        team_points[team] = (team_goals * goals) + (team_penalties * penalties)\n\n    # Find the team with the most points\n    max_points = max(team_points.values())\n    winning_teams = [team for team, points in team_points.items() if points == max_points]\n\n    return winning_teams\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    for file_name in os.listdir(source_dir):\n        if re.search(file_pattern, file_name):\n            shutil.copy2(os.path.join(source_dir, file_name), target_dir)\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    \n    \"\"\"\n    # Normalize text\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Split text into words\n    words1 = text1.split()\n    words2 = text2.split()\n\n    # Count word occurrences\n    counter1 = Counter(words1)\n    counter2 = Counter(words2)\n\n    # Calculate similarity\n    intersection = set(counter1.keys()) & set(counter2.keys())\n    union = set(counter1.keys()) | set(counter2.keys())\n    similarity = float(len(intersection)) / len(union)\n\n    return similarity\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \uc21c\uc5f4 \uc0dd\uc131\n    perms = list(permutations(numbers, len(numbers)))\n\n    # 2. \uc21c\uc5f4\uc744 \uc22b\uc790\ub85c \ubcc0\ud658\n    nums = [reduce(lambda x, y: x*10+y, perm) for perm in perms]\n\n    # 3. \uc18c\uc218 \ud310\ubcc4\n    prime_nums = [num for num in nums if is_prime(num)]\n\n    return max(prime_nums)\n"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    \n    \"\"\"\n    # Check if source directory exists\n    if not os.path.isdir(SOURCE_DIR):\n        warnings.warn(f\"Source directory {SOURCE_DIR} does not exist.\")\n        return\n\n    # Check if destination directory exists\n    if not os.path.isdir(DEST_DIR):\n        warnings.warn(f\"Destination directory {DEST_DIR} does not exist.\")\n        return\n\n    # Get all files with specified extensions\n    files = []\n    for ext in EXTENSIONS:\n        files.extend(glob.glob(os.path.join(SOURCE_DIR, f\"*.{ext}\")))\n\n    # Copy files to destination directory\n    for file in files:\n        shutil.copy2(file, DEST_DIR)\n\n    print(f\"Copied {len(files)} files from {SOURCE_DIR} to {DEST_DIR}\")\n\n"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u6570\u636e\u6e05\u6d17\n    # \u5220\u9664\u7f3a\u5931\u503c\n    data.dropna(inplace=True)\n\n    # \u5220\u9664\u91cd\u590d\u503c\n    data.drop_duplicates(inplace=True)\n\n    # 2. \u6570\u636e\u8f6c\u6362\n    # \u5c06\u5206\u7c7b\u53d8\u91cf\u8f6c\u6362\u4e3a\u865a\u62df\u53d8\u91cf\n    data = pd.get_dummies(data)\n\n    # 3. \u7279\u5f81\u7f29\u653e\n    # \u4f7f\u7528z-score\u6807\u51c6\u5316\n    data = pd.DataFrame(zscore(data), columns=data.columns)\n\n    # \u4f7f\u7528MinMaxScaler\u8fdb\u884c\u7f29\u653e\n    scaler = MinMaxScaler()\n    data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n\n    return data\n\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    \n    \"\"\"\n    # Convert data_list to pandas DataFrame\n    df = pd.DataFrame(data_list, columns=['letter', 'number', 'float'])\n\n    # Create a new column 'square' which is the square of the 'number' column\n    df['square'] = df['number']**2\n\n    # Create a new column 'cube' which is the cube of the 'number' column\n    df['cube'] = df['number']**3\n\n    # Create a new column 'sqrt' which is the square root of the 'float' column\n    df['sqrt'] = np.sqrt(df['float'])\n\n    # Create a new column 'cbrt' which is the cube root of the 'float' column\n    df['cbrt'] = np.cbrt(df['float'])\n\n    # Create a new column 'rounded' which is the rounded value of the 'float' column\n    df['rounded'] = df['float'].round()\n\n    # Create a new column 'rounded_2' which is the rounded value of the 'float' column to 2 decimal places\n    df['rounded_2'] = df['float'].round(2)\n\n    # Create a new column 'rounded_3' which is the rounded value of the 'float' column to 3 decimal places\n    df['rounded_3'] = df['float'].round(3)\n\n    # Create a new column 'factorial' which is the factorial of the 'number' column\n    df['factorial'] = df['number'].apply(np.math.factorial)\n\n    # Create a new column 'is_even' which is True if the 'number' column is even and False if it is odd\n    df['is_even'] = df['number'].apply(lambda x: x % 2 == "}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the columns exist in the dataframe\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both of the columns do not exist in the dataframe\")\n\n    # Create a contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n    # Return the chi-square statistic, p-value, degrees of freedom and expected frequencies\n    return chi2, p, dof, expected\n\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    results = []\n    for _ in range(rolls):\n        results.append(random.choice(NUMBERS))\n\n    return results\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(source_dir):\n        return 'Source directory does not exist'\n\n    if not os.path.exists(target_dir):\n        return 'Target directory does not exist'\n\n    if not re.match(r'.*\\.zip$', archive_name):\n        return 'Archive name should have .zip extension'\n\n    archive_path = os.path.join(target_dir, archive_name)\n\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, arcname=file)\n\n    return 'Archive created successfully'\n\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # 2.\n    df['Month'] = df.index.month\n    df['Year'] = df.index.year\n\n    # 3.\n    df['Month-Year'] = df['Month'].astype(str) + '-' + df['Year'].astype(str)\n\n    # 4.\n    df_grouped = df.groupby('Month-Year').mean()\n\n    # 5.\n    X = df_grouped.index.values.reshape(-1,1)\n    Y = df_grouped['Value'].values.reshape(-1,1)\n\n    linear_regressor = LinearRegression()  # create object for the class\n    linear_regressor.fit(X, Y)  # perform linear regression\n    Y_pred = linear_regressor.predict(X)  # make predictions\n\n    # 6.\n    plt.scatter(X, Y)\n    plt.plot(X, Y_pred, color='red')\n    plt.show()\n\n    # 7.\n    df_grouped['Trend'] = Y_pred\n\n    return df_grouped\n"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate z-scores\n    df['z_score'] = zscore(df['value'])\n\n    # Filter out outliers\n    df = df[np.abs(df['z_score']) <= z_threshold]\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['date'], df['value'], label='Filtered Data')\n    plt.plot(df['date'], z_threshold*np.ones(len(df['date'])), '--', label='Z-threshold')\n    plt.plot(df['date'], -z_threshold*np.ones(len(df['date'])), '--')\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    plt.title('Filtered Data with Z-score threshold')\n    plt.legend()\n    plt.show()\n    \n    return df\n<jupyter_output>\n<empty_output>\n<jupyter_text>\nTesting the function\n<jupyter_code>\nimport pandas as pd\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    df_copy = df.copy()\n\n    # 2.\n    df_copy.drop(['Unnamed: 0'], axis=1, inplace=True)\n\n    # 3.\n    df_copy.rename(columns={'Country or region': 'Country'}, inplace=True)\n\n    # 4.\n    df_copy.set_index('Country', inplace=True)\n\n    # 5.\n    df_copy.replace(to_replace='..', value=None, inplace=True)\n\n    # 6.\n    df_copy.fillna(value=0, inplace=True)\n\n    # 7.\n    df_copy = df_copy.astype('float64')\n\n    # 8.\n    df_copy.drop(['Low income', 'Lower middle income', 'Upper middle income', 'High income'], axis=1, inplace=True)\n\n    # 9.\n    df_copy.sort_values(by='2020', ascending=False, inplace=True)\n\n    # 10.\n    df_copy.loc['World'] = df_copy.sum(numeric_only=True)\n\n    # 11.\n    df_copy.loc['World', '2020'] = df_copy.loc[df_copy.index != 'World', '2020'].sum()\n\n    # 12.\n    df_copy.loc['World', '2019'] = df_copy.loc[df_copy.index != 'World', '2019'].sum()\n\n    # 13.\n    df_copy.loc['World', '2018'] = df_copy.loc[df_copy.index != 'World', '2018'].sum()\n\n    # 14.\n    df_copy.loc['World', '201"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n\n    # 2.\n    series = df['#Passengers']\n    series.plot(kind='line')\n    plt.show()\n\n    # 3.\n    model = ARIMA(series, order=(5,1,0))\n    model_fit = model.fit()\n    residuals = pd.DataFrame(model_fit.resid)\n\n    # 4.\n    residuals.plot(kind='kde')\n    plt.show()\n\n    # 5.\n    residuals_values = residuals.values\n    return residuals_values, plt.gca()\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u7edf\u8ba1\u6bcf\u4e2a\u5b57\u7b26\u7684\u51fa\u73b0\u6b21\u6570\n    char_counter = Counter(word)\n\n    # 2. \u7edf\u8ba1\u6bcf\u4e2a\u5143\u97f3\u5b57\u6bcd\u7684\u51fa\u73b0\u6b21\u6570\n    vowel_counter = Counter(char for char in word if char in 'aeiou')\n\n    # 3. \u7edf\u8ba1\u6bcf\u4e2a\u8f85\u97f3\u5b57\u6bcd\u7684\u51fa\u73b0\u6b21\u6570\n    consonant_counter = Counter(char for char in word if char in string.ascii_lowercase.replace('aeiou', ''))\n\n    # 4. \u7edf\u8ba1\u6bcf\u4e2a\u5b57\u7b26\u7684\u51fa\u73b0\u6b21\u6570\u662f\u5426\u4e3a\u5076\u6570\n    even_counter = Counter(char for char in word if char_counter[char] % 2 == 0)\n\n    # 5. \u7edf\u8ba1\u6bcf\u4e2a\u5143\u97f3\u5b57\u6bcd\u7684\u51fa\u73b0\u6b21\u6570\u662f\u5426\u4e3a\u5076\u6570\n    even_vowel_counter = Counter(char for char in word if vowel_counter[char] % 2 == 0)\n\n    # 6. \u7edf\u8ba1\u6bcf\u4e2a\u8f85\u97f3\u5b57\u6bcd\u7684\u51fa\u73b0\u6b21\u6570\u662f\u5426\u4e3a\u5076\u6570\n    even_consonant_counter = Counter(char for char in word if consonant_counter[char] % 2 == 0)\n\n    return {\n        'char_counter': dict(char_counter),\n        'vowel_counter': dict(vowel_counter),\n        'consonant_counter': dict(consonant_counter),\n        'even_counter': dict(even_counter),\n        'even_vowel_counter': dict(even_vowel_counter),\n        'even_consonant_counter': dict(even_consonant_counter),\n    }\n\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    \n    \"\"\"\n    # Create date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create dataframe\n    df = pd.DataFrame(date_range, columns=['Date'])\n\n    # Add random categories\n    df['Category'] = np.random.choice(categories, size=periods)\n\n    # Add random sales\n    df['Sales'] = np.random.randint(100, 1000, size=periods)\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    for category in categories:\n        ax.plot(df[df['Category'] == category]['Date'], df[df['Category'] == category]['Sales'], label=category)\n\n    ax.legend(title='Categories')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales over time by category')\n\n    plt.show()\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    \n    \"\"\"\n    # Create date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create dataframe with date range\n    df = pd.DataFrame(date_range, columns=['Date'])\n    \n    # Create a new column 'Week'\n    df['Week'] = df['Date'].dt.week\n    \n    # Create a new column 'Month'\n    df['Month'] = df['Date'].dt.month\n    \n    # Create a new column 'Year'\n    df['Year'] = df['Date'].dt.year\n    \n    # Create a new column 'Day of Week'\n    df['Day of Week'] = df['Date'].dt.dayofweek\n    \n    # Create a new column 'Day of Month'\n    df['Day of Month'] = df['Date'].dt.day\n    \n    # Create a new column 'Day of Year'\n    df['Day of Year'] = df['Date'].dt.dayofyear\n    \n    # Create a new column 'Is Weekend'\n    df['Is Weekend'] = df['Date'].dt.weekday >= 5\n    \n    # Create a new column 'Is Holiday'\n    df['Is Holiday'] = np.where(df['Date'].isin(sales_data['Date']), True, False)\n    \n    # Create a new column 'Sales'\n    df['Sales'] = np.where(df['Date'].isin(sales_data['Date']), sales_data['Sales'], 0)\n    \n    # Create a new column 'Previous Sales'\n    df['Previous Sales'] = df['Sales'].shift(1)\n    \n    # Create a new column 'Average Sales'\n    df['Average Sales'] = df['Sales'].expanding().mean()\n    \n    # Create"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    tasks = random.choices(task_list, k=n_tasks)\n    employees = random.choices(employees, k=n_tasks)\n    dates = [datetime.now().strftime(\"%Y-%m-%d\") for _ in range(n_tasks)]\n    df = pd.DataFrame({\"Task\": tasks, \"Employee\": employees, \"Date\": dates})\n    return df\n\n"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n\ndef task_func(mystrings, text):\n    \"\"\"\n    \n    \"\"\"\n    # 1.1\n    print(f\"1.1: {len(mystrings)}\")\n\n    # 1.2\n    print(f\"1.2: {len(text)}\")\n\n    # 1.3\n    print(f\"1.3: {len(set(text))}\")\n\n    # 1.4\n    print(f\"1.4: {len(set(mystrings))}\")\n\n    # 1.5\n    print(f\"1.5: {len(re.findall(r'\\b\\w+\\b', text))}\")\n\n    # 1.6\n    print(f\"1.6: {len(re.findall(r'\\b\\w+\\b', mystrings))}\")\n\n    # 1.7\n    print(f\"1.7: {Counter(text)}\")\n\n    # 1.8\n    print(f\"1.8: {Counter(mystrings)}\")\n\n    # 1.9\n    print(f\"1.9: {text.count(' ')}\")\n\n    # 1.10\n    print(f\"1.10: {text.count('\\n')}\")\n\n    # 1.11\n    print(f\"1.11: {text.count('.')}\")\n\n    # 1.12\n    print(f\"1.12: {text.count(',')}\")\n\n    # 1.13\n    print(f\"1.13: {text.count('!')}\")\n\n    # 1.14\n    print(f\"1.14: {text.count('?')}\")\n\n    # 1.15\n    print(f\"1.15: {text.count(';')}\")\n\n    # 1.16\n    print(f\"1.16: {text.count(':')}\")\n\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    \n    \"\"\"\n    # \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u5185\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u89a7\u3092\u53d6\u5f97\n    file_list = glob.glob(source_directory + '/*')\n\n    # \u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5\n    for file in file_list:\n        # \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u53d6\u5f97\n        file_name = Path(file).name\n        # \u79fb\u52d5\u5148\u306e\u30d1\u30b9\u3092\u4f5c\u6210\n        target_path = os.path.join(target_directory, file_name)\n        # \u30d5\u30a1\u30a4\u30eb\u306e\u79fb\u52d5\n        os.rename(file, target_path)\n\n    # zip\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210\n    with zipfile.ZipFile(zip_name, 'w') as myzip:\n        for file in file_list:\n            myzip.write(file, arcname=Path(file).name)\n\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u68c0\u67e5\u6e90\u76ee\u5f55\u662f\u5426\u5b58\u5728\n    if not os.path.exists(source_directory):\n        print(f\"\u6e90\u76ee\u5f55 {source_directory} \u4e0d\u5b58\u5728\")\n        return 1\n\n    # 2. \u68c0\u67e5\u76ee\u6807\u76ee\u5f55\u662f\u5426\u5b58\u5728\uff0c\u5982\u679c\u4e0d\u5b58\u5728\u5219\u521b\u5efa\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    # 3. \u904d\u5386\u6e90\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\n    for file in os.listdir(source_directory):\n        # 4. \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u4e3a.docx\u6587\u4ef6\n        if file.endswith(\".docx\"):\n            # 5. \u8bfb\u53d6.docx\u6587\u4ef6\n            doc = docx.Document(os.path.join(source_directory, file))\n\n            # 6. \u63d0\u53d6\u6587\u672c\n            text = [p.text for p in doc.paragraphs]\n\n            # 7. \u5c06\u6587\u672c\u5199\u5165\u65b0\u7684.txt\u6587\u4ef6\n            with open(os.path.join(target_directory, file.replace(\".docx\", \".txt\")), \"w\") as f:\n                f.write(\"\\n\".join(text))\n\n    return 0\n\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u6570\u636e\u6e05\u6d17\n    df = df.dropna()\n\n    # 2. \u7279\u5f81\u5de5\u7a0b\n    df['month'] = df['date'].apply(lambda x: x.split('-')[1])\n    df['day'] = df['date'].apply(lambda x: x.split('-')[2])\n    df['weekday'] = df['date'].apply(lambda x: x.split('-')[0])\n\n    # 3. \u6570\u636e\u8f6c\u6362\n    scaler = MinMaxScaler()\n    df[['pollution']] = scaler.fit_transform(df[['pollution']])\n\n    return df\n\n"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u68c0\u67e5\u76ee\u5f55\u662f\u5426\u5b58\u5728\n    if not os.path.exists(directory_path):\n        print(f\"\u76ee\u5f55 {directory_path} \u4e0d\u5b58\u5728\")\n        return\n\n    # 2. \u83b7\u53d6\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\n    files = os.listdir(directory_path)\n\n    # 3. \u904d\u5386\u6587\u4ef6\uff0c\u83b7\u53d6\u6587\u4ef6\u7684\u521b\u5efa\u65f6\u95f4\n    for file in files:\n        file_path = os.path.join(directory_path, file)\n        if os.path.isfile(file_path):\n            creation_time = os.path.getctime(file_path)\n            creation_time = datetime.fromtimestamp(creation_time, tz=timezone.utc)\n            print(f\"\u6587\u4ef6 {file} \u7684\u521b\u5efa\u65f6\u95f4\u4e3a {creation_time}\")\n\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data into a pandas DataFrame\n    df = pd.DataFrame(json_data)\n\n    # Calculate the population density for each country\n    df['population_density'] = df['population'] / df['area']\n\n    # Sort the data by population density in descending order\n    df = df.sort_values('population_density', ascending=False)\n\n    # Save the sorted data to a CSV file\n    df.to_csv(os.path.join(output_dir, file_name), index=False)\n\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    result = {}\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            for predicate in predicates:\n                if re.search(predicate, file):\n                    file_path = os.path.join(root, file)\n                    if predicate in result:\n                        result[predicate].append(file_path)\n                    else:\n                        result[predicate] = [file_path]\n    return result\n\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\n\ndef task_func(hex_string):\n    \"\"\"\n    \n    \"\"\"\n    # Convert hex to base64\n    base64_string = base64.b64encode(binascii.unhexlify(hex_string))\n    return base64_string\n\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Download file\n    print(\"Downloading file from: \", url)\n    urllib.request.urlretrieve(url, \"downloaded_file\")\n\n    # Check if file exists\n    if not os.path.exists(\"downloaded_file\"):\n        print(\"File does not exist.\")\n        return\n\n    # Check if file is empty\n    if os.path.getsize(\"downloaded_file\") == 0:\n        print(\"File is empty.\")\n        return\n\n    # Check MD5 checksum\n    md5_checksum = hashlib.md5(open(\"downloaded_file\", 'rb').read()).hexdigest()\n    if md5_checksum != EXPECTED_MD5_CHECKSUM:\n        print(\"MD5 checksum does not match.\")\n        return\n\n    # Compress file\n    print(\"Compressing file...\")\n    with tarfile.open(TARGET_TAR_FILE, \"w:gz\") as tar:\n        tar.add(\"downloaded_file\")\n\n    print(\"File compressed successfully.\")\n\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\n\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    \n    \"\"\"\n    # \u30d5\u30a1\u30a4\u30eb\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\n    file_name = url.split('/')[-1]\n    urllib.request.urlretrieve(url, file_name)\n\n    # CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\n    with open(file_name, 'r') as f:\n        reader = csv.DictReader(f)\n        data = [row for row in reader]\n\n    # \u6307\u5b9a\u306e\u5217\u306e\u5024\u3092\u30ab\u30a6\u30f3\u30c8\n    counter = collections.Counter(row[column_name] for row in data)\n\n    # CSV\u30d5\u30a1\u30a4\u30eb\u306b\u66f8\u304d\u8fbc\u307f\n    with open(csv_file_path, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(['column_name', 'count'])\n        for key, value in counter.items():\n            writer.writerow([key, value])\n\n    # \u30d5\u30a1\u30a4\u30eb\u306e\u524a\u9664\n    os.remove(file_name)\n\n"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u53d1\u9001\u8bf7\u6c42\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf-8')\n\n    # \u89e3\u6790html\n    html = etree.HTML(html)\n\n    # \u63d0\u53d6\u6570\u636e\n    data = []\n    for tr in html.xpath('//table[@class=\"table table-striped table-bordered\"]/tr'):\n        tds = tr.xpath('./td')\n        if len(tds) > 0:\n            data.append({\n                'rank': tds[0].text,\n                'name': tds[1].text,\n                'score': tds[2].text,\n            })\n\n    # \u8f6c\u6362\u4e3aDataFrame\n    df = pd.DataFrame(data)\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf8')\n    words = re.findall(r'\\w+', html)\n    counter = Counter(words)\n    most_common = counter.most_common(10)\n    words, counts = zip(*most_common)\n    plt.bar(words, counts)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        if not os.path.exists(download_path):\n            os.makedirs(download_path)\n        file_name = url.split(\"/\")[-1]\n        file_path = os.path.join(download_path, file_name)\n        if not os.path.exists(file_path):\n            print(f\"Downloading {file_name} from {url}\")\n            response = requests.get(url, stream=True)\n            with open(file_path, 'wb') as f:\n                f.write(response.content)\n        else:\n            print(f\"{file_name} already exists in {download_path}\")\n        return file_path\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n        return None\n\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\n\ndef task_func(url, table_id):\n    \"\"\"\n    \n    \"\"\"\n    # Send HTTP request\n    response = requests.get(url)\n\n    # Parse the content of the request with BeautifulSoup\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find the table with the given id\n    table = soup.find('table', {'id': table_id})\n\n    # Read the table into a pandas DataFrame\n    df = pd.read_html(str(table))[0]\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\n\ndef task_func(url, filename):\n    \"\"\"\n    \n    \"\"\"\n    # Create download directory if it doesn't exist\n    if not DOWNLOAD_DIR.exists():\n        DOWNLOAD_DIR.mkdir()\n\n    # Create unzipped files directory if it doesn't exist\n    if not ZIP_DIR.exists():\n        ZIP_DIR.mkdir()\n\n    # Download file\n    response = requests.get(url, stream=True)\n    file_path = DOWNLOAD_DIR / filename\n    with open(file_path, \"wb\") as handle:\n        for data in response.iter_content():\n            handle.write(data)\n\n    # Unzip file\n    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n        zip_ref.extractall(ZIP_DIR)\n\n    print(f\"Downloaded and unzipped {filename}\")\n\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # 1. Send a GET request to the URL\n    response = requests.get(url)\n\n    # 2. Parse the HTML content of the page with BeautifulSoup\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    # 3. Find all the links on the page\n    links = soup.find_all(\"a\")\n\n    # 4. Filter out any links that are not absolute URLs\n    absolute_links = [\n        urljoin(base_url, link.get(\"href\")) for link in links if link.get(\"href\")\n    ]\n\n    # 5. Write the absolute links to a CSV file\n    with open(csv_file, \"w\") as file:\n        writer = csv.writer(file)\n        writer.writerow([\"URL\"])\n        for link in absolute_links:\n            writer.writerow([link])\n\n    # 6. Return the number of links found\n    return len(absolute_links)\n\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Connect to the database\n    conn = sqlite3.connect(database_name)\n    c = conn.cursor()\n\n    # Create table\n    c.execute('''CREATE TABLE IF NOT EXISTS webpage_data\n                 (title TEXT, content TEXT)''')\n\n    # Send a GET request to the webpage\n    response = requests.get(webpage_url)\n\n    # Parse the page content\n    page = html.fromstring(response.content)\n\n    # Extract the title of the webpage\n    title = page.xpath('//head/title/text()')[0]\n\n    # Extract the main content of the webpage\n    content = page.xpath('//body//text()')\n    content = ' '.join(content)\n\n    # Insert the data into the database\n    c.execute(\"INSERT INTO webpage_data VALUES (?, ?)\", (title, content))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    return 0\n\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\n\nIMAGE_PATH = \"image.png\"\n\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    with codecs.open(filename, \"r\", from_encoding) as f:\n        text = f.read()\n        f.close()\n\n    with codecs.open(filename, \"w\", to_encoding) as f:\n        f.write(text)\n        f.close()\n\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    # Get data from API\n    response = requests.get(url)\n    data = response.content\n\n    # Detect encoding\n    if not from_encoding:\n        from_encoding = chardet.detect(data)['encoding']\n\n    # Convert encoding\n    data = data.decode(from_encoding).encode(to_encoding)\n\n    # Convert to JSON\n    data = json.loads(data)\n\n    return data\n\n"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        df = pd.read_csv(csv_file_path)\n    except FileNotFoundError:\n        print(f\"File {csv_file_path} not found.\")\n        return\n    except EmptyDataError:\n        print(f\"File {csv_file_path} is empty.\")\n        return\n    except Exception as e:\n        print(f\"Error reading file {csv_file_path}: {str(e)}\")\n        return\n\n    if column_name not in df.columns:\n        print(f\"Column {column_name} not found in file {csv_file_path}.\")\n        return\n\n    try:\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    except ValueError as e:\n        print(f\"Error converting column {column_name} to datetime: {str(e)}\")\n        return\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    \n    \"\"\"\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(cert_file, key_file)\n\n    server_socket = context.wrap_socket(client_socket, server_side=True)\n\n    while True:\n        data = server_socket.recv(buffer_size)\n        if not data:\n            break\n        server_socket.send(data)\n\n    server_socket.close()\n\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    \n    \"\"\"\n    # Create a TCP/IP socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    # Connect the socket to the port where the server is listening\n    server_addr = (server_address, server_port)\n    sock.connect(server_addr)\n\n    # Make the socket non-blocking\n    sock.setblocking(0)\n\n    # Create a queue for incoming messages\n    message_queue = queue.Queue()\n\n    # Start time\n    start_time = datetime.now()\n\n    while True:\n        # Check if it's time to stop\n        if datetime.now() - start_time > timedelta(seconds=run_duration):\n            break\n\n        # Check if there's data to receive\n        readable, writable, exceptional = select.select([sock], [], [], 1)\n\n        if sock in readable:\n            # Receive data from the server\n            data = sock.recv(buffer_size)\n            if data:\n                # Acknowledge the message\n                message_queue.put(data)\n            else:\n                # No data means the connection has been closed\n                print(\"closing socket\")\n                sock.close()\n                break\n\n    # Empty the message queue\n    while not message_queue.empty():\n        print(\"message:\", message_queue.get())\n\n"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\n\ndef task_func(client_socket):\n    \"\"\"\n    \n    \"\"\"\n    # get the email and password from the client\n    email = client_socket.recv(BUFFER_SIZE).decode()\n    password = getpass.getpass()\n\n    # create an email message\n    msg = EmailMessage()\n    msg['Subject'] = \"Hello\"\n    msg['From'] = email\n    msg['To'] = \"recipient@example.com\"\n    msg.set_content(\"This is a test email\")\n\n    # send the email\n    try:\n        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n        server.starttls()\n        server.login(email, password)\n        server.send_message(msg)\n        server.quit()\n        print(\"Email sent successfully\")\n    except Exception as e:\n        print(f\"Failed to send email: {str(e)}\")\n\n    # close the connection\n    client_socket.close()\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    \n    \"\"\"\n    # Load data\n    df = pd.read_csv(file_path)\n\n    # Preprocess data\n    df['text'] = df['text'].apply(lambda x: x.lower())\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in STOP_WORDS]))\n\n    # Vectorize data\n    vectorizer = CountVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(df['text'])\n\n    # Get word counts\n    word_counts = X.toarray().sum(axis=0)\n\n    # Create dataframe\n    df_word_counts = pd.DataFrame(list(zip(vectorizer.get_feature_names(), word_counts)), columns=['word', 'count'])\n    df_word_counts = df_word_counts.sort_values(by='count', ascending=False)\n\n    # Save data\n    if save_path:\n        df_word_counts.to_csv(save_path, index=False)\n\n    # Plot data\n    df_word_counts[:20].plot(x='word', y='count', kind='bar', legend=None)\n    plt.title('Top 20 words by count')\n    plt.xlabel('Word')\n    plt.ylabel('Count')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\n\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    \n    \"\"\"\n    if animals is None:\n        animals = ['cat', 'dog', 'rabbit']\n    if foods is None:\n        foods = ['fish', 'meat', 'vegetables']\n\n    combinations = list(itertools.product(animals, foods))\n\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    \n    \"\"\"\n    time_strings = np.array(time_strings)\n    timezone = pytz.timezone(timezone)\n    time_objects = [datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\") for time in time_strings]\n    time_objects = [timezone.localize(time) for time in time_objects]\n    time_objects = [time.astimezone(pytz.utc) for time in time_objects]\n    time_objects = [time.strftime(\"%Y-%m-%d %H:%M:%S\") for time in time_objects]\n    return time_objects\n\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 \u0441\u043b\u043e\u0432\u0430\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # 2. \u041f\u043e\u0434\u0441\u0447\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0439 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\n    word_counts = Counter(words)\n\n    # 3. \u0412\u044b\u0432\u043e\u0434 10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432\n    most_common_words = word_counts.most_common(10)\n    print(\"10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432:\")\n    for word, count in most_common_words:\n        print(f\"{word}: {count}\")\n\n    # 4. \u0413\u0440\u0430\u0444\u0438\u043a, \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0439 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\n    plt.figure(figsize=(10, 5))\n    words, counts = zip(*most_common_words)\n    plt.bar(words, counts)\n    plt.xlabel('Words')\n    plt.ylabel('Counts')\n    plt.title('10 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0445\u0441\u044f \u0441\u043b\u043e\u0432')\n    plt.xticks(rotation=90)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n\ndef task_func(myString):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \uc815\uaddc\uc2dd\uc744 \uc774\uc6a9\ud55c \ubb38\uc790\uc5f4 \ucd94\ucd9c\n    url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[\ud55c\uae00]|[\ud7a3])+')\n    url_list = re.findall(url_pattern, myString)\n\n    # 2. \ucd94\ucd9c\ud55c url\uc744 \uc774\uc6a9\ud55c \uc6f9 \ud06c\ub864\ub9c1\n    for url in url_list:\n        try:\n            html = requests.get(url)\n            soup = BeautifulSoup(html.text, 'html.parser')\n            print(f\"{url}\uc758 \ud0c0\uc774\ud2c0 : {soup.title.text}\")\n        except Exception as e:\n            print(f\"{url} \uc811\uadfc \uc2e4\ud328 : {e}\")\n\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    \n    \"\"\"\n    if save_dir is not None:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n    for item in json_data:\n        if unknown_key in item:\n            if save_dir is not None:\n                with open(os.path.join(save_dir, f\"{item['id']}.json\"), 'w') as f:\n                    json.dump(item, f)\n            else:\n                print(item)\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n    res = requests.get(url)\n    res.raise_for_status()\n    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n\n    # 2. \u63d0\u53d6\u6240\u9700\u4fe1\u606f\n    # 2.1 \u63d0\u53d6\u6807\u9898\n    title = soup.select_one('.post-title').text.strip()\n\n    # 2.2 \u63d0\u53d6\u4f5c\u8005\n    author = soup.select_one('.post-user').text.strip()\n\n    # 2.3 \u63d0\u53d6\u53d1\u5e03\u65e5\u671f\n    date = soup.select_one('.post-date').text.strip()\n\n    # 2.4 \u63d0\u53d6\u5185\u5bb9\n    content = soup.select_one('.post-content').text.strip()\n\n    # 2.5 \u63d0\u53d6\u6807\u7b7e\n    tags = [tag.text.strip() for tag in soup.select('.post-tag')]\n\n    # 3. \u5c06\u4fe1\u606f\u4fdd\u5b58\u5230\u6587\u4ef6\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump({\n            'title': title,\n            'author': author,\n            'date': date,\n            'content': content,\n            'tags': tags\n        }, f, ensure_ascii=False)\n\n    return [title, author, date, content, tags]\n\n"}
