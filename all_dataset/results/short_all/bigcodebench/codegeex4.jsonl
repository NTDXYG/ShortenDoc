{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    \n    \"\"\"\n    # Connect to FTP server\n    ftp = ftplib.FTP(ftp_server)\n    ftp.login(ftp_user, ftp_password)\n    \n    # Change to the specified directory\n    ftp.cwd(ftp_dir)\n    \n    # Get a list of files in the directory\n    files = ftp.nlst()\n    \n    # Download each file\n    for file in files:\n        local_file = os.path.join('/tmp', file)\n        with open(local_file, 'wb') as f:\n            ftp.retrbinary('RETR ' + file, f.write)\n    \n    # Close the connection\n    ftp.quit()\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    \n    \"\"\"\n    # Read the commands from the file\n    with open(commands_file_path, 'r') as f:\n        commands = f.readlines()\n    \n    # Remove leading/trailing whitespaces and newlines\n    commands = [command.strip() for command in commands]\n    \n    # Create the output directory if it doesn't exist\n    os.makedirs(output_dir_path, exist_ok=True)\n    \n    # Execute each command and save the output to a file\n    for i, command in enumerate(commands):\n        # Create the output file path\n        output_file_path = os.path.join(output_dir_path, f'output_{i}.txt')\n        \n        # Execute the command and save the output to a file\n        with open(output_file_path, 'w') as f:\n            subprocess.run(command, shell=True, stdout=f, stderr=f)\n    \n    # Create a CSV file to store the command and its corresponding output file\n    csv_file_path = os.path.join(output_dir_path, 'command_output.csv')\n    with open(csv_file_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Command', 'Output File'])\n        for i, command in enumerate(commands):\n            output_file_path = os.path.join(output_dir_path, f'output_{i}.txt')\n            writer.writerow([command, output_file_path])\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Get the process ID (PID) of the process with the given name\n    pid = psutil.pid_exists(process_name)\n    if pid is None:\n        return f\"Process '{process_name}' not found.\"\n    \n    # Get the process object\n    process = psutil.Process(pid)\n    \n    # Get the process's CPU usage\n    cpu_usage = process.cpu_percent(interval=1)\n    \n    # Get the process's memory usage\n    memory_usage = process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n    \n    # Get the process's start time\n    start_time = process.create_time()\n    \n    # Get the process's running time\n    running_time = time.time() - start_time\n    \n    # Get the process's status\n    status = process.status()\n    \n    # Get the process's command line\n    command_line = process.cmdline()\n    \n    # Get the process's username\n    username = process.username()\n    \n    # Get the process's nice value\n    nice_value = process.nice()\n    \n    # Get the process's IO statistics\n    io_stats = process.io_counters()\n    \n    # Get the process's open files\n    open_files = process.open_files()\n    \n    # Get the process's connections\n    connections = process.connections()\n    \n    # Get the process's threads\n    threads = process.threads()\n    \n    # Get the process's children\n    children = process.children()\n    \n    # Get the process's parent\n    parent = process.parent()\n    \n    # Get the process's executable path\n    executable_path = process.exe()\n    \n    # Get the process's working directory\n    working_directory = process.cwd()\n    \n    # Get the process's environment variables\n    environment_variables = process.environ()\n    \n    # Get the process's open file descriptors\n    file_descriptors = process.open_files()\n    \n    # Get the process's CPU affinity\n    cpu_affinity = process.cpu_affinity()\n    \n    # Get the process's memory maps\n    memory_maps = process.memory_maps()\n    \n    # Get the process's numa node\n    numa_node = process.numa_node()\n    \n    # Get the process's terminal\n    terminal = process.terminal()\n    \n    # Get the process's nice value\n    nice_value = process.nice()\n    \n    # Get the"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # Get all zip files in the directory\n    zip_files = glob.glob(os.path.join(directory, '*.zip'))\n    \n    # Loop through each zip file\n    for zip_file in zip_files:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall(directory)\n        \n        # Remove the zip file\n        os.remove(zip_file)\n"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation and numbers\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\d+', '', text)\n    \n    # Create word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    \n    # Display word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud.to_file('wordcloud.png')\n\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, target_column):\n    \"\"\"\n    \n    \"\"\"\n    # Separate features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    # Create a random forest classifier\n    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    # Fit the classifier\n    clf.fit(X, y)\n    \n    # Get feature importances\n    importances = clf.feature_importances_\n    \n    # Get feature names\n    feature_names = X.columns\n    \n    # Create a dataframe of feature importances\n    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n    \n    # Sort the dataframe by importance\n    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n    \n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='importance', y='feature', data=feature_importance_df)\n    plt.title('Feature Importances')\n    plt.show()\n    \n    return feature_importance_df\n\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True\n    app.config['TEMPLATES_AUTO_RELOAD'] = True"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the z-score for each data point\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers based on the z-score threshold\n    outliers = data[z_scores > outlier_z_score]\n    \n    # Remove outliers from the data\n    data_cleaned = data[z_scores <= outlier_z_score]\n    \n    return data_cleaned\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    \n    \"\"\"\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    \n    # Get the cluster centers\n    cluster_centers = kmeans.cluster_centers_\n    \n    # Get the labels for each data point\n    labels = kmeans.labels_\n    \n    # Plot the data points with different colors for each cluster\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\n    \n    # Plot the cluster centers\n    ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='x', s=200)\n    \n    # Add a legend\n    legend1 = ax.legend(*scatter.legend_elements(), title=\"Clusters\")\n    ax.add_artist(legend1)\n    \n    # Add a title\n    ax.set_title(\"KMeans Clustering\")\n    \n    # Show the plot\n    plt.show()\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    \n    \"\"\"\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed_data = pca.transform(data)\n    return transformed_data\n"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['target'] = iris.target\n    df['species'] = iris.target_names[df.target]\n\n    # Plot the correlation matrix\n    corr = df.corr()\n    sns.heatmap(corr, annot=True, cmap='coolwarm')\n\n    # Show the plot\n    plt.show()\n"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    df = pd.DataFrame({\n        'date': pd.date_range(start='2022-01-01', periods=100, freq='D'),\n        'value': [random.random() for _ in range(100)]\n    })\n    df['date'] = pd.to_datetime(df['date'])\n    df['value'] = df['value'].cumsum()\n    df['value'] = df['value'] * 100\n    df['value'] = df['value'].astype(int)\n    df['value'] = df['value'].astype(str)\n    df['value'] = df['value'].str.zfill(5)\n    df['value'] = df['value'] + '\u20ac'\n    df['date'] = df['date'].dt.strftime('%d/%m/%Y')\n    df = df.set_index('date')\n    df = df.sort_index(ascending=False)\n    df = df.iloc[:10]\n    df = df.reset_index()\n    df = df.rename(columns={'index': 'Date'})\n    df = df[['Date', 'value']]\n    df = df.set_index('Date')\n    df = df.sort_index(ascending=False)\n    df = df.reset_index()\n    df = df.rename(columns={'index': 'Date'})\n    df = df[['Date', 'value']]\n    df = df.set_index('Date')\n    df = df.sort_index(ascending=False)\n    df = df.reset_index()\n    df = df.rename(columns={'index': 'Date'})\n    df = df[['Date', 'value']]\n    df = df.set_index('Date')\n    df = df.sort_index(ascending=False)\n    df = df.reset_index()\n    df = df.rename(columns={'index': 'Date'})\n    df = df[['Date', 'value']]\n    df = df.set_index('Date')\n    df = df.sort_index(ascending=False)\n    df = df.reset_index()\n    df = df.rename(columns={'index': 'Date'})\n    df = df[['Date', 'value']]\n    df = df.set_index('Date')\n    df = df.sort_index(ascending=False)\n    df = df.reset_index()\n    df = df.rename(columns={'index': 'Date'})\n    df = df[['Date', 'value']]\n    df = df.set_index('Date')\n    df = df.sort_index(ascending"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n    \n    # Split the data into features and target\n    X = data.drop(\"MEDV\", axis=1)\n    y = data[\"MEDV\"]\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n    \n    # Scale the features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train the model\n    model = LinearRegression()\n    model.fit(X_train_scaled, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test_scaled)\n    \n    # Calculate the mean squared error\n    mse = mean_squared_error(y_test, y_pred)\n    \n    return mse\n\nmse = task_func()"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    \n    \"\"\"\n    # Decompose the time series\n    decomposition = seasonal_decompose(df, model=decomposition_model, period=freq)\n    \n    # Plot the decomposed time series\n    decomposition.plot()\n    plt.show()\n    \n    # Return the decomposed time series\n    return decomposition\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    df = pd.DataFrame(columns=['date', 'task', 'duration'])\n    current_date = start_date\n    while current_date <= end_date:\n        task = f\"Task {randint(1, 10)}\"\n        duration = randint(1, 10)\n        df = df.append({'date': current_date, 'task': task, 'duration': duration}, ignore_index=True)\n        current_date += timedelta(days=1)\n    return df\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list to store the dataframes\n    df_list = []\n    \n    # Loop through the list of files\n    for file_name in my_list:\n        # Create the full file path\n        file_path = os.path.join(file_dir, file_name)\n        \n        # Check if the file exists\n        if os.path.isfile(file_path):\n            # Read the file into a dataframe\n            df = pd.read_csv(file_path)\n            \n            # Append the dataframe to the list\n            df_list.append(df)\n        else:\n            print(f\"File {file_path} does not exist.\")\n    \n    # Concatenate the dataframes in the list\n    df_concat = pd.concat(df_list, ignore_index=True)\n    \n    # Return the concatenated dataframe\n    return df_concat\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    my_list = [randint(0, size) for i in range(size)]\n    my_list.sort()\n    return my_list\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find all the links on the page\n        links = soup.find_all('a')\n        \n        # Extract the text and href attributes of each link\n        link_data = [(link.get_text(), link.get('href')) for link in links]\n        \n        # Create a DataFrame from the link data\n        df = pd.DataFrame(link_data, columns=['Text', 'URL'])\n        \n        # Return the DataFrame\n        return df\n    else:\n        # If the request was not successful, return an error message\n        return f\"Error: Unable to retrieve page. Status code: {response.status_code}\"\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the 'value' column\n    mean_value = df['value'].mean()\n    \n    # Calculate the standard deviation of the 'value' column\n    std_value = df['value'].std()\n    \n    # Calculate the minimum value of the 'value' column\n    min_value = df['value'].min()\n    \n    # Calculate the maximum value of the 'value' column\n    max_value = df['value'].max()\n    \n    # Calculate the 25th percentile of the 'value' column\n    percentile_25 = df['value'].quantile(0.25)\n    \n    # Calculate the 75th percentile of the 'value' column\n    percentile_75 = df['value'].quantile(0.75)\n    \n    # Calculate the median of the 'value' column\n    median_value = df['value'].median()\n    \n    # Calculate the number of missing values in the 'value' column\n    missing_values = df['value'].isnull().sum()\n    \n    # Calculate the number of unique values in the 'value' column\n    unique_values = df['value'].nunique()\n    \n    # Calculate the skewness of the 'value' column\n    skewness = df['value'].skew()\n    \n    # Calculate the kurtosis of the 'value' column\n    kurtosis = df['value'].kurt()\n    \n    # Calculate the correlation between the 'value' column and the 'time' column\n    correlation = df['value'].corr(df['time'])\n    \n    # Calculate the covariance between the 'value' column and the 'time' column\n    covariance = df['value'].cov(df['time'])\n    \n    # Calculate the linear regression coefficients between the 'value' column and the 'time' column\n    slope, intercept = np.polyfit(df['time'], df['value'], 1)\n    \n    # Calculate the Pearson correlation coefficient between the 'value' column and the 'time' column\n    pearson_corr = df['value'].corr(df['time'], method='pearson')\n    \n    # Calculate the Spearman correlation coefficient between the 'value' column and the 'time' column\n    spearman_corr = df['value'].corr(df['time'], method='spearman')\n    \n    # Calculate the Kendall correlation coefficient between the 'value"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    \n    \"\"\"\n    for ip in ip_range:\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(1)\n            result = sock.connect_ex((str(ip), port))\n            if result == 0:\n                print(f\"Port {port} is open on {ip}\")\n            sock.close()\n        except Exception as e:\n            print(f\"Error: {e}\")\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(log_file):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bfb\u53d6\u65e5\u5fd7\u6587\u4ef6\n    with open(log_file, 'r') as f:\n        log_data = f.readlines()\n    \n    # \u521d\u59cb\u5316\u7ed3\u679c\u5217\u8868\n    result = []\n    \n    # \u904d\u5386\u65e5\u5fd7\u6570\u636e\n    for line in log_data:\n        # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u65e5\u5fd7\u4e2d\u7684\u65f6\u95f4\u6233\u548c\u9519\u8bef\u4fe1\u606f\n        match = re.search(r'\\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\[(.*?)\\] \\"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    \n    \"\"\"\n    # Remove all non-alphanumeric characters except spaces\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = {}\n    for word in words:\n        if word in word_counts:\n            word_counts[word] += 1\n        else:\n            word_counts[word] = 1\n    \n    # Sort the words by frequency\n    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n    \n    # Get the top 10 most frequent words\n    top_words = sorted_words[:10]\n    \n    # Get the frequencies of the top 10 words\n    top_word_freqs = [word[1] for word in top_words]\n    \n    # Get the labels for the top 10 words\n    top_word_labels = [word[0] for word in top_words]\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.barh(top_word_labels, top_word_freqs, align='center', alpha=0.5, color='blue')\n    ax.set_yticks(top_word_labels)\n    ax.set_yticklabels(top_word_labels)\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.set_xlabel('Frequency')\n    ax.set_title('Top 10 most frequent words')\n    \n    # Add a text box with the total number of words\n    total_words = len(words)\n    text_box = ax.text(0.5, 1.02, f'Total words: {total_words}', transform=ax.transAxes, ha='center', va='bottom')\n    text_box.set_bbox(dict(facecolor='white', alpha=0.5))\n    \n    # Add a text box with the number of unique words\n    unique_words = len(word_counts)\n    text_box = ax.text(0.5, 1.01, f'Unique words: {unique_words}', transform=ax.transAxes, ha='center', va='bottom')\n    text_box.set_bbox(dict(facecolor='white', alpha=0.5))\n    \n    # Add a text box with the average word length\n    avg_word_length = np.mean([len(word) for word in words])\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\n\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation\n    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n    \n    # Tokenize\n    df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n    \n    # Remove stopwords\n    stopwords = nltk.corpus.stopwords.words('english')\n    df['text'] = df['text'].apply(lambda x: [word for word in x if word.lower() not in stopwords])\n    \n    # Remove numbers\n    df['text'] = df['text'].apply(lambda x: [word for word in x if not word.isdigit()])\n    \n    # Remove single character words\n    df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 1])\n    \n    # Remove words that are too common\n    common_words = ['the', 'and', 'a', 'to', 'of', 'in', 'that', 'it', 'is', 'was', 'for', 'on', 'with', 'as', 'at', 'by', 'an', 'be', 'this', 'which', 'or', 'from', 'but', 'not', 'are', 'have', 'you', 'they', 'we', 'his', 'her', 'their', 'there', 'been', 'has', 'were', 'if', 'will', 'would', 'about', 'who', 'when', 'so', 'up', 'out', 'into', 'some', 'could', 'than', 'then', 'only', 'also', 'may', 'should', 'like', 'other', 'its', 'over', 'such', 'very', 'much', 'well', 'most', 'many', 'before', 'after', 'through', 'between', 'under', 'above', 'against', 'around', 'about', 'between', 'during', 'before', 'after', 'since', 'until', 'while', 'though', 'even', 'still', 'yet', 'so', 'too', 'both', 'either', 'neither', 'not', 'nor', 'but', 'or', 'if', 'whether', 'while', 'until', 'as', 'since', 'for', 'against', 'in', '"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n    \n    # Convert to lowercase\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: x.lower())\n    \n    # Tokenize\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: x.split())\n    \n    # Remove stopwords\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: [word for word in x if word not in STOPWORDS])\n    \n    # Join words back into a string\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join(x))\n    \n    # Vectorize\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    return X\n\n"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of points\n    points = [Point(np.random.uniform(dic['Lon'][0], dic['Lon'][1]), np.random.uniform(dic['Lat'][0], dic['Lat'][1])) for _ in range(1000)]\n    \n    # Create a GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities}, geometry=points)\n    \n    # Save the GeoDataFrame to a shapefile\n    gdf.to_file('cities.shp')\n    \n    return gdf\n"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    \n    \"\"\"\n    set_seed(seed)\n    city = cities[randint(0, len(cities) - 1)]\n    weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n    timezone = timezones[city]\n    local_datetime = utc_datetime.astimezone(pytz.timezone(timezone))\n    return {\n        'city': city,\n        'weather_condition': weather_condition,\n        'local_datetime': local_datetime\n    }\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    return np.random.rand(elements)\n\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    \n    \"\"\"\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n    \n    # Send a GET request to the URL\n    response = requests.get(url, headers=headers)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        # Extract the filename from the URL\n        filename = url.split(\"/\")[-1]\n        \n        # Create the full path for the file\n        file_path = os.path.join(destination_directory, filename)\n        \n        # Write the content of the response to the file\n        with open(file_path, \"wb\") as file:\n            file.write(response.content)\n        \n        # Check if the file is a zip file\n        if filename.endswith(\".zip\"):\n            # Extract the contents of the zip file\n            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n                zip_ref.extractall(destination_directory)\n            \n            # Remove the zip file\n            os.remove(file_path)\n    else:\n        print(f\"Failed to download file from {url}. Status code: {response.status_code}\")\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    \n    # Create a random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n    \n    # Convert the image to grayscale\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # Apply Gaussian blur to the grayscale image\n    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n    \n    # Apply Canny edge detection to the blurred image\n    edges = cv2.Canny(blurred_image, 50, 150)\n    \n    # Find contours in the edge-detected image\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Draw the contours on the original image\n    image_with_contours = cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n    \n    # Display the original image and the image with contours\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.subplot(1, 2, 2)\n    plt.imshow(image_with_contours)\n    plt.title('Image with Contours')\n    plt.axis('off')\n    plt.show()\n    \n    return image_with_contours\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    \n    \"\"\"\n    # Load audio file\n    y, sr = librosa.load(audio_file, sr=None)\n    \n    # Compute STFT\n    D = librosa.stft(y)\n    \n    # Compute magnitude and phase\n    magnitude = np.abs(D)\n    phase = np.angle(D)\n    \n    # Compute spectrogram\n    spectrogram = 20 * np.log10(magnitude + 1e-10)\n    \n    # Compute mel spectrogram\n    mel_spectrogram = librosa.feature.melspectrogram(S=magnitude, sr=sr)\n    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n    \n    # Compute CQT\n    CQT = librosa.amplitude_to_db(librosa.cqt(y, sr=sr), ref=np.max)\n    \n    # Compute chroma\n    chroma = librosa.feature.chroma_stft(S=magnitude, sr=sr)\n    \n    # Compute MFCC\n    mfcc = librosa.feature.mfcc(S=magnitude, sr=sr)\n    \n    # Compute tonnetz\n    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n    \n    # Compute zero-crossing rate\n    zcr = librosa.feature.zero_crossing_rate(y)\n    \n    # Compute spectral contrast\n    contrast = librosa.feature.spectral_contrast(S=magnitude, sr=sr)\n    \n    # Compute spectral flatness\n    flatness = librosa.feature.spectral_flatness(S=magnitude)\n    \n    # Compute spectral roll-off\n    roll_off = librosa.feature.spectral_rolloff(S=magnitude, sr=sr)\n    \n    # Compute spectral bandwidth\n    bandwidth = librosa.feature.spectral_bandwidth(S=magnitude, sr=sr)\n    \n    # Compute spectral centroid\n    centroid = librosa.feature.spectral_centroid(S=magnitude, sr=sr)\n    \n    # Compute spectral bandwidth\n    bandwidth = librosa.feature.spectral_bandwidth(S=magnitude, sr=sr)\n    \n    # Compute spectral contrast\n    contrast = librosa.feature.spectral_contrast(S=magnitude, sr=sr)\n    \n    # Compute spectral flatness\n    flatness = librosa.feature.spectral_flatness(S=magnitude)\n    \n    # Compute spectral roll-off\n"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    # Define the task function\n    def task(x):\n        return original(x) + np.random.normal(0, 0.1, x.shape)\n    \n    return task\n\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    # Normalize the data\n    normalized = preprocessing.normalize(original)\n    \n    # Calculate the mean and standard deviation of the normalized data\n    mean = np.mean(normalized, axis=0)\n    std = np.std(normalized, axis=0)\n    \n    # Calculate the z-score of each data point\n    z_scores = (normalized - mean) / std\n    \n    # Calculate the mean and standard deviation of the z-scores\n    mean_z_scores = np.mean(z_scores, axis=0)\n    std_z_scores = np.std(z_scores, axis=0)\n    \n    # Calculate the z-score of each z-score\n    z_scores_z_scores = (z_scores - mean_z_scores) / std_z_scores\n    \n    # Calculate the mean and standard deviation of the z-scores of z-scores\n    mean_z_scores_z_scores = np.mean(z_scores_z_scores, axis=0)\n    std_z_scores_z_scores = np.std(z_scores_z_scores, axis=0)\n    \n    # Calculate the z-score of each z-score of z-scores\n    z_scores_z_scores_z_scores = (z_scores_z_scores - mean_z_scores_z_scores) / std_z_scores_z_scores\n    \n    # Calculate the mean and standard deviation of the z-scores of z-scores of z-scores\n    mean_z_scores_z_scores_z_scores = np.mean(z_scores_z_scores_z_scores, axis=0)\n    std_z_scores_z_scores_z_scores = np.std(z_scores_z_scores_z_scores, axis=0)\n    \n    # Calculate the z-score of each z-score of z-scores of z-scores\n    z_scores_z_scores_z_scores_z_scores = (z_scores_z_scores_z_scores - mean_z_scores_z_scores_z_scores) / std_z_scores_z_scores_z_scores\n    \n    # Calculate the mean and standard deviation of the z-scores of z-scores of z-scores of z-scores\n    mean_z_scores_z_scores_z_scores_z_scores = np.mean(z_scores_z_scores_z_scores_z_scores, axis=0)\n    std_z_scores_z_scores_z_scores_z_scores = np.std(z_scores_z_scores_z_scores_z_scores, axis=0)\n    \n    # Calculate the z-score of each z-score of z-scores of z-scores of z-scores of z-scores\n    z_scores_z_scores_z_scores_z_scores_z_scores = ("}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    \n    \"\"\"\n    # Perform FFT on the data\n    fft_data = fftpack.fft(data)\n    \n    # Calculate the frequency bins\n    freq_bins = fftpack.fftfreq(len(data), 1/sample_rate)\n    \n    # Find the index of the maximum power\n    max_power_index = np.argmax(np.abs(fft_data))\n    \n    # Find the frequency corresponding to the maximum power\n    max_power_freq = freq_bins[max_power_index]\n    \n    # Find the index of the second maximum power\n    second_max_power_index = np.argsort(np.abs(fft_data))[-2]\n    \n    # Find the frequency corresponding to the second maximum power\n    second_max_power_freq = freq_bins[second_max_power_index]\n    \n    # Calculate the difference between the two frequencies\n    freq_diff = np.abs(max_power_freq - second_max_power_freq)\n    \n    # Return the difference\n    return freq_diff\n\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    pass\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: Implement the task function here\n    pass\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    with open(os.path.join(directory, filename), 'r') as f:\n        data = json.load(f)\n    \n    # Count the number of occurrences of each label\n    label_counts = Counter(data['labels'])\n    \n    # Return the counts\n    return label_counts\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    \n    \"\"\"\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Sort the dataframe by 'Date'\n    df = df.sort_values('Date')\n    \n    # Plot the dataframe if plot is True\n    if plot:\n        plt.figure(figsize=(10, 6))\n        sns.lineplot(data=df, x='Date', y='Value')\n        plt.title('Time Series Plot')\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.show()\n    \n    return df\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    \n    \"\"\"\n    # Generate the data for the report\n    data = {field: [random.randint(0, 100) for _ in range(len(STUDENTS))] for field in FIELDS}\n    data.update({field: [random.randint(0, 100) for _ in range(len(STUDENTS))] for field in additional_fields})\n    df = pd.DataFrame(data)\n    \n    # Calculate the mean for each student\n    df['Mean'] = df.mean(axis=1)\n    \n    # Sort the students by their mean score\n    df = df.sort_values(by='Mean', ascending=False)\n    \n    # Get the top 10 students\n    top_10_students = df.head(10)\n    \n    # Get the mean score for the top 10 students\n    top_10_mean = top_10_students['Mean'].mean()\n    \n    # Get the mean score for the bottom 10 students\n    bottom_10_students = df.tail(10)\n    bottom_10_mean = bottom_10_students['Mean'].mean()\n    \n    # Get the mean score for all students\n    all_mean = df['Mean'].mean()\n    \n    # Get the mean score for each field\n    field_means = df.mean()\n    \n    # Get the standard deviation for each field\n    field_stds = df.std()\n    \n    # Get the correlation matrix\n    corr_matrix = df.corr()\n    \n    # Get the covariance matrix\n    cov_matrix = df.cov()\n    \n    # Get the skewness for each field\n    field_skews = df.skew()\n    \n    # Get the kurtosis for each field\n    field_kurtoses = df.kurt()\n    \n    # Get the number of students who scored above the mean for each field\n    field_above_mean_counts = df.apply(lambda x: (x > x.mean()).sum())\n    \n    # Get the number of students who scored below the mean for each field\n    field_below_mean_counts = df.apply(lambda x: (x < x.mean()).sum())\n    \n    # Get the number of students who scored above the mean for each field,"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(filename, 'r') as file:\n        reader = csv.DictReader(file)\n        data = [row for row in reader]\n    \n    # Calculate the average age, height, and weight\n    ages = [int(row['Age']) for row in data]\n    heights = [float(row['Height']) for row in data]\n    weights = [float(row['Weight']) for row in data]\n    avg_age = mean(ages)\n    avg_height = mean(heights)\n    avg_weight = mean(weights)\n    \n    # Print the results\n    print(f\"Average age: {avg_age}\")\n    print(f\"Average height: {avg_height}\")\n    print(f\"Average weight: {avg_weight}\")\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # Get the current date and time\n    now = datetime.now()\n    # Format the date and time as a string\n    date_time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    # Create a new directory with the current date and time as its name\n    new_directory = os.path.join(directory, date_time_str)\n    os.makedirs(new_directory)\n    # Copy all files from the original directory to the new directory\n    for filename in os.listdir(directory):\n        file_path = os.path.join(directory, filename)\n        if os.path.isfile(file_path):\n            shutil.copy2(file_path, new_directory)\n    # Print a message indicating that the task is complete\n    print(f\"Task complete. Files copied to {new_directory}\")\n\n"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\n\ndef task_func(file_list):\n    \"\"\"\n    \n    \"\"\"\n    for file_name in file_list:\n        print(f\"Processing file: {file_name}\")\n        # Your processing code here\n        time.sleep(1)  # Simulate processing time\n        print(f\"Finished processing file: {file_name}\")\n\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of files in the directory\n    files = glob.glob(os.path.join(directory_path, '*.txt'))\n\n    # Iterate over the files\n    for file in files:\n        # Open the file\n        with open(file, 'r') as f:\n            # Read the contents of the file\n            contents = f.read()\n\n        # Replace the string\n        new_contents = contents.replace('old_string', 'new_string')\n\n        # Write the new contents back to the file\n        with open(file, 'w') as f:\n            f.write(new_contents)\n"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    \n    \"\"\"\n    # Create a new column 'task' with the value 'task1'\n    df['task'] = 'task1'\n    \n    # Create a new column 'value' with the value 1\n    df['value'] = 1\n    \n    # Create a new column 'color' with the value 'red'\n    df['color'] = 'red'\n    \n    # Create a new column 'size' with the value 10\n    df['size'] = 10\n    \n    # Create a new column 'shape' with the value 'circle'\n    df['shape'] = 'circle'\n    \n    # Create a new column 'label' with the value 'label1'\n    df['label'] = 'label1'\n    \n    # Create a new column 'x' with the value 0\n    df['x'] = 0\n    \n    # Create a new column 'y' with the value 0\n    df['y'] = 0\n    \n    # Create a new column 'z' with the value 0\n    df['z'] = 0\n    \n    # Create a new column 'w' with the value 0\n    df['w'] = 0\n    \n    # Create a new column 'h' with the value 0\n    df['h'] = 0\n    \n    # Create a new column 'd' with the value 0\n    df['d'] = 0\n    \n    # Create a new column 'e' with the value 0\n    df['e'] = 0\n    \n    # Create a new column 'f' with the value 0\n    df['f'] = 0\n    \n    # Create a new column 'g' with the value 0\n    df['g'] = 0\n    \n    # Create a new column 'i' with the value 0\n    df['i'] = 0\n    \n    # Create a new column 'j' with the value 0\n    df['j'] = 0\n    \n    # Create a new column 'k' with the value 0\n    df['k'] = 0\n    \n    # Create a new column 'l' with the value 0\n    df['l'] = 0\n    \n    # Create a new column 'm' with the value 0\n    df['m"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    \n    \"\"\"\n    if wait:\n        subprocess.call([script_path, *args])\n    else:\n        subprocess.Popen([script_path, *args])\n\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    \n    \"\"\"\n    # Load the Excel file\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n    \n    # Get the number of rows and columns\n    num_rows, num_cols = df.shape\n    \n    # Create a figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot the data\n    ax.plot(df['Date'], df['Value'])\n    \n    # Set the title and labels\n    ax.set_title('Time Series Data')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    \n    # Show the plot\n    plt.show()\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef task_func(activities):\n    \"\"\"\n    \n    \"\"\"\n    # Create a dictionary to store the start and end times of each activity\n    activity_times = defaultdict(list)\n    \n    # Iterate over the activities and store their start and end times\n    for activity in activities:\n        activity_times[activity[0]].append(activity[1])\n        activity_times[activity[0]].append(activity[2])\n    \n    # Sort the activities by their start times\n    sorted_activities = sorted(activity_times.items(), key=lambda x: x[1][0])\n    \n    # Initialize the current end time to 0\n    current_end_time = 0\n    \n    # Initialize the maximum number of activities to 0\n    max_activities = 0\n    \n    # Initialize the list of activities to be selected\n    selected_activities = []\n    \n    # Iterate over the sorted activities\n    for activity, times in sorted_activities:\n        # Check if the start time of the current activity is after the current end time\n        if times[0] > current_end_time:\n            # Add the current activity to the list of selected activities\n            selected_activities.append(activity)\n            \n            # Update the current end time to the end time of the current activity\n            current_end_time = times[1]\n            \n            # Update the maximum number of activities to the length of the list of selected activities\n            max_activities = len(selected_activities)\n    \n    # Return the maximum number of activities and the list of selected activities\n    return max_activities, selected_activities\n\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Set the seed for reproducibility\n    random.seed(seed)\n\n    # Create the destination directory if it doesn't exist\n    os.makedirs(dest_dir, exist_ok=True)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n\n    # Shuffle the list of files\n    random.shuffle(files)\n\n    # Split the list of files into two halves\n    split_point = len(files) // 2\n    train_files = files[:split_point]\n    test_files = files[split_point:]\n\n    # Copy the files to the train and test directories\n    for file in train_files:\n        src_path = os.path.join(src_dir, file)\n        dest_path = os.path.join(dest_dir, 'train', file)\n        shutil.copy(src_path, dest_path)\n\n    for file in test_files:\n        src_path = os.path.join(src_dir, file)\n        dest_path = os.path.join(dest_dir, 'test', file)\n        shutil.copy(src_path, dest_path)\n\n    return dest_dir\n\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    \n    \"\"\"\n    # Get all xlsx files in the directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n\n    # Loop through each xlsx file\n    for xlsx_file in xlsx_files:\n        # Load the xlsx file\n        wb = load_workbook(xlsx_file)\n        # Get the first sheet\n        sheet = wb.active\n\n        # Loop through each row in the sheet\n        for row in sheet.iter_rows(min_row=2, values_only=True):\n            # Get the value of the first cell in the row\n            value = row[0]\n\n            # Check if the value is a string\n            if isinstance(value, str):\n                # Check if the value contains a date\n                if re.search(r'\\d{4}-\\d{2}-\\d{2}', value):\n                    # Replace the date with a link\n                    value = re.sub(r'(\\d{4}-\\d{2}-\\d{2})', r'=\\1', value)\n\n            # Update the value in the cell\n            sheet.cell(row=row[0].row, column=1, value=value)\n\n        # Save the xlsx file\n        wb.save(xlsx_file)\n\n"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random numbers\n    random_numbers = np.random.rand(sample_size)\n    \n    # Calculate the sum of the random numbers\n    sum_random_numbers = np.sum(random_numbers)\n    \n    # Calculate the mean of the random numbers\n    mean_random_numbers = sum_random_numbers / sample_size\n    \n    # Calculate the standard deviation of the random numbers\n    std_dev_random_numbers = np.std(random_numbers)\n    \n    # Calculate the variance of the random numbers\n    variance_random_numbers = np.var(random_numbers)\n    \n    # Calculate the skewness of the random numbers\n    skewness_random_numbers = np.mean((random_numbers - mean_random_numbers) ** 3) / (std_dev_random_numbers ** 3)\n    \n    # Calculate the kurtosis of the random numbers\n    kurtosis_random_numbers = np.mean((random_numbers - mean_random_numbers) ** 4) / (std_dev_random_numbers ** 4)\n    \n    # Calculate the autocorrelation of the random numbers\n    autocorrelation_random_numbers = np.correlate(random_numbers, random_numbers, mode='full')[sample_size - 1] / (sample_size * std_dev_random_numbers ** 2)\n    \n    # Calculate the power spectral density of the random numbers\n    power_spectral_density_random_numbers = np.abs(np.fft.fft(random_numbers)) ** 2 / sample_size\n    \n    # Calculate the entropy of the random numbers\n    entropy_random_numbers = -np.sum(np.histogram(random_numbers, bins=sample_size)[0] / sample_size * np.log2(np.histogram(random_numbers, bins=sample_size)[0] / sample_size))\n    \n    # Calculate the correlation coefficient between the random numbers and their indices\n    correlation_coefficient_random_numbers = np.corrcoef(random_numbers, np.arange(sample_size))[0, 1]\n    \n    # Calculate the correlation coefficient between the random numbers and their squares\n    correlation_coefficient_random_numbers_squared = np.corrcoef(random_numbers, random_numbers ** 2)[0, 1]\n    \n    # Calculate the correlation coefficient between the random numbers and their cubes\n    correlation_coefficient_random_numbers_cubed = np.corrcoef(random_numbers, random_numbers ** 3)[0, 1]\n    \n    # Calculate the correlation coefficient between the random numbers and their fourth powers\n    correlation_coefficient_random_numbers_fourth = np"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = 'smtp.gmail.com'\n    app.config['MAIL_PORT'] = 465\n    app.config['MAIL_USERNAME'] = 'your_email@gmail.com'\n    app.config['MAIL_PASSWORD'] = 'your_password'\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USE_SSL'] = True\n\n    mail = Mail(app)\n\n    @app.route('/')\n    def index():\n        mail.send_message('Hello', sender='your_email@gmail.com', recipients=['recipient_email@gmail.com'])\n        return 'Email sent!'\n\n    if __name__ == '__main__':\n        app.run(debug=True)\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    \n    \"\"\"\n    # Load the Excel file\n    df = pd.read_excel(excel_file_path)\n    \n    # Check if the column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column '{column_name}' does not exist in the Excel file.\")\n    \n    # Get the column data\n    column_data = df[column_name].values\n    \n    # Calculate the mean\n    mean_value = np.mean(column_data)\n    \n    # Calculate the standard deviation\n    std_dev = np.std(column_data)\n    \n    # Calculate the variance\n    variance = np.var(column_data)\n    \n    # Calculate the median\n    median_value = np.median(column_data)\n    \n    # Calculate the mode\n    mode_value = df[column_name].mode()[0]\n    \n    # Calculate the range\n    range_value = np.ptp(column_data)\n    \n    # Calculate the minimum\n    min_value = np.min(column_data)\n    \n    # Calculate the maximum\n    max_value = np.max(column_data)\n    \n    # Calculate the sum\n    sum_value = np.sum(column_data)\n    \n    # Calculate the count\n    count_value = len(column_data)\n    \n    # Calculate the skewness\n    skewness_value = df[column_name].skew()\n    \n    # Calculate the kurtosis\n    kurtosis_value = df[column_name].kurt()\n    \n    # Create a dictionary with the calculated statistics\n    statistics_dict = {\n        \"Mean\": mean_value,\n        \"Standard Deviation\": std_dev,\n        \"Variance\": variance,\n        \"Median\": median_value,\n        \"Mode\": mode_value,\n        \"Range\": range_value,\n        \"Minimum\": min_value,\n        \"Maximum\": max_value,\n        \"Sum\": sum_value,\n        \"Count\": count_value,\n        \"Skewness\": skewness_value,\n        \"Kurtosis\": kurtosis_value\n    }\n    \n    # Create a new Excel file with the calculated statistics\n    new_excel_file_path = os.path.join(os.path.dirname(excel_file_path), f\"statistics_{file_name}\")\n    df_statistics = pd.DataFrame(statistics_dict, index=[0])\n    df_statistics.to_excel(new_excel_file_path, index=False)\n    \n    return new_excel_file_path\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Create a neural network model\n    model = Sequential()\n    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Compile the model\n    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n    # Train the model\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test))\n\n    # Evaluate the model\n    loss, accuracy = model.evaluate(X_test, Y_test)\n    print('Test loss:', loss)\n    print('Test accuracy:', accuracy)\n\n    # Plot the training and validation accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot the training and validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    return model\n"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Define the model architecture\n    model = keras.Sequential([\n        keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, Y_test)\n    print(f'Test accuracy: {accuracy:.2f}')\n\n    # Make predictions on the test set\n    Y_pred = model.predict(X_test)\n\n    # Compute the ROC curve and AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model\n"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path)\n    \n    # Convert the image to the HSV color space\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # Reshape the image to a 2D array of pixels\n    pixel_values = hsv_image.reshape((-1, 3))\n    \n    # Convert the pixel values to float type\n    pixel_values = np.float32(pixel_values)\n    \n    # Apply k-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixel_values)\n    \n    # Get the cluster centers\n    centers = kmeans.cluster_centers_\n    \n    # Convert the cluster centers back to uint8 type\n    centers = np.uint8(centers)\n    \n    # Create a new image with the cluster centers as colors\n    segmented_image = centers[kmeans.labels_].reshape(hsv_image.shape)\n    \n    # Convert the segmented image back to the BGR color space\n    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_HSV2BGR)\n    \n    # Save the segmented image\n    cv2.imwrite('segmented_image.jpg', segmented_image)\n    \n    # Return the cluster centers\n    return centers\n"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(P)\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(P[:, 0], P[:, 1], c=labels, cmap='viridis')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('KMeans Clustering')\n\n    return labels, ax\n\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\n\ndef task_func(points, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    n_points = len(points)\n    vor = Voronoi(points)\n    voronoi_plot_2d(vor)\n    plt.scatter(points[:, 0], points[:, 1], c='r')\n    plt.show()\n    return vor\n\n"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    \n    \"\"\"\n    # Create the destination directory if it doesn't exist\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # Get a list of all files with the specified extension in the source directory\n    files = glob.glob(os.path.join(src_dir, f'*.{ext}'))\n    \n    # Copy each file to the destination directory\n    for file in files:\n        shutil.copy(file, dest_dir)\n\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\n\ndef task_func(json_str):\n    \"\"\"\n    \n    \"\"\"\n    # \u89e3\u6790json\u5b57\u7b26\u4e32\n    data = json.loads(json_str)\n    \n    # \u63d0\u53d6\u9700\u8981\u7684\u4fe1\u606f\n    result = []\n    for item in data:\n        # \u63d0\u53d6id\n        id = item.get('id')\n        # \u63d0\u53d6title\n        title = item.get('title')\n        # \u63d0\u53d6content\n        content = item.get('content')\n        # \u63d0\u53d6url\n        url = item.get('url')\n        # \u63d0\u53d6time\n        time = item.get('time')\n        # \u63d0\u53d6source\n        source = item.get('source')\n        # \u63d0\u53d6author\n        author = item.get('author')\n        # \u63d0\u53d6keywords\n        keywords = item.get('keywords')\n        # \u63d0\u53d6abstract\n        abstract = item.get('abstract')\n        # \u63d0\u53d6references\n        references = item.get('references')\n        # \u63d0\u53d6categories\n        categories = item.get('categories')\n        # \u63d0\u53d6tags\n        tags = item.get('tags')\n        # \u63d0\u53d6images\n        images = item.get('images')\n        # \u63d0\u53d6videos\n        videos = item.get('videos')\n        # \u63d0\u53d6audios\n        audios = item.get('audios')\n        # \u63d0\u53d6files\n        files = item.get('files')\n        # \u63d0\u53d6pdf\n        pdf = item.get('pdf')\n        # \u63d0\u53d6html\n        html = item.get('html')\n        # \u63d0\u53d6xml\n        xml = item.get('xml')\n        # \u63d0\u53d6json\n        json = item.get('json')\n        # \u63d0\u53d6csv\n        csv = item.get('csv')\n        # \u63d0\u53d6tsv\n        tsv = item.get('tsv')\n        # \u63d0\u53d6txt\n        txt = item.get('txt')\n        # \u63d0\u53d6doc\n        doc = item.get('doc')\n        # \u63d0\u53d6docx\n        docx = item.get('docx')\n        # \u63d0\u53d6ppt\n        ppt = item.get('ppt')\n        # \u63d0\u53d6pptx\n        pptx = item.get('pptx')\n        # \u63d0\u53d6xls\n        xls = item.get('xls')\n        # \u63d0\u53d6xlsx\n        xlsx = item.get('xlsx')\n        # \u63d0\u53d6zip\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    start_time = time.time()\n    process = subprocess.Popen(['python', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    while True:\n        if process.poll() is not None:\n            break\n        if time.time() - start_time > timeout:\n            process.terminate()\n            break\n        time.sleep(0.1)\n    stdout, stderr = process.communicate()\n    return {\n        'stdout': stdout.decode(),\n        'stderr': stderr.decode(),\n        'returncode': process.returncode\n    }\n\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    # Create a DataFrame with random data\n    df = pd.DataFrame({\n        \"Category\": np.random.choice(CATEGORIES, N),\n        \"Value\": np.random.rand(N)\n    })\n    \n    # Group by Category and calculate the mean of Value\n    grouped = df.groupby(\"Category\")[\"Value\"].mean()\n    \n    # Plot the grouped data\n    grouped.plot(kind=\"bar\")\n    plt.title(\"Mean Value by Category\")\n    plt.xlabel(\"Category\")\n    plt.ylabel(\"Mean Value\")\n    plt.show()\n    \n    return grouped\n\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    # Generate random data\n    x = np.arange(start_time, end_time, step)\n    y = np.random.randn(len(x))\n    # Add trend\n    y += trend * x\n    # Create dataframe\n    df = pd.DataFrame({'x': x, 'y': y})\n    # Add datetime index\n    df['datetime'] = pd.to_datetime(df['x'], unit='s')\n    df.set_index('datetime', inplace=True)\n    return df\n\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(random_seed)\n    # Convert epoch milliseconds to datetime\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000.0)\n    # Generate random number of products\n    num_products = random.randint(1, 5)\n    # Select random products\n    selected_products = random.sample(products, num_products)\n    # Generate random quantities for each product\n    quantities = [random.randint(1, 10) for _ in range(num_products)]\n    # Create a dictionary with product names as keys and quantities as values\n    product_dict = dict(zip(selected_products, quantities))\n    # Create a DataFrame\n    df = pd.DataFrame.from_dict(product_dict, orient=\"index\", columns=[\"Quantity\"])\n    # Add a datetime column\n    df[\"Datetime\"] = dt\n    # Add a random_id column\n    df[\"random_id\"] = random.randint(1, 1000)\n    # Return the DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    \n    \"\"\"\n    # \u89e3\u6790json\u5b57\u7b26\u4e32\n    data = json.loads(json_str)\n    \n    # \u521b\u5efa\u4e00\u4e2aWorkbook\u5bf9\u8c61\n    workbook = xlwt.Workbook()\n    \n    # \u521b\u5efa\u4e00\u4e2aWorksheet\u5bf9\u8c61\n    worksheet = workbook.add_sheet(sheet_name)\n    \n    # \u5199\u5165\u8868\u5934\n    worksheet.write(0, 0, \"id\")\n    worksheet.write(0, 1, \"name\")\n    worksheet.write(0, 2, \"age\")\n    \n    # \u5199\u5165\u6570\u636e\n    for i, item in enumerate(data):\n        worksheet.write(i+1, 0, item[\"id\"])\n        worksheet.write(i+1, 1, item[\"name\"])\n        worksheet.write(i+1, 2, item[\"age\"])\n    \n    # \u4fdd\u5b58Excel\u6587\u4ef6\n    workbook.save(filename)\n    \n    # \u8fd4\u56de\u6587\u4ef6\u8def\u5f84\n    return os.path.abspath(filename)\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(random_seed)\n    # Create a list of dates\n    dates = [datetime.now() - timedelta(days=i) for i in range(days_in_past)]\n    # Create a list of random numbers\n    numbers = [random.randint(1, 100) for _ in range(days_in_past)]\n    # Create a dataframe\n    df = pd.DataFrame({'date': dates, 'number': numbers})\n    # Sort the dataframe by date\n    df = df.sort_values('date')\n    # Set the index to date\n    df.set_index('date', inplace=True)\n    # Return the dataframe\n    return df\n\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(random_seed)\n    # Create a DataFrame with random stock prices\n    df = pd.DataFrame(\n        np.random.rand(100, len(stock_names)),\n        columns=stock_names,\n        index=pd.date_range(\n            start=datetime.now() - pd.Timedelta(days=days_in_past),\n            periods=100,\n            freq=\"D\",\n        ),\n    )\n    return df\n\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    \n    \"\"\"\n    # Read the two CSV files into pandas dataframes\n    df1 = pd.read_csv(file_path1, delimiter=delimiter, quotechar=quotechar)\n    df2 = pd.read_csv(file_path2, delimiter=delimiter, quotechar=quotechar)\n    \n    # Compare the two dataframes\n    diff = df1.compare(df2)\n    \n    # Write the differences to a new CSV file\n    diff.to_csv('diff.csv', index=False)\n    \n    # Return the number of differences\n    return len(diff)\n\n"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the column\n    mean = np.mean(data[column])\n    \n    # Calculate the standard deviation of the column\n    std_dev = np.std(data[column])\n    \n    # Calculate the z-score for each value in the column\n    z_scores = (data[column] - mean) / std_dev\n    \n    # Return the z-scores\n    return z_scores\n\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the column\n    mean = np.mean(data[column])\n    \n    # Calculate the standard deviation of the column\n    std_dev = np.std(data[column])\n    \n    # Calculate the z-score for each value in the column\n    z_scores = (data[column] - mean) / std_dev\n    \n    # Return the z-scores\n    return z_scores\n\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    \n    \"\"\"\n    with open(input_file, \"r\") as f:\n        data = json.load(f)\n    \n    # Create a dictionary to store the results\n    results = defaultdict(list)\n    \n    # Iterate over the data\n    for item in data:\n        # Extract the relevant information\n        id = item[\"id\"]\n        text = item[\"text\"]\n        label = item[\"label\"]\n        label_text = item[\"label_text\"]\n        \n        # Append the information to the results dictionary\n        results[\"id\"].append(id)\n        results[\"text\"].append(text)\n        results[\"label\"].append(label)\n        results[\"label_text\"].append(label_text)\n    \n    # Convert the results dictionary to a pandas DataFrame\n    df = pd.DataFrame(results)\n    \n    # Calculate the number of occurrences of each label_text\n    label_counts = df[\"label_text\"].value_counts()\n    \n    # Calculate the percentage of each label_text\n    label_percentages = label_counts / len(df) * 100\n    \n    # Create a new DataFrame with the label_text and its corresponding counts and percentages\n    label_df = pd.DataFrame({\"label_text\": label_counts.index, \"counts\": label_counts.values, \"percentages\": label_percentages.values})\n    \n    # Sort the DataFrame by the counts in descending order\n    label_df = label_df.sort_values(by=\"counts\", ascending=False)\n    \n    # Print the results\n    print(label_df)\n    \n    # Return the results\n    return label_df\n\n"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n    \n    # Extract the 'text' column\n    text_data = [row[0] for row in data]\n    \n    # Count the occurrences of each word\n    word_counts = Counter(' '.join(text_data).split())\n    \n    # Create a DataFrame from the word counts\n    df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n    \n    # Sort the DataFrame by 'Count' in descending order\n    df = df.sort_values(by='Count', ascending=False)\n    \n    # Plot the word counts\n    plt.figure(figsize=(10, 6))\n    plt.bar(df['Word'], df['Count'])\n    plt.xlabel('Word')\n    plt.ylabel('Count')\n    plt.title('Word Count')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \n    # Return the DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: Your code here\n    return Counter(), None\n\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, bins=4):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the number of bins\n    num_bins = bins\n    \n    # Calculate the bin edges\n    bin_edges = np.histogram_bin_edges(df, bins=num_bins)\n    \n    # Calculate the bin counts\n    bin_counts, _ = np.histogram(df, bins=bin_edges)\n    \n    # Calculate the bin centers\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    \n    # Calculate the bin widths\n    bin_widths = bin_edges[1:] - bin_edges[:-1]\n    \n    # Calculate the bin heights\n    bin_heights = bin_counts / bin_widths\n    \n    # Calculate the mean and standard deviation of the data\n    mean = np.mean(df)\n    std = np.std(df)\n    \n    # Calculate the normal distribution values\n    normal_values = norm.pdf(bin_centers, mean, std)\n    \n    # Calculate the chi-squared statistic\n    chi_squared = np.sum((bin_heights - normal_values) ** 2 / normal_values)\n    \n    # Calculate the degrees of freedom\n    degrees_of_freedom = len(bin_counts) - 2\n    \n    # Calculate the p-value\n    p_value = 1 - norm.cdf(chi_squared, df=degrees_of_freedom)\n    \n    # Plot the histogram and the normal distribution\n    plt.hist(df, bins=bin_edges, alpha=0.5, label='Data')\n    plt.plot(bin_centers, normal_values, 'r', label='Normal Distribution')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram and Normal Distribution')\n    plt.legend()\n    plt.show()\n    \n    return chi_squared, p_value\n\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\n\ndef task_func(a, b):\n    \"\"\"\n    \n    \"\"\"\n    return a + b\n\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the 'date' column to datetime\n    data['date'] = pd.to_datetime(data['date'])\n    \n    # Group the data by 'date' and 'name' and calculate the sum of 'value'\n    grouped_data = data.groupby(['date', 'name']).sum().reset_index()\n    \n    # Create a pivot table with 'date' as the index, 'name' as the columns, and 'value' as the values\n    pivot_table = grouped_data.pivot_table(index='date', columns='name', values='value')\n    \n    # Plot the pivot table\n    pivot_table.plot()\n    plt.show()\n    \n    return pivot_table\n\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate the mean of each column\n    mean_values = df.mean()\n    \n    # Calculate the standard deviation of each column\n    std_values = df.std()\n    \n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n    \n    # Plot the correlation matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n    plt.title('Correlation Matrix')\n    plt.show()\n    \n    return mean_values, std_values, corr_matrix\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random data\n    x = np.linspace(0, 10, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n    \n    # Fit the data to a sine function\n    popt, pcov = curve_fit(lambda x, a, b, c: a * np.sin(b * x + c), x, y)\n    \n    # Plot the data and the fitted curve\n    plt.plot(x, y, 'o', label='Data')\n    plt.plot(x, popt[0] * np.sin(popt[1] * x + popt[2]), label='Fitted curve')\n    plt.legend()\n    plt.show()\n    \n    return popt, pcov\n\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(csv_file, 'r', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n    \n    # Extract the text from the CSV file\n    text = ' '.join([row[0] for row in data])\n    \n    # Remove punctuation and convert to lowercase\n    text = ''.join(c for c in text if c.isalnum() or c.isspace()).lower()\n    \n    # Tokenize the text\n    tokens = text.split()\n    \n    # Count the frequency of each token\n    token_counts = Counter(tokens)\n    \n    # Find the most common tokens\n    most_common_tokens = token_counts.most_common(10)\n    \n    # Print the most common tokens\n    for token, count in most_common_tokens:\n        print(f'{token}: {count}')\n    \n    # Plot the frequency of each token\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(len(most_common_tokens)), [count for token, count in most_common_tokens], tick_label=[token for token, count in most_common_tokens])\n    plt.xlabel('Tokens')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Tokens')\n    plt.xticks(rotation=45)\n    plt.show()\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef task_func(size=1000):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random data\n    x = np.random.normal(0, 1, size)\n    y = np.random.normal(0, 1, size)\n    \n    # Calculate the correlation coefficient\n    corr, _ = stats.pearsonr(x, y)\n    \n    return corr\n\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # \u751f\u6210RSA\u5bc6\u94a5\u5bf9\n    key = rsa.generate_keys(1024)\n    public_key = key[0]\n    private_key = key[1]\n\n    # \u751f\u6210AES\u5bc6\u94a5\n    aes_key = get_random_bytes(16)\n\n    # \u52a0\u5bc6AES\u5bc6\u94a5\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n\n    # \u751f\u6210\u968f\u673aIV\n    iv = get_random_bytes(16)\n\n    # \u52a0\u5bc6\u6570\u636e\n    data = b\"Hello, World!\"\n    cipher = AES.new(aes_key, AES.MODE_CBC, iv)\n    encrypted_data = cipher.encrypt(data)\n\n    # \u5c06\u52a0\u5bc6\u540e\u7684\u6570\u636e\u3001\u52a0\u5bc6\u540e\u7684AES\u5bc6\u94a5\u548cIV\u6253\u5305\n    encrypted_message = b64encode(encrypted_data + encrypted_aes_key + iv)\n\n    # \u5c06\u52a0\u5bc6\u540e\u7684\u6570\u636e\u3001\u52a0\u5bc6\u540e\u7684AES\u5bc6\u94a5\u548cIV\u5199\u5165\u6587\u4ef6\n    with open(\"encrypted_message.txt\", \"wb\") as f:\n        f.write(encrypted_message)\n\n    # \u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u52a0\u5bc6\u540e\u7684\u6570\u636e\u3001\u52a0\u5bc6\u540e\u7684AES\u5bc6\u94a5\u548cIV\n    with open(\"encrypted_message.txt\", \"rb\") as f:\n        encrypted_message = f.read()\n\n    # \u89e3\u5305\u52a0\u5bc6\u540e\u7684\u6570\u636e\u3001\u52a0\u5bc6\u540e\u7684AES\u5bc6\u94a5\u548cIV\n    encrypted_data = b64decode(encrypted_message[:-32])\n    encrypted_aes_key = encrypted_message[-32:-16]\n    iv = encrypted_message[-16:]\n\n    # \u89e3\u5bc6AES\u5bc6\u94a5\n    aes_key = rsa.decrypt(encrypted_aes_key, private_key)\n\n    # \u89e3\u5bc6\u6570\u636e\n    cipher = AES.new(aes_key, AES.MODE_CBC, iv)\n    decrypted_data = cipher.decrypt(encrypted_data)\n\n    # \u6253\u5370\u89e3\u5bc6\u540e\u7684\u6570\u636e\n    print(decrypted_data.decode())\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Generate RSA key pair\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n        backend=default_backend()\n    )\n    public_key = key.public_key()\n\n    # Read file content\n    with open(file_path, 'rb') as file:\n        file_content = file.read()\n\n    # Encrypt file content with RSA\n    encrypted_content = public_key.encrypt(\n        file_content,\n        padding.OAEP(\n            mgf=padding.MGF1(algorithm=hashes.SHA256()),\n            algorithm=hashes.SHA256(),\n            label=None\n        )\n    )\n\n    # Save encrypted content to a new file\n    encrypted_file_path = os.path.join(os.path.dirname(file_path), 'encrypted_' + os.path.basename(file_path))\n    with open(encrypted_file_path, 'wb') as file:\n        file.write(encrypted_content)\n\n    # Return the path to the encrypted file\n    return encrypted_file_path\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n    response = urllib.request.urlopen(url)\n    html = response.read()\n    doc = pq(html)\n    \n    # \u83b7\u53d6\u6807\u9898\n    title = doc('h1').text()\n    \n    # \u83b7\u53d6\u65f6\u95f4\n    time_str = doc('span.time').text()\n    time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n    \n    # \u83b7\u53d6\u6b63\u6587\n    content = doc('div.text').text()\n    \n    # \u83b7\u53d6\u8bc4\u8bba\u6570\n    comment_count = doc('span.comment').text()\n    \n    # \u83b7\u53d6\u70b9\u8d5e\u6570\n    like_count = doc('span.like').text()\n    \n    # \u83b7\u53d6\u6536\u85cf\u6570\n    favorite_count = doc('span.favorite').text()\n    \n    # \u83b7\u53d6\u5206\u4eab\u6570\n    share_count = doc('span.share').text()\n    \n    # \u83b7\u53d6\u9605\u8bfb\u6570\n    read_count = doc('span.read').text()\n    \n    # \u83b7\u53d6\u6807\u7b7e\n    tags = doc('div.tags a').text()\n    \n    # \u83b7\u53d6\u4f5c\u8005\n    author = doc('div.author a').text()\n    \n    # \u83b7\u53d6\u6765\u6e90\n    source = doc('div.source a').text()\n    \n    # \u83b7\u53d6\u5206\u7c7b\n    category = doc('div.category a').text()\n    \n    # \u83b7\u53d6\u8bc4\u8bba\u5185\u5bb9\n    comments = []\n    for comment in doc('div.comment'):\n        comment_content = pq(comment).text()\n        comments.append(comment_content)\n    \n    # \u83b7\u53d6\u70b9\u8d5e\u7528\u6237\n    like_users = []\n    for like_user in doc('div.like-user a'):\n        like_user_name = pq(like_user).text()\n        like_users.append(like_user_name)\n    \n    # \u83b7\u53d6\u6536\u85cf\u7528\u6237\n    favorite_users = []\n    for favorite_user in doc('div.favorite-user a'):\n        favorite_user_name = pq(favorite_user).text()\n        favorite_users.append(favorite_user_name)\n    \n    # \u83b7\u53d6\u5206\u4eab\u7528\u6237\n    share_users = []\n    for share_user in doc('div.share-user a'):\n        share_user_name = pq(share_user).text()\n        share_users.append(share_user_name)\n    \n    # \u83b7\u53d6\u9605\u8bfb\u7528\u6237\n    read_users = []\n    for read_user in doc('div.read-user a'):\n        read_user_name = pq(read_user).text()\n       "}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Generate random data\n    data = []\n    for _ in range(hours * 60):\n        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        sensor = SENSORS[randint(0, len(SENSORS) - 1)]\n        value = randint(0, 100)\n        data.append([timestamp, sensor, value])\n    \n    # Write data to CSV file\n    filename = f'{output_dir}/data_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv'\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Timestamp', 'Sensor', 'Value'])\n        writer.writerows(data)\n    \n    print(f'Generated {len(data)} data points and saved to {filename}')\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create a list to store the data\n    data = []\n    \n    # Generate data for each hour\n    for hour in range(hours):\n        # Generate a random number of vehicles for each type\n        num_cars = randint(0, 100)\n        num_buses = randint(0, 50)\n        num_trucks = randint(0, 30)\n        num_bikes = randint(0, 200)\n        \n        # Calculate the total number of vehicles\n        total_vehicles = num_cars + num_buses + num_trucks + num_bikes\n        \n        # Calculate the percentage of each type\n        percent_cars = (num_cars / total_vehicles) * 100\n        percent_buses = (num_buses / total_vehicles) * 100\n        percent_trucks = (num_trucks / total_vehicles) * 100\n        percent_bikes = (num_bikes / total_vehicles) * 100\n        \n        # Append the data to the list\n        data.append([hour, num_cars, num_buses, num_trucks, num_bikes, percent_cars, percent_buses, percent_trucks, percent_bikes])\n    \n    # Create a CSV file to store the data\n    with open(os.path.join(output_dir, 'vehicle_data.csv'), 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Hour', 'Cars', 'Buses', 'Trucks', 'Bikes', 'Percent Cars', 'Percent Buses', 'Percent Trucks', 'Percent Bikes'])\n        writer.writerows(data)\n    \n    # Create a bar chart to visualize the data\n    df = pd.read_csv(os.path.join(output_dir, 'vehicle_data.csv'))\n    df = df.drop(['Hour', 'Percent Cars', 'Percent Buses', 'Percent Trucks', 'Percent Bikes'], axis=1)\n    df.plot(kind='bar', stacked=True)\n    plt.xlabel('Hour')\n    plt.ylabel"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Create a list to store the data\n    data = []\n    \n    # Generate data for each hour\n    for hour in range(hours):\n        # Generate a random weather condition\n        weather_condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        \n        # Generate a random temperature\n        temperature = randint(-20, 50)\n        \n        # Generate a random wind speed\n        wind_speed = randint(0, 30)\n        \n        # Generate a random humidity\n        humidity = randint(0, 100)\n        \n        # Generate a random pressure\n        pressure = randint(900, 1100)\n        \n        # Generate a random visibility\n        visibility = randint(0, 10)\n        \n        # Generate a random precipitation\n        precipitation = randint(0, 100)\n        \n        # Generate a random cloud cover\n        cloud_cover = randint(0, 100)\n        \n        # Generate a random air quality index\n        air_quality_index = randint(0, 500)\n        \n        # Generate a random solar radiation\n        solar_radiation = randint(0, 1000)\n        \n        # Generate a random wind direction\n        wind_direction = randint(0, 360)\n        \n        # Generate a random wind gust\n        wind_gust = randint(0, 30)\n        \n        # Generate a random dew point\n        dew_point = randint(-20, 50)\n        \n        # Generate a random heat index\n        heat_index = randint(-20, 50)\n        \n        # Generate a random wind chill\n        wind_chill = randint(-20, 50)\n        \n        # Generate a random apparent temperature\n        apparent_temperature = randint(-20, 50)\n        \n        # Generate a random comfort index\n        comfort_index = randint(0, 100)\n        \n        # Generate a random uv index\n        uv_index = randint(0, 15)\n        \n        # Generate a random precipitation type\n        precipitation_type = WEATHER_CONDITION"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the number of goals scored by each team\n    goals_per_team = [goals[team] for team in TEAMS]\n    \n    # Calculate the number of penalties scored by each team\n    penalties_per_team = [penalties[team] for team in TEAMS]\n    \n    # Calculate the total number of goals scored\n    total_goals = sum(goals_per_team)\n    \n    # Calculate the total number of penalties scored\n    total_penalties = sum(penalties_per_team)\n    \n    # Calculate the total penalty cost\n    total_penalty_cost = total_penalties * PENALTY_COST\n    \n    # Calculate the total score\n    total_score = total_goals - total_penalty_cost\n    \n    return total_score\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\n\ndef task_func(rows):\n    \"\"\"\n    \n    \"\"\"\n    # Create a DataFrame from the rows\n    df = pd.DataFrame(rows, columns=COLUMNS)\n    \n    # Calculate the mean of each column\n    means = df.mean()\n    \n    # Calculate the standard deviation of each column\n    stds = df.std()\n    \n    # Calculate the correlation matrix\n    corr_matrix = df.corr()\n    \n    # Calculate the covariance matrix\n    cov_matrix = df.cov()\n    \n    # Calculate the skewness of each column\n    skews = df.skew()\n    \n    # Calculate the kurtosis of each column\n    kurtoses = df.kurtosis()\n    \n    # Calculate the median of each column\n    medians = df.median()\n    \n    # Calculate the mode of each column\n    modes = df.mode().iloc[0]\n    \n    # Calculate the range of each column\n    ranges = df.max() - df.min()\n    \n    # Calculate the IQR of each column\n    iqrs = df.quantile(0.75) - df.quantile(0.25)\n    \n    # Calculate the variance of each column\n    variances = df.var()\n    \n    # Calculate the coefficient of variation of each column\n    cv = stds / means\n    \n    # Calculate the coefficient of skewness of each column\n    cs = skews / (stds / np.sqrt(len(df)))\n    \n    # Calculate the coefficient of kurtosis of each column\n    ck = kurtoses / (stds ** 2)\n    \n    # Calculate the coefficient of variation of the range of each column\n    cvr = ranges / means\n    \n    # Calculate the coefficient of variation of the IQR of each column\n    cvi = iqrs / means\n    \n    # Calculate the coefficient of variation of the variance of each column\n    cvv = variances / means\n    \n    # Calculate the coefficient of variation of the standard deviation of each column\n    cvs = stds / means\n    \n    # Calculate the coefficient of variation of the skewness of each column\n    cvs = skews / means\n    \n    # Calculate the coefficient of variation of the kurtosis of each column\n    cvk = kurtoses / means\n    \n    # Calculate the coefficient of variation of the"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\ndef task_func(num_students):\n    \"\"\"\n    \n    \"\"\"\n    return num_students * 2\n\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\n\ndef task_func(array, target_value):\n    \"\"\"\n    \n    \"\"\"\n    return np.sum(np.abs(array - target_value))\n\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    \n    \"\"\"\n    # Preprocess the text data\n    texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts]\n\n    # Create a TF-IDF vectorizer\n    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(texts)\n\n    # Apply NMF to the TF-IDF matrix\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(tfidf_matrix)\n\n    # Get the topic names\n    feature_names = vectorizer.get_feature_names_out()\n    topic_names = ['Topic ' + str(i) for i in range(num_topics)]\n\n    # Get the top words for each topic\n    top_words = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_features_ind = topic.argsort()[:-11:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        top_words.append(top_features)\n\n    # Create a dictionary of topics and their top words\n    topics = {topic_names[i]: top_words[i] for i in range(num_topics)}\n\n    return topics\n"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    \n    \"\"\"\n    # Tokenize\n    texts = [ALPHANUMERIC.sub(' ', text).lower().split() for text in texts]\n    # Remove stopwords\n    if stopwords is not None:\n        texts = [[word for word in text if word not in stopwords] for text in texts]\n    # Train word2vec\n    model = Word2Vec(texts, min_count=1)\n    # Get word vectors\n    word_vectors = {word: model.wv[word] for word in model.wv.vocab}\n    return word_vectors\n\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bfb\u53d6json\u6587\u4ef6\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # \u521b\u5efa\u65b0\u7684json\u6587\u4ef6\n    new_data = {}\n    new_data['prompt'] = data['prompt']\n    new_data['response'] = data['response']\n    new_data['reference'] = data['reference']\n    new_data['history'] = data['history']\n    new_data['model'] = data['model']\n    new_data['tool_call'] = data['tool_call']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool_calls']\n    new_data['tool_calls'] = data['tool"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Get the current date and time\n    now = datetime.now()\n    # Format the date and time as a string\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    # Create a list of strings to write to the file\n    data = [now_str]\n    # Open the file in append mode\n    with open(\"data.csv\", \"a\") as file:\n        # Create a CSV writer object\n        writer = csv.writer(file)\n        # Write the data to the file\n        writer.writerow(data)\n"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = urllib.request.urlopen(url)\n    html = response.read()\n    \n    # Parse the HTML content\n    soup = BeautifulSoup(html, 'html.parser')\n    \n    # Find the table element\n    table = soup.find('table')\n    \n    # Extract the table headers\n    headers = [header.text.strip() for header in table.find_all('th')]\n    \n    # Extract the table rows\n    rows = []\n    for row in table.find_all('tr'):\n        cells = [cell.text.strip() for cell in row.find_all('td')]\n        rows.append(cells)\n    \n    # Write the data to a CSV file\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(headers)\n        writer.writerows(rows)\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(data, target_column, test_size=test_size, random_state=random_state)\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Train the model using the training data\n    model.fit(X_train, y_train)\n    \n    # Make predictions using the testing data\n    y_pred = model.predict(X_test)\n    \n    # Calculate the mean squared error of the predictions\n    mse = np.mean((y_pred - y_test) ** 2)\n    \n    return mse\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \"\"\"\n    \n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Create a list of years\n    years = list(range(start_year, end_year+1))\n    \n    # Create a list of email addresses\n    email_addresses = []\n    for year in years:\n        for name in latin_names:\n            email_addresses.append(f\"{name.lower()}@{email_domain}\")\n        for name in other_names:\n            email_addresses.append(f\"{name.lower()}@{email_domain}\")\n    \n    # Create a list of dates\n    dates = []\n    for year in years:\n        for month in range(1, 13):\n            for day in range(1, 29):\n                dates.append(datetime.date(year, month, day))\n    \n    # Create a list of random numbers\n    random_numbers = np.random.rand(len(years)*len(latin_names)*2)\n    \n    # Create a list of random booleans\n    random_booleans = np.random.choice([True, False], len(years)*len(latin_names)*2)\n    \n    # Create a list of random strings\n    random_strings = []\n    for i in range(len(years)*len(latin_names)*2):\n        random_strings.append(f\"String {i}\")\n    \n    # Create a list of random lists\n    random_lists = []\n    for i in range(len(years)*len(latin_names)*2):\n        random_lists.append([i, i+1, i+2])\n    \n    # Create a list of random dictionaries\n    random_dicts = []\n    for i in range(len(years)*len(latin_names)*2):\n        random_dicts.append({'key1': i, 'key2': i+1, 'key3': i+2})\n    \n    # Create a list of random tuples\n    random_tuples = []\n    for i in range(len(years)*len(latin_names)*2):\n        random_tuples.append((i, i+1, i+2))\n    \n    # Create a"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    \n    \"\"\"\n    # Load the input file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize a dictionary to store the counts\n    counts = defaultdict(int)\n    \n    # Iterate over the data\n    for item in data:\n        # Increment the count for the item\n        counts[item] += 1\n    \n    # Write the counts to the output file\n    with open(output_file, 'w') as f:\n        writer = csv.writer(f)\n        for item, count in counts.items():\n            writer.writerow([item, count])\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of files in the directory\n    files = os.listdir(kwargs[\"input_dir\"])\n    \n    # Create the target directory if it doesn't exist\n    Path(target_dir).mkdir(parents=True, exist_ok=True)\n    \n    # Loop through the files and move the non-none files to the target directory\n    for file in files:\n        file_path = os.path.join(kwargs[\"input_dir\"], file)\n        if os.path.isfile(file_path) and os.path.getsize(file_path) > 0:\n            shutil.move(file_path, target_dir)\n    \n    # Return the target directory\n    return target_dir\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of files in the directory\n    files = os.listdir(directory)\n    \n    # Filter the files based on the pattern\n    filtered_files = [file for file in files if re.match(pattern, file)]\n    \n    # Create a list to store the results\n    results = []\n    \n    # Loop through the filtered files\n    for file in filtered_files:\n        # Create the full path to the file\n        file_path = os.path.join(directory, file)\n        \n        # Open the file in read mode\n        with open(file_path, 'r') as f:\n            # Read the contents of the file\n            contents = f.read()\n            \n            # Use regular expressions to find the matches\n            matches = re.findall(pattern, contents)\n            \n            # Add the matches to the results list\n            results.extend(matches)\n    \n    # Return the results\n    return results\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\u540d\n    file_names = os.listdir(directory)\n    \n    # \u904d\u5386\u6587\u4ef6\u540d\n    for file_name in file_names:\n        # \u5982\u679c\u6587\u4ef6\u540d\u7b26\u5408\u6b63\u5219\u8868\u8fbe\u5f0f\n        if re.match(pattern, file_name):\n            # \u83b7\u53d6\u6587\u4ef6\u8def\u5f84\n            file_path = os.path.join(directory, file_name)\n            \n            # \u6253\u5f00zip\u6587\u4ef6\n            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n                # \u89e3\u538b\u6587\u4ef6\u5230\u6307\u5b9a\u76ee\u5f55\n                zip_ref.extractall(directory)\n                \n            # \u5220\u9664zip\u6587\u4ef6\n            os.remove(file_path)\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    \n    \"\"\"\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files = glob.glob(pattern)\n\n    # Check if any files were found\n    if not files:\n        print(f\"No files found matching pattern '{pattern}'\")\n        return\n\n    # Create a tar archive of the files\n    archive_name = os.path.join(ARCHIVE_DIR, 'archive.tar')\n    subprocess.run(['tar', '-cvf', archive_name] + files)\n\n    # Print the name of the archive\n    print(f\"Created archive '{archive_name}' containing the following files:\")\n    for file in files:\n        print(f\"  {file}\")\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    # Count the occurrences of goals and penalties\n    goal_counts = Counter(int(row[1]) for row in data)\n    penalty_counts = Counter(int(row[2]) for row in data)\n\n    # Find the most common goal and penalty counts\n    most_common_goals = goal_counts.most_common()\n    most_common_penalties = penalty_counts.most_common()\n\n    # Find the team with the most goals and penalties\n    team_with_most_goals = most_common_goals[0][0] if most_common_goals else None\n    team_with_most_penalties = most_common_penalties[0][0] if most_common_penalties else None\n\n    # Find the team with the least goals and penalties\n    team_with_least_goals = most_common_goals[-1][0] if most_common_goals else None\n    team_with_least_penalties = most_common_penalties[-1][0] if most_common_penalties else None\n\n    # Find the team with the most goals and the least penalties\n    team_with_most_goals_and_least_penalties = None\n    if most_common_goals and most_common_penalties:\n        for goal, penalty in zip(most_common_goals, most_common_penalties):\n            if goal[0] == penalty[0]:\n                team_with_most_goals_and_least_penalties = goal[0]\n                break\n\n    # Find the team with the least goals and the most penalties\n    team_with_least_goals_and_most_penalties = None\n    if most_common_goals and most_common_penalties:\n        for goal, penalty in zip(most_common_goals, most_common_penalties"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    \n    \"\"\"\n    # Create a regular expression pattern to match the file names\n    pattern = re.compile(file_pattern)\n    \n    # Iterate over all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the file name matches the pattern\n        if pattern.match(filename):\n            # Construct the full path to the file\n            file_path = os.path.join(source_dir, filename)\n            \n            # Check if the file is a regular file (not a directory or other type of file)\n            if os.path.isfile(file_path):\n                # Construct the full path to the target directory\n                target_path = os.path.join(target_dir, filename)\n                \n                # Move the file to the target directory\n                shutil.move(file_path, target_path)\n                \n                # Print a message to indicate that the file was moved\n                print(f'Moved file: {file_path} to {target_path}')\n                \n            else:\n                # Print a message to indicate that the file is not a regular file\n                print(f'Skipped file: {file_path} (not a regular file)')\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    \n    \"\"\"\n    # Remove non-alphanumeric characters\n    text1 = ALPHANUMERIC.sub('', text1)\n    text2 = ALPHANUMERIC.sub('', text2)\n    \n    # Calculate the Levenshtein ratio\n    lev_ratio = ratio(text1, text2)\n    \n    return lev_ratio\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    \n    \"\"\"\n    # Sort the numbers in descending order\n    numbers.sort(reverse=True)\n    \n    # Calculate the sum of the numbers\n    total = sum(numbers)\n    \n    # Calculate the product of the numbers\n    product = reduce(lambda x, y: x * y, numbers)\n    \n    # Calculate the average of the numbers\n    average = total / len(numbers)\n    \n    # Calculate the median of the numbers\n    median = numbers[len(numbers) // 2] if len(numbers) % 2 == 1 else (numbers[len(numbers) // 2 - 1] + numbers[len(numbers) // 2]) / 2\n    \n    # Calculate the mode of the numbers\n    mode = max(set(numbers), key=numbers.count)\n    \n    # Calculate the range of the numbers\n    range_ = max(numbers) - min(numbers)\n    \n    # Calculate the variance of the numbers\n    variance = sum((x - average) ** 2 for x in numbers) / len(numbers)\n    \n    # Calculate the standard deviation of the numbers\n    std_dev = math.sqrt(variance)\n    \n    # Calculate the factorial of the sum of the numbers\n    factorial = math.factorial(total)\n    \n    # Calculate the permutations of the numbers\n    permutations_ = list(permutations(numbers))\n    \n    # Calculate the combinations of the numbers\n    combinations_ = list(combinations(numbers))\n    \n    # Calculate the greatest common divisor of the numbers\n    gcd = reduce(math.gcd, numbers)\n    \n    # Calculate the least common multiple of the numbers\n    lcm = reduce(lambda x, y: x * y // math.gcd(x, y), numbers)\n    \n    # Calculate the prime factors of the product of the numbers\n    prime_factors = prime_factors_of_product(product)\n    \n    # Calculate the prime factors of the sum of the numbers\n    prime_factors_sum = prime_factors_of_sum(total)\n    \n    # Calculate the prime factors of the average of the numbers\n    prime_factors_average = prime_factors_of_average(average)\n    \n    # Calculate the prime factors of the median of the numbers\n    prime_factors_median = prime_factors_of_median(median)\n    \n    # Calculate the prime factors of the mode of the numbers\n    prime_factors_mode = prime_factors_of_mode(mode)\n    \n    # Calculate the prime factors of the range of the numbers\n    prime_factors_range = prime_factors_of_range"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the source directory exists\n    if not os.path.exists(SOURCE_DIR):\n        print(f\"Source directory '{SOURCE_DIR}' does not exist.\")\n        return\n    \n    # Check if the destination directory exists\n    if not os.path.exists(DEST_DIR):\n        print(f\"Destination directory '{DEST_DIR}' does not exist.\")\n        return\n    \n    # Create a list of all files with the specified extensions in the source directory\n    files = []\n    for extension in EXTENSIONS:\n        files.extend(glob.glob(os.path.join(SOURCE_DIR, f\"*.{extension}\")))\n    \n    # Check if there are any files to move\n    if not files:\n        print(f\"No files with extensions {EXTENSIONS} found in source directory '{SOURCE_DIR}'.\")\n        return\n    \n    # Move the files to the destination directory\n    for file in files:\n        shutil.move(file, DEST_DIR)\n        print(f\"Moved file '{file}' to destination directory '{DEST_DIR}'.\")\n    \n    print(f\"Moved {len(files)} files with extensions {EXTENSIONS} from source directory '{SOURCE_DIR}' to destination directory '{DEST_DIR}'.\")\n"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Convert data to pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate z-score for each column\n    df_zscore = pd.DataFrame(zscore(df), columns=df.columns)\n    \n    # Scale data to [0, 1] range\n    scaler = MinMaxScaler()\n    df_scaled = pd.DataFrame(scaler.fit_transform(df_zscore), columns=df.columns)\n    \n    return df_scaled.to_dict(orient='records')\n\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the list of tuples to a DataFrame\n    df = pd.DataFrame(data_list, columns=['A', 'B', 'C'])\n    \n    # Calculate the mean of column 'C'\n    mean_C = df['C'].mean()\n    \n    # Calculate the standard deviation of column 'C'\n    std_C = df['C'].std()\n    \n    # Calculate the variance of column 'C'\n    var_C = df['C'].var()\n    \n    # Calculate the sum of column 'C'\n    sum_C = df['C'].sum()\n    \n    # Calculate the product of column 'C'\n    prod_C = df['C'].prod()\n    \n    # Calculate the minimum value of column 'C'\n    min_C = df['C'].min()\n    \n    # Calculate the maximum value of column 'C'\n    max_C = df['C'].max()\n    \n    # Calculate the range of column 'C'\n    range_C = max_C - min_C\n    \n    # Calculate the median of column 'C'\n    median_C = df['C'].median()\n    \n    # Calculate the mode of column 'C'\n    mode_C = df['C'].mode()\n    \n    # Calculate the count of column 'C'\n    count_C = df['C'].count()\n    \n    # Calculate the quantiles of column 'C'\n    quantiles_C = df['C'].quantile([0.25, 0.5, 0.75])\n    \n    # Calculate the correlation between column 'B' and 'C'\n    corr_BC = df['B'].corr(df['C'])\n    \n    # Calculate the covariance between column 'B' and 'C'\n    cov_BC = df['B'].cov(df['C'])\n    \n    # Calculate the covariance matrix of columns 'B' and 'C'\n    cov_matrix = df[['B', 'C']].cov()\n    \n    # Calculate the covariance matrix of all columns\n    cov_matrix_all = df.cov()\n    \n    # Calculate the correlation matrix of columns 'B' and 'C'\n    corr_matrix = df[['B', 'C"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    \n    \"\"\"\n    # Create a contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Return the results\n    return chi2, p, dof, expected\n\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    dice_rolls = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate the sum of the dice rolls\n    total = sum(dice_rolls)\n    \n    # Calculate the number of times each number appears\n    counts = {num: dice_rolls.count(num) for num in NUMBERS}\n    \n    # Calculate the probability of each number appearing\n    probabilities = {num: count / rolls for num, count in counts.items()}\n    \n    # Calculate the expected value of the sum\n    expected_value = sum(num * prob for num, prob in probabilities.items())\n    \n    return total, probabilities, expected_value\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Create a list to store the paths of the files to be archived\n    files_to_archive = []\n    \n    # Walk through the source directory and its subdirectories\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            # Check if the file is a text file\n            if file.endswith('.txt'):\n                # Get the full path of the file\n                file_path = os.path.join(root, file)\n                # Add the file path to the list\n                files_to_archive.append(file_path)\n    \n    # Create a zip file with the specified name\n    with zipfile.ZipFile(os.path.join(target_dir, archive_name), 'w') as zipf:\n        # Add each file to the zip file\n        for file_path in files_to_archive:\n            zipf.write(file_path, os.path.relpath(file_path, source_dir))\n    \n    # Return the path of the zip file\n    return os.path.join(target_dir, archive_name)\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Create a new column 'x' which is the sum of 'a' and 'b'\n    df['x'] = df['a'] + df['b']\n    \n    # Create a new column 'y' which is the product of 'a' and 'b'\n    df['y'] = df['a'] * df['b']\n    \n    # Create a new column 'z' which is the difference of 'a' and 'b'\n    df['z'] = df['a'] - df['b']\n    \n    # Create a new column 'w' which is the ratio of 'a' to 'b'\n    df['w'] = df['a'] / df['b']\n    \n    # Create a new column 'v' which is the square root of 'a'\n    df['v'] = np.sqrt(df['a'])\n    \n    # Create a new column 'u' which is the exponential of 'a'\n    df['u'] = np.exp(df['a'])\n    \n    # Create a new column 't' which is the natural logarithm of 'a'\n    df['t'] = np.log(df['a'])\n    \n    # Create a new column 's' which is the sine of 'a'\n    df['s'] = np.sin(df['a'])\n    \n    # Create a new column 'r' which is the cosine of 'a'\n    df['r'] = np.cos(df['a'])\n    \n    # Create a new column 'q' which is the tangent of 'a'\n    df['q'] = np.tan(df['a'])\n    \n    # Create a new column 'p' which is the arcsine of 'a'\n    df['p'] = np.arcsin(df['a'])\n    \n    # Create a new column 'o' which is the arccosine of 'a'\n    df['o'] = np.arccos(df['a'])\n    \n    # Create a new column 'n' which is the arctangent of 'a'\n    df['n'] = np.arctan(df['a'])\n    \n    # Create a new column 'm' which is the hyperbolic sine of 'a'\n    df['m'] = np.sinh(df['a'])\n    \n    # Create a new column 'l'"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate z-scores for each column\n    z_scores = zscore(df, axis=0)\n    \n    # Find the column with the highest z-score\n    max_z_score = np.max(z_scores, axis=0)\n    max_z_score_index = np.argmax(max_z_score)\n    \n    # Find the row with the highest z-score in the column with the highest z-score\n    max_z_score_row = np.argmax(z_scores[:, max_z_score_index])\n    \n    # Find the value in the cell with the highest z-score\n    max_z_score_value = df.iloc[max_z_score_row, max_z_score_index]\n    \n    # Find the number of rows with z-scores greater than the threshold\n    num_rows_above_threshold = np.sum(z_scores > z_threshold, axis=0)\n    \n    # Find the number of columns with z-scores greater than the threshold\n    num_cols_above_threshold = np.sum(z_scores > z_threshold, axis=1)\n    \n    # Find the number of cells with z-scores greater than the threshold\n    num_cells_above_threshold = np.sum(z_scores > z_threshold)\n    \n    # Find the mean z-score of all cells\n    mean_z_score = np.mean(z_scores)\n    \n    # Find the standard deviation of the z-scores\n    std_z_score = np.std(z_scores)\n    \n    # Find the median z-score of all cells\n    median_z_score = np.median(z_scores)\n    \n    # Find the mode z-score of all cells\n    mode_z_score = np.mode(z_scores)\n    \n    # Find the minimum z-score of all cells\n    min_z_score = np.min(z_scores)\n    \n    # Find the maximum z-score of all cells\n    max_z_score = np.max(z_scores)\n    \n    # Find the range of z-scores\n    z_score_range = max_z_score - min_z_score\n    \n    # Find the variance of the z-scores\n    var_z_score = np.var(z_scores)\n    \n    # Find the skewness of the z-scores\n    skew_z_score = scipy.stats.skew(z_scores)\n    \n    # Find the kurtosis of the z-scores\n    kurt_z_score = scipy.stats.kurtosis(z_scores)\n    \n    # Find the correlation matrix of the z-scores\n    corr_z_score"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the 'value' column\n    mean_value = df['value'].mean()\n    \n    # Calculate the standard deviation of the 'value' column\n    std_dev_value = df['value'].std()\n    \n    # Calculate the correlation between 'value' and 'time' columns\n    corr_value_time = df['value'].corr(df['time'])\n    \n    # Plot the 'value' column as a line plot\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(data=df, x='time', y='value')\n    plt.title('Value over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.show()\n    \n    return mean_value, std_dev_value, corr_value_time\n"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    \n    \"\"\"\n    # TODO: Implement the task function here\n    return [], None\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    word = word.lower()\n    word = ''.join([char for char in word if char in string.ascii_lowercase])\n    word = ''.join(sorted(word))\n    return Counter(word)\n\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    \n    \"\"\"\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with random sales data\n    sales_data = pd.DataFrame({\n        'Date': date_range,\n        'Category': np.random.choice(categories, size=periods),\n        'Sales': np.random.randint(100, 1000, size=periods)\n    })\n    \n    # Group by date and category, and sum the sales\n    sales_grouped = sales_data.groupby(['Date', 'Category']).sum().reset_index()\n    \n    # Pivot the DataFrame to get sales by category over time\n    sales_pivot = sales_grouped.pivot(index='Date', columns='Category', values='Sales')\n    \n    # Plot the sales data\n    plt.figure(figsize=(10, 6))\n    for category in categories:\n        plt.plot(sales_pivot.index, sales_pivot[category], label=category)\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.title('Sales by Category Over Time')\n    plt.legend()\n    plt.show()\n    \n    return sales_pivot\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    \n    \"\"\"\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with the date range\n    df = pd.DataFrame(date_range, columns=['date'])\n    \n    # Generate random sales data\n    if sales_data is None:\n        sales_data = np.random.randint(100, 1000, size=(periods,))\n    \n    # Add sales data to the DataFrame\n    df['sales'] = sales_data\n    \n    # Fit a linear regression model\n    model = LinearRegression()\n    model.fit(df[['date']], df['sales'])\n    \n    # Predict sales for the next period\n    next_period = pd.to_datetime('2023-01-13', format='%Y-%m-%d')\n    next_period_sales = model.predict([[next_period]])\n    \n    return next_period_sales[0]\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a list of tasks with random durations and employees\n    tasks = []\n    for i in range(n_tasks):\n        duration = random.randint(1, 10)\n        employee = random.choice(employees)\n        tasks.append({\"Task\": f\"Task {i+1}\", \"Duration\": duration, \"Employee\": employee})\n\n    # Create a DataFrame from the list of tasks\n    df = pd.DataFrame(tasks)\n\n    # Sort the DataFrame by duration\n    df = df.sort_values(by=\"Duration\")\n\n    # Create a list of task lists\n    task_lists = []\n    current_list = []\n    current_duration = 0\n    for _, row in df.iterrows():\n        if current_duration + row[\"Duration\"] <= 8:\n            current_list.append(row)\n            current_duration += row[\"Duration\"]\n        else:\n            task_lists.append(current_list)\n            current_list = [row]\n            current_duration = row[\"Duration\"]\n    task_lists.append(current_list)\n\n    # Create a list of task lists with random start times\n    for i, task_list in enumerate(task_lists):\n        start_time = random.randint(9, 17)\n        for j, task in enumerate(task_list):\n            task[\"Start Time\"] = start_time\n            task[\"End Time\"] = start_time + task[\"Duration\"]\n            start_time = task[\"End Time\"]\n\n    # Create a list of task lists with random start times\n    for i, task_list in enumerate(task_lists):\n        start_time = random.randint(9, 17)\n        for j, task in enumerate(task_list):\n            task[\"Start Time\"] = start_time\n            task[\"End Time\"] = start_time + task[\"Duration\"]\n            start_time = task[\"End Time\"]\n\n    # Create a list of task lists with random start times\n    for i, task_list in enumerate(task_lists):\n        start_time = random.randint(9, 17)\n        for j, task in enumerate(task_list):\n            task[\"Start Time\"] = start_time\n            task[\"End Time\"] = start_time + task[\"Duration\"]\n            start_time = task[\"End Time\"]\n\n    # Create a list of task"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n\ndef task_func(mystrings, text):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list to store the counts of each string\n    counts = []\n    \n    # Loop through each string in the list\n    for string in mystrings:\n        # Count the number of times the string appears in the text\n        count = text.count(string)\n        # Append the count to the list\n        counts.append(count)\n    \n    # Return the list of counts\n    return counts\n\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    \n    \"\"\"\n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n\n    # Get all files in the source directory\n    files = glob.glob(os.path.join(source_directory, '**', '*'), recursive=True)\n\n    # Filter out directories\n    files = [f for f in files if os.path.isfile(f)]\n\n    # Create a zip file\n    with zipfile.ZipFile(os.path.join(target_directory, zip_name), 'w') as zipf:\n        for file in files:\n            # Get the relative path of the file\n            relative_path = os.path.relpath(file, source_directory)\n            # Add the file to the zip file with the relative path\n            zipf.write(file, relative_path)\n\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Create the target directory if it doesn't exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n\n    # Get a list of all files in the source directory\n    files = os.listdir(source_directory)\n\n    # Loop through each file\n    for file in files:\n        # Check if the file is a .docx file\n        if file.endswith('.docx'):\n            # Open the .docx file\n            doc = docx.Document(os.path.join(source_directory, file))\n\n            # Create a new .docx file with the same name in the target directory\n            new_doc = docx.Document()\n            new_doc.add_heading('New Document', 0)\n\n            # Loop through each paragraph in the original .docx file\n            for para in doc.paragraphs:\n                # Add the paragraph to the new .docx file\n                new_doc.add_paragraph(para.text)\n\n            # Save the new .docx file\n            new_doc.save(os.path.join(target_directory, file))\n\n    return 0\n\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    \n    \"\"\"\n    # Convert the 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Sort the dataframe by 'date' in ascending order\n    df = df.sort_values('date')\n    \n    # Create a new column 'month' to store the month part of the 'date' column\n    df['month'] = df['date'].dt.month\n    \n    # Create a new column 'year' to store the year part of the 'date' column\n    df['year'] = df['date'].dt.year\n    \n    # Create a new column 'day_of_week' to store the day of the week part of the 'date' column\n    df['day_of_week'] = df['date'].dt.dayofweek\n    \n    # Create a new column 'day_of_month' to store the day of the month part of the 'date' column\n    df['day_of_month'] = df['date'].dt.day\n    \n    # Create a new column 'day_of_year' to store the day of the year part of the 'date' column\n    df['day_of_year'] = df['date'].dt.dayofyear\n    \n    # Create a new column 'week_of_year' to store the week of the year part of the 'date' column\n    df['week_of_year'] = df['date'].dt.weekofyear\n    \n    # Create a new column 'quarter' to store the quarter part of the 'date' column\n    df['quarter'] = df['date'].dt.quarter\n    \n    # Create a new column 'is_weekend' to store whether the date is a weekend or not\n    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n    \n    # Create a new column 'is_holiday' to store whether the date is a holiday or not\n    # df['is_holiday'] = df['date'].apply(lambda x: 1 if x in holidays else 0)\n    \n    # Create a new column 'is_leap_year' to store whether the year is a leap year or not\n    df['is_leap_year'] = df['year'].apply(lambda x: 1 if x % 4 == 0 and ("}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    \"\"\"\n    \n    \"\"\"\n    # Get the current date and time\n    now = datetime.now(timezone.utc)\n    # Format the date and time as a string\n    formatted_now = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    # Create a new directory with the formatted date and time as its name\n    new_directory_path = os.path.join(directory_path, formatted_now)\n    # Create the new directory\n    os.makedirs(new_directory_path)\n    # Return the path to the new directory\n    return new_directory_path\n\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data from the JSON file\n    with open(json_data, 'r') as f:\n        data = json.load(f)\n    \n    # Create a list to store the data\n    data_list = []\n    \n    # Iterate over the data\n    for country, population in data.items():\n        # Calculate the population density\n        area = data[country][\"area\"]\n        density = population / area\n        \n        # Append the data to the list\n        data_list.append([country, population, area, density])\n    \n    # Create a DataFrame from the list\n    df = pd.DataFrame(data_list, columns=[\"Country\", \"Population\", \"Area\", \"Density\"])\n    \n    # Sort the DataFrame by population\n    df = df.sort_values(by=\"Population\", ascending=False)\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(os.path.join(output_dir, file_name), index=False)\n    \n    # Print the top 10 countries by population\n    print(df.head(10))\n\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    # Get all files in the directory\n    files = list(Path(dir_path).rglob(\"*\"))\n    \n    # Filter files based on predicates\n    filtered_files = [file for file in files if all(predicate(file) for predicate in predicates)]\n    \n    # Get the file names\n    file_names = [str(file.relative_to(dir_path)) for file in filtered_files]\n    \n    # Get the file sizes\n    file_sizes = [file.stat().st_size for file in filtered_files]\n    \n    # Get the file modification times\n    file_mod_times = [file.stat().st_mtime for file in filtered_files]\n    \n    # Get the file creation times\n    file_create_times = [file.stat().st_ctime for file in filtered_files]\n    \n    # Get the file access times\n    file_access_times = [file.stat().st_atime for file in filtered_files]\n    \n    # Get the file permissions\n    file_permissions = [file.stat().st_mode for file in filtered_files]\n    \n    # Get the file owners\n    file_owners = [file.stat().st_uid for file in filtered_files]\n    \n    # Get the file groups\n    file_groups = [file.stat().st_gid for file in filtered_files]\n    \n    # Get the file types\n    file_types = [file.suffix for file in filtered_files]\n    \n    # Get the file paths\n    file_paths = [str(file) for file in filtered_files]\n    \n    # Get the file hashes\n    file_hashes = [hash(file.read_bytes()) for file in filtered_files]\n    \n    # Get the file contents\n    file_contents = [file.read_text() for file in filtered_files]\n    \n    # Get the file line counts\n    file_line_counts = [len(file_contents[i].splitlines()) for i in range(len(file_contents))]\n    \n    # Get the file word counts\n    file_word_counts = [len(file_contents[i].split()) for i in range(len(file_contents))]\n    \n    # Get the file character counts\n    file_character_counts = [len(file_contents[i]) for i in range(len(file_contents))]\n    \n    # Get the file line lengths\n    file_line_lengths = [max(len(line) for line in file_contents[i].splitlines()) for i in range(len(file_contents))]\n    \n    # Get the file word lengths\n   "}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\n\ndef task_func(hex_string):\n    \"\"\"\n    \n    \"\"\"\n    # Convert hex string to bytes\n    bytes_data = binascii.unhexlify(hex_string)\n    \n    # Decode bytes to string\n    decoded_string = bytes_data.decode('utf-8')\n    \n    # Encode string to base64\n    base64_encoded = base64.b64encode(decoded_string.encode('utf-8'))\n    \n    # Decode base64 to string\n    base64_decoded = base64_encoded.decode('utf-8')\n    \n    # Encode string to URL\n    url_encoded = urllib.parse.quote(base64_decoded)\n    \n    # Decode URL to string\n    url_decoded = urllib.parse.unquote(url_encoded)\n    \n    # Encode string to hex\n    hex_encoded = codecs.encode(url_decoded.encode('utf-8'), 'hex').decode('utf-8')\n    \n    return hex_encoded\n\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    \n    # Compute the MD5 checksum\n    md5_checksum = hashlib.md5()\n    with open(TARGET_TAR_FILE, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            md5_checksum.update(chunk)\n    md5_checksum = md5_checksum.hexdigest()\n    \n    # Check the MD5 checksum\n    if md5_checksum != EXPECTED_MD5_CHECKSUM:\n        raise Exception(\"MD5 checksum does not match\")\n    \n    # Extract the file\n    with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n        tar.extractall()\n    \n    # Remove the downloaded file\n    os.remove(TARGET_TAR_FILE)\n\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\n\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, 'temp.csv')\n    \n    # Read the file\n    with open('temp.csv', 'r') as f:\n        reader = csv.DictReader(f)\n        data = [row[column_name] for row in reader]\n    \n    # Write the data to the CSV file\n    with open(csv_file_path, 'a') as f:\n        writer = csv.writer(f)\n        writer.writerow(data)\n    \n    # Remove the temporary file\n    os.remove('temp.csv')\n\n"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bf7\u6c42\u7f51\u9875\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf-8')\n    # \u89e3\u6790\u7f51\u9875\n    html = etree.HTML(html)\n    # \u83b7\u53d6\u6570\u636e\n    data = html.xpath('//div[@class=\"item\"]/div[@class=\"info\"]/div[@class=\"title\"]/a/text()')\n    # \u8fd4\u56de\u6570\u636e\n    return data\n\n"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Get the HTML content of the page\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf-8')\n\n    # Find all the links on the page\n    links = re.findall(r'href=\"(.*?)\"', html)\n\n    # Count the number of times each domain appears\n    domain_counts = Counter()\n    for link in links:\n        domain = link.split('/')[2]\n        domain_counts[domain] += 1\n\n    # Find the domain with the most links\n    most_common_domain = domain_counts.most_common(1)[0][0]\n\n    # Find the number of links to the most common domain\n    num_links = domain_counts[most_common_domain]\n\n    return most_common_domain, num_links\n\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    with open(download_path + os.path.basename(url), 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n\n    # Unzip the file\n    try:\n        with ZipFile(download_path + os.path.basename(url), 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n    except BadZipFile:\n        print(f\"Error: {download_path + os.path.basename(url)} is not a valid zip file.\")\n        return\n\n    # Remove the zip file\n    os.remove(download_path + os.path.basename(url))\n\n    # Return the path to the extracted files\n    return download_path + os.path.basename(url).replace('.zip', '')\n\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\n\ndef task_func(url, table_id):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the table with the given ID\n    table = soup.find('table', {'id': table_id})\n    \n    # Extract the table headers\n    headers = [header.text.strip() for header in table.find_all('th')]\n    \n    # Extract the table rows\n    rows = []\n    for row in table.find_all('tr'):\n        cells = [cell.text.strip() for cell in row.find_all('td')]\n        rows.append(cells)\n    \n    # Create a DataFrame from the extracted data\n    df = pd.DataFrame(rows, columns=headers)\n    \n    # Return the DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\n\ndef task_func(url, filename):\n    \"\"\"\n    \n    \"\"\"\n    # Create the download directory if it doesn't exist\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    \n    # Create the zip directory if it doesn't exist\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n    \n    # Download the file\n    response = requests.get(url)\n    \n    # Save the file to the download directory\n    filepath = DOWNLOAD_DIR / filename\n    filepath.write_bytes(response.content)\n    \n    # Unzip the file\n    with zipfile.ZipFile(filepath, 'r') as zip_ref:\n        zip_ref.extractall(ZIP_DIR)\n    \n    # Remove the zip file\n    filepath.unlink()\n\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Check if the URL is valid\n    if not url.startswith(\"http\"):\n        url = urljoin(base_url, url)\n\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}\")\n        return 1\n\n    # Parse the HTML content\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    # Find all the links on the page\n    links = soup.find_all(\"a\")\n\n    # Extract the text and href attributes from each link\n    data = []\n    for link in links:\n        text = link.get_text()\n        href = link.get(\"href\")\n        data.append([text, href])\n\n    # Write the data to a CSV file\n    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Text\", \"URL\"])\n        writer.writerows(data)\n\n    print(f\"Data successfully written to {csv_file}\")\n    return 0\n\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Create a connection to the database\n    conn = sqlite3.connect(database_name)\n    c = conn.cursor()\n\n    # Create a table if it doesn't exist\n    c.execute('''CREATE TABLE IF NOT EXISTS my_table\n                 (id INTEGER PRIMARY KEY, name TEXT, price REAL)''')\n\n    # Send a GET request to the webpage\n    response = requests.get(webpage_url)\n\n    # Parse the HTML content\n    tree = html.fromstring(response.content)\n\n    # Extract the product names and prices\n    product_names = tree.xpath('//div[@class=\"product-name\"]/a/text()')\n    product_prices = tree.xpath('//span[@class=\"price\"]/text()')\n\n    # Insert the data into the database\n    for name, price in zip(product_names, product_prices):\n        c.execute(\"INSERT INTO my_table (name, price) VALUES (?, ?)\", (name, price))\n\n    # Commit the changes and close the connection\n    conn.commit()\n    conn.close()\n\n    return len(product_names)\n\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\n\nIMAGE_PATH = \"image.png\"\n\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    # Load image\n    img = Image.open(filename)\n    \n    # Convert to grayscale\n    img = img.convert(\"L\")\n    \n    # Convert to binary\n    img = img.point(lambda p: p > 128 and 255)\n    \n    # Save binary image\n    img.save(\"binary.png\")\n    \n    # Extract text\n    text = pytesseract.image_to_string(img, lang=\"rus\", config=\"--psm 6\")\n    \n    # Decode text from cp1251 to utf8\n    text = text.encode(from_encoding).decode(to_encoding)\n    \n    # Save text\n    with codecs.open(\"text.txt\", \"w\", encoding=to_encoding) as f:\n        f.write(text)\n    \n    return text\n\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the API\n    response = requests.get(url)\n    \n    # Detect the encoding of the response content\n    encoding = chardet.detect(response.content)[\"encoding\"]\n    \n    # Decode the response content using the detected encoding\n    decoded_content = response.content.decode(encoding)\n    \n    # Convert the decoded content to a JSON object\n    json_obj = json.loads(decoded_content)\n    \n    # Encode the JSON object to the target encoding\n    encoded_content = json.dumps(json_obj, ensure_ascii=False).encode(to_encoding)\n    \n    # Return the encoded content\n    return encoded_content\n\n"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        print(f\"File {csv_file_path} is empty.\")\n        return\n    except FileNotFoundError:\n        print(f\"File {csv_file_path} not found.\")\n        return\n    except pd.errors.ParserError:\n        print(f\"Error parsing file {csv_file_path}.\")\n        return\n\n    if column_name not in df.columns:\n        print(f\"Column {column_name} not found in file {csv_file_path}.\")\n        return\n\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='coerce')\n\n    # Filter out rows with invalid dates\n    df = df.dropna(subset=[column_name])\n\n    # Sort by date\n    df = df.sort_values(by=column_name)\n\n    # Get the first and last date\n    first_date = df[column_name].iloc[0]\n    last_date = df[column_name].iloc[-1]\n\n    # Get the number of days between the first and last date\n    num_days = (last_date - first_date).days\n\n    # Get the number of unique dates\n    num_unique_dates = df[column_name].nunique()\n\n    # Get the number of missing dates\n    missing_dates = set(pd.date_range(start=first_date, end=last_date)).difference(df[column_name].unique())\n\n    # Get the number of missing dates\n    num_missing_dates = len(missing_dates)\n\n    # Get the percentage of missing dates\n    missing_dates_percentage = (num_missing_dates / num_days) * 100\n\n    # Print the results\n    print(f\"File: {csv_file_path}\")\n    print(f\"Column: {column_name}\")\n    print(f\"First date: {first_date}\")\n    print(f\"Last date: {last_date}\")\n    print(f\"Number of days: {num_days}\")\n    print(f\"Number of unique dates: {num_unique_dates}\")\n    print(f\"Number of missing dates: {num_missing_dates}\")\n    print(f\"Percentage of missing dates: {missing_dates_percentage:.2f}%\")\n    print(\"Missing dates:\")\n    for date in missing_dates:\n        print(date)\n    print(\"\\n\")\n\n    # Save the results to a file"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    \n    \"\"\"\n    # Create a SSL context\n    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n\n    # Wrap the socket with the SSL context\n    ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n    # Receive the request\n    request = ssl_socket.recv(buffer_size).decode()\n\n    # Parse the request\n    request_lines = request.split('\\r\\n')\n    request_line = request_lines[0]\n    request_method, request_path, request_protocol = request_line.split(' ')\n\n    # Send the response\n    response = 'HTTP/1.1 200 OK\\r\\nContent-Type: text/html\\r\\n\\r\\nHello, World!'\n    ssl_socket.sendall(response.encode())\n\n    # Close the SSL socket\n    ssl_socket.close()\n\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    \n    \"\"\"\n    # Create a TCP/IP socket\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    # Connect the socket to the server\n    sock.connect((server_address, server_port))\n\n    # Create a queue to store received data\n    received_data_queue = queue.Queue()\n\n    # Set the timeout for the select function\n    timeout = timedelta(seconds=run_duration)\n\n    # Get the current time\n    start_time = datetime.now()\n\n    # Loop until the timeout is reached\n    while datetime.now() - start_time < timeout:\n        # Use select to wait for data to be received\n        readable, _, _ = select.select([sock], [], [], 0)\n\n        # If data is available, read it and put it in the queue\n        if sock in readable:\n            data = sock.recv(buffer_size)\n            received_data_queue.put(data)\n\n    # Close the socket\n    sock.close()\n\n    # Return the received data\n    return received_data_queue\n\n"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\n\ndef task_func(client_socket):\n    \"\"\"\n    \n    \"\"\"\n    # Receive the email address and message from the client\n    email_address = client_socket.recv(BUFFER_SIZE).decode()\n    message = client_socket.recv(BUFFER_SIZE).decode()\n\n    # Send the email\n    send_email(email_address, message)\n\n    # Send a response to the client\n    client_socket.send(\"Email sent successfully\".encode())\n\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    df = pd.read_csv(file_path)\n    \n    # Preprocess the data\n    df['text'] = df['text'].str.lower()\n    df['text'] = df['text'].str.replace(r'\\W', ' ')\n    df['text'] = df['text'].str.replace(r'\\d+', ' ')\n    df['text'] = df['text'].str.split()\n    df['text'] = df['text'].apply(lambda x: [item for item in x if item not in STOP_WORDS])\n    df['text'] = df['text'].apply(lambda x: ' '.join(x))\n    \n    # Vectorize the data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(df['text'])\n    \n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Get the word counts\n    word_counts = X.sum(axis=0).tolist()[0]\n    \n    # Create a dataframe\n    df_word_counts = pd.DataFrame({'word': feature_names, 'count': word_counts})\n    \n    # Sort the dataframe by count\n    df_word_counts = df_word_counts.sort_values(by='count', ascending=False)\n    \n    # Plot the word counts\n    plt.figure(figsize=(10, 6))\n    plt.bar(df_word_counts['word'], df_word_counts['count'])\n    plt.xticks(rotation=90)\n    plt.xlabel('Word')\n    plt.ylabel('Count')\n    plt.title('Word Count')\n    plt.tight_layout()\n    \n    # Save the plot\n    if save_path:\n        plt.savefig(save_path)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the dataframe\n    return df_word_counts\n\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\n\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    \n    \"\"\"\n    if animals is None:\n        animals = ['cat', 'dog', 'bird']\n    if foods is None:\n        foods = ['fish', 'meat', 'seed']\n    \n    # Create a list of all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Create a DataFrame to store the results\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    \n    # Calculate the number of animals and foods\n    num_animals = len(animals)\n    num_foods = len(foods)\n    \n    # Calculate the number of combinations\n    num_combinations = len(combinations)\n    \n    # Calculate the probability of each combination\n    probabilities = [1 / num_combinations] * num_combinations\n    \n    # Add the probabilities to the DataFrame\n    df['Probability'] = probabilities\n    \n    return df\n\n"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    \n    \"\"\"\n    # Convert time strings to datetime objects\n    time_objects = [datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S') for time_string in time_strings]\n    \n    # Convert datetime objects to UTC\n    utc_time_objects = [time_object.astimezone(pytz.utc) for time_object in time_objects]\n    \n    # Convert UTC datetime objects to the specified timezone\n    local_time_objects = [time_object.astimezone(pytz.timezone(timezone)) for time_object in utc_time_objects]\n    \n    # Convert datetime objects to strings in the format 'YYYY-MM-DD HH:MM:SS'\n    local_time_strings = [time_object.strftime('%Y-%m-%d %H:%M:%S') for time_object in local_time_objects]\n    \n    return local_time_strings\n\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text.lower())\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Find the most common words\n    most_common_words = word_counts.most_common(10)\n    \n    # Print the most common words and their frequencies\n    for word, count in most_common_words:\n        print(f'{word}: {count}')\n    \n    # Plot the word frequencies\n    words, counts = zip(*most_common_words)\n    plt.bar(words, counts)\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Most Common Words')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n\ndef task_func(myString):\n    \"\"\"\n    \n    \"\"\"\n    # Parse the URL\n    parsed_url = urlparse(myString)\n    domain = parsed_url.netloc\n    path = parsed_url.path\n\n    # Make a request to the website\n    response = requests.get(myString)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find all the links on the page\n        links = soup.find_all('a')\n\n        # Extract the domain and path from each link\n        extracted_links = []\n        for link in links:\n            link_url = link.get('href')\n            if link_url:\n                parsed_link = urlparse(link_url)\n                link_domain = parsed_link.netloc\n                link_path = parsed_link.path\n                extracted_links.append((link_domain, link_path))\n\n        # Filter out the links that match the domain and path\n        filtered_links = []\n        for link in extracted_links:\n            if link[0] == domain and link[1] == path:\n                filtered_links.append(link)\n\n        # Return the filtered links\n        return filtered_links\n    else:\n        return \"Error: Unable to retrieve the webpage.\"\n\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    \n    \"\"\"\n    # \u89e3\u6790 JSON \u6570\u636e\n    data = json.loads(json_data)\n    \n    # \u83b7\u53d6\u5f53\u524d\u65f6\u95f4\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # \u521b\u5efa\u4fdd\u5b58\u76ee\u5f55\n    if save_dir is None:\n        save_dir = os.path.join(os.getcwd(), \"data\")\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # \u4fdd\u5b58 JSON \u6570\u636e\u5230\u6587\u4ef6\n    file_path = os.path.join(save_dir, f\"{current_time}.json\")\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=4)\n    \n    # \u68c0\u67e5\u5e76\u5220\u9664\u672a\u77e5\u952e\n    if unknown_key in data:\n        del data[unknown_key]\n    \n    # \u53d1\u9001 POST \u8bf7\u6c42\n    url = \"http://127.0.0.1:5000/api/v1/task\"\n    headers = {\"Content-Type\": \"application/json\"}\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    \n    # \u68c0\u67e5\u54cd\u5e94\u72b6\u6001\u7801\n    if response.status_code == 200:\n        print(\"\u4efb\u52a1\u6267\u884c\u6210\u529f\")\n    else:\n        print(\"\u4efb\u52a1\u6267\u884c\u5931\u8d25\")\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}\")\n        return []\n    \n    # Parse the HTML content of the page\n    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n    \n    # Find all the links on the page\n    links = soup.find_all(\"a\")\n    \n    # Extract the href attribute from each link\n    hrefs = [link.get(\"href\") for link in links]\n    \n    # Remove any empty hrefs\n    hrefs = [href for href in hrefs if href]\n    \n    # Remove any hrefs that start with a hash\n    hrefs = [href for href in hrefs if not href.startswith(\"#\")]\n    \n    # Remove any hrefs that start with a slash\n    hrefs = [href for href in hrefs if not href.startswith(\"/\")]\n    \n    # Remove any hrefs that start with a question mark\n    hrefs = [href for href in hrefs if not href.startswith(\"?\")]\n    \n    # Remove any hrefs that start with a dot\n    hrefs = [href for href in hrefs if not href.startswith(\".\")]\n    \n    # Remove any hrefs that start with a backslash\n    hrefs = [href for href in hrefs if not href.startswith(\"\\\\\")]\n    \n    # Remove any hrefs that start with a tilde\n    hrefs = [href for href in hrefs if not href.startswith(\"~\")]\n    \n    # Remove any hrefs that start with a plus\n    hrefs = [href for href in hrefs if not href.startswith(\"+\")]\n    \n    # Remove any hrefs that start with a hyphen\n    hrefs = [href for href in hrefs if not href.startswith(\"-\")]\n    \n    # Remove any hrefs that start with a space\n    hrefs = [href for href in hrefs if not href.startswith(\" \")]\n    \n    # Remove any hrefs that start with a tab\n    hrefs = [href for href in hrefs if not href.startswith(\"\\t\")]\n    \n    # Remove any hrefs that start with a newline\n    hrefs = [href for href in hrefs if not"}
