{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    \n    \"\"\"\n    # Connect to the FTP server\n    ftp = ftplib.FTP(ftp_server)\n    ftp.login(ftp_user, ftp_password)\n    ftp.cwd(ftp_dir)\n\n    # Get the list of files in the directory\n    files = ftp.nlst()\n\n    # Download the files\n    for file in files:\n        # Get the file size\n        file_size = ftp.size(file)\n\n        # Download the file\n        with open(file, 'wb') as f:\n            ftp.retrbinary('RETR ' + file, f.write, blocksize=1024)\n\n        # Print the file size\n        print(f\"Downloaded {file} with size {file_size} bytes\")\n\n    # Close the FTP connection\n    ftp.quit()\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    \n    \"\"\"\n    # Read the commands file\n    with open(commands_file_path, 'r') as f:\n        commands = f.readlines()\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    # Iterate over the commands\n    for i, command in enumerate(commands):\n        # Run the command and capture the output\n        output = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n        # If the command failed, print an error message\n        if output.returncode != 0:\n            print(f\"Error running command {i+1}: {command}\")\n            print(output.stderr)\n            continue\n\n        # Write the output to a file\n        output_file_path = os.path.join(output_dir_path, f\"output_{i+1}.txt\")\n        with open(output_file_path, 'w') as f:\n            f.write(output.stdout)\n\n        # Print a success message\n        print(f\"Command {i+1} successful: {command}\")\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Get the process ID of the process with the given name\n    try:\n        process_id = psutil.Process(process_name)\n    except psutil.NoSuchProcess:\n        return f\"Process {process_name} not found\"\n\n    # Get the process details\n    process_details = process_id.as_dict(attrs=['pid', 'name', 'status', 'create_time'])\n\n    # Get the process status\n    process_status = process_details['status']\n\n    # Get the process create time\n    process_create_time = process_details['create_time']\n\n    # Get the process memory info\n    process_memory_info = process_id.memory_info()\n\n    # Get the process memory usage\n    process_memory_usage = process_memory_info.rss / (1024 * 1024)\n\n    # Get the process CPU usage\n    process_cpu_usage = process_id.cpu_percent(interval=1)\n\n    # Get the process open files\n    process_open_files = process_id.open_files()\n\n    # Get the process connections\n    process_connections = process_id.connections()\n\n    # Get the process threads\n    process_threads = process_id.threads()\n\n    # Get the process memory info\n    process_memory_info = process_id.memory_info()\n\n    # Get the process memory usage\n    process_memory_usage = process_memory_info.rss / (1024 * 1024)\n\n    # Get the process CPU usage\n    process_cpu_usage = process_id.cpu_percent(interval=1)\n\n    # Get the process open files\n    process_open_files = process_id.open_files()\n\n    # Get the process connections\n    process_connections = process_id.connections()\n\n    # Get the process threads\n    process_threads = process_id.threads()\n\n    # Get the process memory info\n    process_memory_info = process_id.memory_info()\n\n    # Get the process memory usage\n    process_memory_usage = process_memory_info.rss / (1024 * 1024)\n\n    # Get the process CPU usage\n    process_cpu_usage = process_id.cpu_percent(interval=1)\n\n    # Get the process open files\n    process_open_files = process_id.open_files()\n\n    # Get the process connections\n    process_connections = process_id.connections()\n\n    # Get the process threads\n    process_threads = process_id.threads()\n\n    # Get the process memory info\n    process_memory_info = process_id.memory_info()\n\n    # Get the process memory usage\n    process_memory_usage = process"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of all zip files in the directory\n    zip_files = glob.glob(os.path.join(directory, \"*.zip\"))\n    \n    # Iterate over each zip file\n    for zip_file in zip_files:\n        # Extract the zip file to a temporary directory\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall(os.path.join(directory, \"temp\"))\n        \n        # Do something with the extracted files\n        # For example, let's just print the names of the files\n        for file in os.listdir(os.path.join(directory, \"temp\")):\n            print(file)\n        \n        # Remove the temporary directory\n        shutil.rmtree(os.path.join(directory, \"temp\"))\n"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Convert to lower case\n    text = text.lower()\n    \n    # Tokenize the text\n    tokens = word_tokenize(text)\n    \n    # Remove stop words\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    # Remove short words\n    tokens = [word for word in tokens if len(word) > 2]\n    \n    # Create a word cloud\n    wordcloud = WordCloud(width=800, height=400, max_words=100).generate(' '.join(tokens))\n    \n    # Display the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, target_column):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the target column to categorical\n    df[target_column] = pd.Categorical(df[target_column])\n    \n    # One-hot encoding\n    df = pd.get_dummies(df, columns=[target_column])\n    \n    # Drop the original target column\n    df = df.drop(target_column, axis=1)\n    \n    # Split the data into features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    # Train a random forest classifier\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X, y)\n    \n    # Get the feature importances\n    feature_importances = rf.feature_importances_\n    \n    # Plot the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=X.columns, y=feature_importances)\n    plt.title('Feature Importances')\n    plt.xlabel('Feature')\n    plt.ylabel('Importance')\n    plt.show()\n    \n    return feature_importances\n\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_FOLDER'] = template_folder\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n    login_manager.login_message = 'Please log in to access this page'\n    login_manager.login_message_category = 'info'\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return User.query.get(int(user_id))\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    class UserForm(FlaskForm):\n        username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n        password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n        submit = SubmitField('Create Account')\n\n    class UserDB:\n        def __init__(self):\n            self.users = {}\n\n        def add_user(self, username, password):\n            self.users[username] = User(len(self.users), username, password)\n\n        def get_user(self, username):\n            return self.users.get(username)\n\n    user_db = UserDB()\n\n    @app.route('/')\n    def index():\n        return render_template('index.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = user_db.get_user(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the Z-score for each value in the column\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers based on the Z-score\n    outliers = data[z_scores > outlier_z_score]\n    \n    # Remove outliers from the data\n    data = data[z_scores <= outlier_z_score]\n    \n    return data\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create a KMeans model with the specified number of clusters\n    kmeans = KMeans(n_clusters=n_clusters)\n    \n    # Fit the model to the data\n    kmeans.fit(df)\n    \n    # Get the cluster labels for each data point\n    labels = kmeans.labels_\n    \n    # Create a scatter plot of the data with different colors for each cluster\n    plt.scatter(df[0], df[1], c=labels)\n    \n    # Create a legend for the plot\n    plt.legend(handles=plt.gca().lines, labels=[f'Cluster {i}' for i in range(1, n_clusters+1)])\n    \n    # Show the plot\n    plt.show()\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    \n    \"\"\"\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    return pca.transform(data)\n"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Load the iris dataset\n    iris = load_iris()\n    \n    # Create a DataFrame\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    \n    # Add the target variable\n    df['target'] = iris.target\n    \n    # Plot a histogram for each feature\n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='sepal length (cm)', hue='target', multiple='dodge')\n    plt.title('Histogram of Sepal Length')\n    plt.show()\n\n    # Plot a scatter plot for each feature\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', hue='target')\n    plt.title('Scatter Plot of Sepal Length vs Sepal Width')\n    plt.show()\n\n    # Plot a bar chart for each feature\n    plt.figure(figsize=(10, 6))\n    sns.barplot(data=df, x='target', y='sepal length (cm)')\n    plt.title('Bar Chart of Sepal Length by Target')\n    plt.show()\n\n    # Plot a box plot for each feature\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df, x='target', y='sepal length (cm)')\n    plt.title('Box Plot of Sepal Length by Target')\n    plt.show()\n\n    # Plot a violin plot for each feature\n    plt.figure(figsize=(10, 6))\n    sns.violinplot(data=df, x='target', y='sepal length (cm)')\n    plt.title('Violin Plot of Sepal Length by Target')\n    plt.show()\n\n    # Plot a heatmap of the correlation matrix\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(data=df.corr(), annot=True, cmap='coolwarm', square=True)\n    plt.title('Heatmap of Correlation Matrix')\n    plt.show()\n\ntask_func()"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    # Generate some random data\n    data = np.random.rand(100, 4)\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D'])\n    # Add a timestamp column\n    df['timestamp'] = pd.to_datetime(datetime.now())\n    # Save the DataFrame to a CSV file\n    df.to_csv('data.csv', index=False)\n    # Plot a histogram of the data\n    plt.hist(df['A'], bins=10)\n    plt.title('Histogram of A')\n    plt.show()\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load the Boston housing dataset\n    df = pd.read_csv(data_url)\n    \n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Create a new column 'target' which is a random value between 0 and 1\n    df['target'] = np.random.rand(len(df))\n    \n    # Create a new column 'feature' which is a random value between 0 and 1\n    df['feature'] = np.random.rand(len(df))\n    \n    # Create a scatter plot of the 'feature' vs 'target'\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(x='feature', y='target', data=df)\n    plt.title('Scatter plot of feature vs target')\n    plt.show()\n    \n    # Print the first 5 rows of the dataframe\n    print(df.head())\n    \n    # Print the summary statistics of the dataframe\n    print(df.describe())\n    \n    # Print the correlation matrix of the dataframe\n    print(df.corr())\n    \n    # Print the first 5 rows of the dataframe with the 'target' column sorted in descending order\n    print(df.sort_values(by='target', ascending=False).head())\n    \n    # Print the first 5 rows of the dataframe with the 'feature' column sorted in ascending order\n    print(df.sort_values(by='feature', ascending=True).head())\n    \n    # Print the mean of the 'target' column\n    print(df['target'].mean())\n    \n    # Print the median of the 'target' column\n    print(df['target'].median())\n    \n    # Print the standard deviation of the 'target' column\n    print(df['target'].std())\n    \n    # Print the variance of the 'target' column\n    print(df['target'].var())\n    \n    # Print the count of non-null values in the 'target' column\n    print(df['target'].count())\n    \n    # Print the count of unique values in the 'target' column\n    print(df['target'].nunique())\n    \n    # Print the count of missing values in the 'target' column\n    print(df['target'].isnull().sum())\n    \n    # Print the count of missing values in the 'feature' column\n    print(df"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Set the index to the date column\n    df.set_index('date', inplace=True)\n    \n    # Resample the data to the specified frequency\n    df_resampled = df.resample(freq).mean()\n    \n    # Decompose the time series into trend, seasonal, and residual components\n    decomposition = seasonal_decompose(df_resampled, model=decomposition_model)\n    \n    # Plot the original time series\n    plt.figure(figsize=(10, 6))\n    plt.subplot(411)\n    plt.plot(df_resampled, label='Original')\n    plt.legend(loc='best')\n    plt.subplot(412)\n    plt.plot(decomposition.trend, label='Trend')\n    plt.legend(loc='best')\n    plt.subplot(413)\n    plt.plot(decomposition.seasonal, label='Seasonality')\n    plt.legend(loc='best')\n    plt.subplot(414)\n    plt.plot(decomposition.resid, label='Residuals')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    # Generate a list of random dates between start_date and end_date\n    dates = [start_date + timedelta(days=randint(0, (end_date - start_date).days)) for _ in range(100)]\n    \n    # Create a DataFrame with the generated dates\n    df = pd.DataFrame({'date': dates})\n    \n    # Calculate the number of days between each date and the next date\n    df['days_to_next'] = df['date'].shift(-1) - df['date']\n    \n    # Calculate the number of days between each date and the previous date\n    df['days_to_prev'] = df['date'] - df['date'].shift(1)\n    \n    # Calculate the number of days between each date and the next date, excluding the last date\n    df['days_to_next_excl'] = df['date'].shift(-1) - df['date']\n    \n    # Calculate the number of days between each date and the previous date, excluding the first date\n    df['days_to_prev_excl'] = df['date'] - df['date'].shift(1)\n    \n    return df\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of all files in the directory\n    all_files = glob.glob(os.path.join(file_dir, \"*.{}\".format(file_ext)))\n    \n    # Initialize an empty list to store the dataframes\n    dfs = []\n    \n    # Iterate over each file\n    for file in all_files:\n        # Read the file into a dataframe\n        df = pd.read_csv(file)\n        \n        # Append the dataframe to the list\n        dfs.append(df)\n    \n    # Concatenate the dataframes\n    df_concat = pd.concat(dfs, ignore_index=True)\n    \n    # Return the concatenated dataframe\n    return df_concat\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    my_list[:] = [randint(0, 100) for _ in range(size)]\n    return my_list\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request\n    response = requests.get(url)\n    \n    # If the GET request is successful, the status code will be 200\n    if response.status_code == 200:\n        # Get the content of the response\n        page_content = response.content\n        \n        # Create a BeautifulSoup object and specify the parser\n        soup = BeautifulSoup(page_content, 'html.parser')\n        \n        # Find all the links on the page\n        links = soup.find_all('a')\n        \n        # Create a list to store the links\n        link_list = []\n        \n        # Loop through the links and extract the href attribute\n        for link in links:\n            link_list.append(link.get('href'))\n        \n        # Create a DataFrame from the list of links\n        df = pd.DataFrame(link_list, columns=['Links'])\n        \n        # Return the DataFrame\n        return df\n    \n    else:\n        # If the GET request is not successful, return an error message\n        return 'Failed to retrieve the webpage.'\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of each column\n    mean = df.mean()\n    \n    # Calculate the standard deviation of each column\n    std = df.std()\n    \n    # Calculate the variance of each column\n    var = df.var()\n    \n    # Calculate the median of each column\n    median = df.median()\n    \n    # Calculate the mode of each column\n    mode = df.mode()\n    \n    # Calculate the range of each column\n    range = df.max() - df.min()\n    \n    # Calculate the interquartile range (IQR) of each column\n    iqr = df.quantile(0.75) - df.quantile(0.25)\n    \n    # Calculate the coefficient of variation (CV) of each column\n    cv = df.std() / df.mean()\n    \n    # Calculate the skewness of each column\n    skew = df.skew()\n    \n    # Calculate the kurtosis of each column\n    kurt = df.kurt()\n    \n    # Create a new DataFrame with the calculated statistics\n    stats_df = pd.DataFrame({\n        'mean': mean,\n        'std': std,\n        'var': var,\n        'median': median,\n        'mode': mode,\n        'range': range,\n        'iqr': iqr,\n        'cv': cv,\n        'skew': skew,\n        'kurt': kurt\n    })\n    \n    return stats_df\n"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    \n    \"\"\"\n    for ip in ip_range:\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.settimeout(1)\n            result = sock.connect_ex((ip, port))\n            if result == 0:\n                print(f\"Port {port} is open on {ip}\")\n            sock.close()\n        except Exception as e:\n            print(f\"Error: {e}\")\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(log_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the log file\n    with open(log_file, 'r') as f:\n        log_data = f.readlines()\n\n    # Extract the timestamp from each line\n    timestamps = []\n    for line in log_data:\n        match = re.search(r'\\[(.*?)\\]', line)\n        if match:\n            timestamps.append(match.group(1))\n\n    # Convert the timestamps to datetime objects\n    datetime_objects = []\n    for timestamp in timestamps:\n        datetime_objects.append(datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S'))\n\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'timestamp': datetime_objects,\n        'log_line': log_data\n    })\n\n    # Sort the DataFrame by timestamp\n    df.sort_values(by='timestamp', inplace=True)\n\n    # Return the sorted DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    \n    \"\"\"\n    # Find all words in the text\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Create a dictionary to store the frequency of each word\n    word_freq = {}\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n    \n    # Sort the dictionary by frequency in descending order\n    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n    \n    # Create a list of unique words\n    unique_words = [word for word, freq in sorted_word_freq]\n    \n    # Create a list of frequencies\n    freqs = [freq for word, freq in sorted_word_freq]\n    \n    # Create a bar chart\n    plt.bar(unique_words, freqs, width=rwidth)\n    \n    # Set the title and labels\n    plt.title('Word Frequency')\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    \n    # Show the plot\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\n\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation\n    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n    \n    # Tokenize text\n    df['text'] = df['text'].apply(lambda x: nltk.word_tokenize(x))\n    \n    # Remove stopwords\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    df['text'] = df['text'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n    \n    # Remove short words\n    df['text'] = df['text'].apply(lambda x: [word for word in x if len(word) > 2])\n    \n    # Remove duplicates\n    df['text'] = df['text'].apply(lambda x: list(set(x)))\n    \n    return df\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    \n    \"\"\"\n    # Remove special characters and numbers\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n    \n    # Remove stopwords\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in STOPWORDS]))\n    \n    # Convert to lowercase\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: x.lower())\n    \n    # Vectorize the text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n    \n    return X\n\n"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    \n    \"\"\"\n    # Create a dictionary to store the results\n    results = {}\n    \n    # Loop over each city\n    for city in cities:\n        # Create a point geometry for the city\n        point = Point(0, 0)\n        \n        # Create a GeoDataFrame with the point geometry\n        gdf = gpd.GeoDataFrame(geometry=[point])\n        \n        # Check if the city is within the bounds\n        if (dic['Lon'][0] <= gdf.geometry.x.min() <= dic['Lon'][1] and\n            dic['Lat'][0] <= gdf.geometry.y.min() <= dic['Lat'][1]):\n            results[city] = True\n        else:\n            results[city] = False\n    \n    return results\n"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    \n    \"\"\"\n    set_seed(seed)\n    # Generate a random weather condition\n    weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n    \n    # Generate a random city\n    city = cities[randint(0, len(cities) - 1)]\n    \n    # Generate a random time zone\n    tz = timezones[city]\n    \n    # Generate a random UTC datetime\n    utc_dt = utc_datetime + pd.DateOffset(days=randint(0, 365))\n    \n    # Convert the UTC datetime to the city's time zone\n    city_dt = utc_dt.tz_convert(tz)\n    \n    # Generate a random temperature\n    temperature = randint(-20, 40)\n    \n    # Generate a random humidity\n    humidity = randint(0, 100)\n    \n    # Generate a random wind speed\n    wind_speed = randint(0, 100)\n    \n    # Generate a random weather condition\n    weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n    \n    # Create a dictionary with the generated data\n    data = {\n        'City': city,\n        'Weather': weather,\n        'Temperature (\u00b0C)': temperature,\n        'Humidity (%)': humidity,\n        'Wind Speed (km/h)': wind_speed,\n        'UTC Datetime': utc_dt,\n        'City Datetime': city_dt\n    }\n    \n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data)\n    \n    return df\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    # Generate random data\n    data = np.random.rand(100, 5)\n    # Create a DataFrame\n    df = pd.DataFrame(data, columns=['A', 'B', 'C', 'D', 'E'])\n    # Perform some operations\n    df['F'] = df['A'] + df['B']\n    df['G'] = df['C'] + df['D']\n    df['H'] = df['E'] + df['F']\n    df['I'] = df['G'] + df['H']\n    # Return the result\n    return df\n\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    # Get the filename from the URL\n    filename = os.path.basename(url)\n\n    # Save the file to the destination directory\n    with open(os.path.join(destination_directory, filename), 'wb') as f:\n        f.write(response.content)\n\n    # Unzip the file\n    with zipfile.ZipFile(os.path.join(destination_directory, filename), 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Remove the zip file\n    os.remove(os.path.join(destination_directory, filename))\n\n    return destination_directory\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    \n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Generate a random image\n    image = np.random.randint(range_low, range_high, size=image_size)\n    \n    # Convert the image to BGR format\n    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    \n    # Display the image\n    plt.imshow(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n    plt.show()\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    \n    \"\"\"\n    # Load audio file\n    audio, sr = librosa.load(audio_file)\n    \n    # Pad audio to length L\n    audio = np.pad(audio, (0, L - len(audio)), 'constant')\n    \n    # Split audio into segments of length M\n    segments = np.array_split(audio, len(audio) // M)\n    \n    # Initialize output array\n    output = np.zeros((len(segments), N))\n    \n    # Iterate over segments\n    for i, segment in enumerate(segments):\n        # Compute features for segment\n        features = np.abs(librosa.stft(segment))\n        \n        # Append features to output array\n        output[i] = features.flatten()\n    \n    return output\n"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    return original\n\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the original data to a numpy array\n    data = np.array(original)\n\n    # Scale the data using StandardScaler\n    scaler = preprocessing.StandardScaler()\n    scaled_data = scaler.fit_transform(data)\n\n    # Plot the original and scaled data\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(original)\n    plt.title('Original Data')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(scaled_data)\n    plt.title('Scaled Data')\n\n    plt.tight_layout()\n    plt.show()\n\n    return scaled_data\n\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    \n    \"\"\"\n    # Convert data to numpy array\n    data = np.array(data)\n\n    # Apply Hamming window\n    window = np.hamming(len(data))\n    data = data * window\n\n    # Perform FFT\n    fft_out = fftpack.fft(data)\n\n    # Get frequencies\n    freqs = fftpack.fftfreq(len(data), 1.0/sample_rate)\n\n    # Get power spectrum\n    power_spectrum = np.abs(fft_out)**2\n\n    # Plot power spectrum\n    plt.plot(freqs, power_spectrum)\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Power')\n    plt.title('Power Spectrum')\n    plt.show()\n\n    return power_spectrum\n\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Your task function code here\n    pass\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    \n    \"\"\"\n    # Create a text message\n    msg = MIMEText(\"Hello, this is a test email sent from a Python script.\")\n    msg['Subject'] = 'Test Email'\n    msg['From'] = smtp_username\n    msg['To'] = 'recipient@example.com'\n\n    # Send the message via our own SMTP server.\n    server = smtplib.SMTP(smtp_server, smtp_port)\n    server.starttls()\n    server.login(smtp_username, smtp_password)\n    server.sendmail(smtp_username, 'recipient@example.com', msg.as_string())\n    server.quit()\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    \"\"\"\n    \n    \"\"\"\n    # Read the file\n    with open(os.path.join(directory, filename), 'r') as f:\n        text = f.read()\n\n    # Split the text into words\n    words = text.split()\n\n    # Count the frequency of each word\n    word_freq = Counter(words)\n\n    # Save the word frequency to a JSON file\n    with open(os.path.join(directory, f'{filename}.json'), 'w') as f:\n        json.dump(dict(word_freq), f)\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Calculate the difference between consecutive dates\n    df['Diff'] = df['Date'].diff()\n\n    # Calculate the difference in days\n    df['Days'] = df['Diff'].dt.days\n\n    # Calculate the average difference in days\n    avg_diff = df['Days'].mean()\n\n    # Print the average difference in days\n    print(f\"Average difference in days: {avg_diff}\")\n\n    # If plot is True, plot the data\n    if plot:\n        # Plot the data\n        plt.figure(figsize=(10, 6))\n        sns.lineplot(x='Date', y='Value', data=df)\n        plt.title('Plot of Data')\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.show()\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    \n    \"\"\"\n    # Generate report data\n    report_data = {\n        'Student': STUDENTS,\n        'Physics': [random.randint(0, 100) for _ in range(100)],\n        'Math': [random.randint(0, 100) for _ in range(100)],\n        'Chemistry': [random.randint(0, 100) for _ in range(100)],\n        'Biology': [random.randint(0, 100) for _ in range(100)],\n        'English': [random.randint(0, 100) for _ in range(100)],\n        'History': [random.randint(0, 100) for _ in range(100)]\n    }\n    \n    # Add additional fields\n    for field in additional_fields:\n        report_data[field] = [random.randint(0, 100) for _ in range(100)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame(report_data)\n    \n    # Calculate the mean of each field\n    mean_values = df.mean(numeric_only=True)\n    \n    # Print the mean values\n    print(mean_values)\n    \n    # Return the DataFrame\n    return df\n"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of people\n    people = []\n    for i in range(PEOPLE_COUNT):\n        person = {\n            'Name': f'Person {i}',\n            'Age': random.randint(18, 80),\n            'Height': random.randint(150, 200),\n            'Weight': random.randint(50, 100)\n        }\n        people.append(person)\n\n    # Write the list of people to a CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=COLUMNS)\n        writer.writeheader()\n        writer.writerows(people)\n\n    # Read the CSV file and calculate the average age\n    with open(filename, 'r') as csvfile:\n        reader = csv.DictReader(csvfile)\n        ages = [int(row['Age']) for row in reader]\n        average_age = mean(ages)\n\n    return average_age\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of all files in the directory\n    files = os.listdir(directory)\n    \n    # Create a list to store the files that are not .txt files\n    non_txt_files = []\n    \n    # Iterate over each file in the directory\n    for file in files:\n        # Check if the file is not a .txt file\n        if not file.endswith('.txt'):\n            # Add the file to the list of non-txt files\n            non_txt_files.append(file)\n    \n    # Create a list to store the files that are .txt files\n    txt_files = []\n    \n    # Iterate over each file in the directory\n    for file in files:\n        # Check if the file is a .txt file\n        if file.endswith('.txt'):\n            # Add the file to the list of txt files\n            txt_files.append(file)\n    \n    # Create a list to store the files that are .txt files and have a size greater than 1000 bytes\n    large_txt_files = []\n    \n    # Iterate over each file in the list of txt files\n    for file in txt_files:\n        # Get the size of the file in bytes\n        file_size = os.path.getsize(os.path.join(directory, file))\n        \n        # Check if the file size is greater than 1000 bytes\n        if file_size > 1000:\n            # Add the file to the list of large txt files\n            large_txt_files.append(file)\n    \n    # Create a list to store the files that are .txt files and have a size less than or equal to 1000 bytes\n    small_txt_files = []\n    \n    # Iterate over each file in the list of txt files\n    for file in txt_files:\n        # Get the size of the file in bytes\n        file_size = os.path.getsize(os.path.join(directory, file))\n        \n        # Check if the file size is less than or equal to 1000 bytes\n        if file_size <= 1000:\n            # Add the file to the list of small txt files\n            small_txt_files.append(file)\n    \n    # Create a list to store the files that are .txt files and have a size greater than 1000 bytes and contain the word 'python'\n    large_python_txt_files = []\n    \n    # Iterate over each file in the list of large txt files\n    for file in large_txt"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\n\ndef task_func(file_list):\n    \"\"\"\n    \n    \"\"\"\n    for file in file_list:\n        print(f\"Processing file: {file}\")\n        # Simulate some work\n        time.sleep(1)\n        print(f\"File {file} processed\")\n\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of files in the directory\n    files = glob.glob(directory_path + \"/*\")\n    \n    # Iterate over the files\n    for file in files:\n        # Check if the file is a directory\n        if os.path.isdir(file):\n            # If it's a directory, recursively call the function\n            task_func(file)\n        else:\n            # If it's a file, print its name\n            print(file)\n"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the column\n    mean = df[col].mean()\n    \n    # Calculate the standard deviation of the column\n    std = df[col].std()\n    \n    # Calculate the variance of the column\n    var = df[col].var()\n    \n    # Calculate the skewness of the column\n    skew = df[col].skew()\n    \n    # Calculate the kurtosis of the column\n    kurt = df[col].kurt()\n    \n    # Print the results\n    print(f\"Mean: {mean}\")\n    print(f\"Standard Deviation: {std}\")\n    print(f\"Variance: {var}\")\n    print(f\"Skewness: {skew}\")\n    print(f\"Kurtosis: {kurt}\")\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(script_path):\n        print(f\"Error: {script_path} does not exist.\")\n        return False\n\n    if not os.access(script_path, os.X_OK):\n        print(f\"Error: {script_path} is not executable.\")\n        return False\n\n    try:\n        if wait:\n            subprocess.run([script_path, *args], check=True)\n        else:\n            subprocess.Popen([script_path, *args])\n    except subprocess.CalledProcessError as e:\n        print(f\"Error: {script_path} failed with return code {e.returncode}.\")\n        return False\n\n    return True\n\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    \n    \"\"\"\n    # Read the Excel file\n    df = pd.read_excel(file_location, sheet_name=sheet_name)\n    \n    # Get the column names\n    columns = df.columns\n    \n    # Get the number of rows\n    num_rows = len(df)\n    \n    # Get the number of columns\n    num_cols = len(columns)\n    \n    # Create a new DataFrame with the column names and number of rows and columns\n    new_df = pd.DataFrame({\n        'Column Names': columns,\n        'Number of Rows': [num_rows] * num_cols,\n        'Number of Columns': [num_cols] * num_cols\n    })\n    \n    # Return the new DataFrame\n    return new_df\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef task_func(activities):\n    \"\"\"\n    \n    \"\"\"\n    # Create a dictionary to store the activities and their corresponding times\n    activity_times = defaultdict(list)\n    \n    # Iterate over the activities and their start and end times\n    for activity, start, end in activities:\n        # Add the start and end times to the list of times for the activity\n        activity_times[activity].append((start, end))\n    \n    # Create a dictionary to store the total time spent on each activity\n    total_time = defaultdict(int)\n    \n    # Iterate over the activities and their times\n    for activity, times in activity_times.items():\n        # Sort the times by start time\n        times.sort(key=lambda x: x[0])\n        \n        # Initialize the current time and the total time for the activity\n        current_time = times[0][0]\n        total_time[activity] = 0\n        \n        # Iterate over the times for the activity\n        for start, end in times:\n            # If the current time is less than the start time, add the time between the current time and the start time to the total time\n            if current_time < start:\n                total_time[activity] += start - current_time\n            \n            # Update the current time to the end time\n            current_time = end\n        \n        # Add the time between the current time and the end of the day to the total time\n        total_time[activity] += 24 - current_time\n    \n    # Return the total time spent on each activity\n    return dict(total_time)\n\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Create a random seed\n    random.seed(seed)\n    \n    # Get a list of all files in the source directory\n    files = os.listdir(src_dir)\n    \n    # Randomly select a subset of files\n    selected_files = random.sample(files, 10)\n    \n    # Create the destination directory if it does not exist\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n    \n    # Copy the selected files to the destination directory\n    for file in selected_files:\n        shutil.copy(os.path.join(src_dir, file), dest_dir)\n    \n    # Return the path to the destination directory\n    return dest_dir\n\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    \n    \"\"\"\n    # Get all the files in the directory\n    files = glob.glob(directory_path + \"*.xlsx\")\n    \n    # Loop through each file\n    for file in files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n        \n        # Get the first sheet\n        sheet = wb.active\n        \n        # Get the first row\n        first_row = sheet[1]\n        \n        # Get the first cell\n        first_cell = first_row[0]\n        \n        # Get the value of the first cell\n        first_cell_value = first_cell.value\n        \n        # Print the value of the first cell\n        print(first_cell_value)\n        \n        # Get the column letters\n        column_letters = [cell.column_letter for cell in first_row]\n        \n        # Print the column letters\n        print(column_letters)\n        \n        # Get the column values\n        column_values = [cell.value for cell in first_row]\n        \n        # Print the column values\n        print(column_values)\n        \n        # Get the column types\n        column_types = [type(cell.value) for cell in first_row]\n        \n        # Print the column types\n        print(column_types)\n        \n        # Get the column widths\n        column_widths = [len(str(cell.value)) for cell in first_row]\n        \n        # Print the column widths\n        print(column_widths)\n        \n        # Get the column max values\n        column_max_values = [max([len(str(cell.value)) for cell in col]) for col in sheet.columns]\n        \n        # Print the column max values\n        print(column_max_values)\n        \n        # Get the column min values\n        column_min_values = [min([len(str(cell.value)) for cell in col]) for col in sheet.columns]\n        \n        # Print the column min values\n        print(column_min_values)\n        \n        # Get the column averages\n        column_averages = [sum([len(str(cell.value)) for cell in col]) / len(col) for col in sheet.columns]\n        \n        # Print the column averages\n        print(column_averages)\n        \n        # Get the column standard deviations\n        column_std_devs = [sum([(len(str(cell.value)) - column_averages[i]) ** 2 for cell in col]) / len(col) for i, col in enumerate(sheet.columns)]\n        \n        #"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    \n    \"\"\"\n    # Generate a time array from 0 to 1 second with sample_size samples\n    time = np.linspace(0, 1, sample_size)\n    \n    # Generate a sine wave with the given frequency\n    sine_wave = np.sin(2 * np.pi * frequency * time)\n    \n    # Calculate the fourier transform of the sine wave\n    fourier_transform = np.fft.fft(sine_wave)\n    \n    # Calculate the frequencies of the fourier transform\n    fourier_frequencies = np.fft.fftfreq(sample_size, d=1/sample_size)\n    \n    # Calculate the absolute values of the fourier transform\n    fourier_abs = np.abs(fourier_transform)\n    \n    # Find the index of the maximum value in the fourier transform\n    max_index = np.argmax(fourier_abs)\n    \n    # Calculate the frequency of the maximum value\n    max_frequency = fourier_frequencies[max_index]\n    \n    return max_frequency\n\n"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = 'smtp.gmail.com'\n    app.config['MAIL_PORT'] = 465\n    app.config['MAIL_USE_TLS'] = False\n    app.config['MAIL_USE_SSL'] = True\n    app.config['MAIL_USERNAME'] = 'your-email@gmail.com'\n    app.config['MAIL_PASSWORD'] = 'your-password'\n    mail = Mail(app)\n    return app, mail\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    \n    \"\"\"\n    # Read the Excel file\n    df = pd.read_excel(excel_file_path)\n\n    # Get the column values\n    column_values = df[column_name].values\n\n    # Create a new list with the column values\n    new_list = [str(value) for value in column_values]\n\n    # Save the new list to a new Excel file\n    new_excel_file_path = os.path.join(os.path.dirname(excel_file_path), f\"{file_name}_new.xlsx\")\n    pd.DataFrame(new_list, columns=[column_name]).to_excel(new_excel_file_path, index=False)\n\n    return new_excel_file_path\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Create a model\n    model = Sequential()\n    model.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n\n    # Compile the model\n    model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n\n    # Train the model\n    model.fit(X_train, Y_train, epochs=100, batch_size=128, verbose=0)\n\n    # Make predictions\n    Y_pred = model.predict(X_test)\n\n    # Return the mean squared error\n    return np.mean((Y_pred - Y_test) ** 2)\n"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Define the model\n    model = keras.Sequential([\n        keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n        keras.layers.Dense(32, activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=0)\n\n    # Make predictions\n    Y_pred = model.predict(X_test)\n\n    # Plot the ROC curve\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return Y_pred\n"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path)\n    \n    # Convert the image to HSV color space\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # Reshape the image to a 2D array\n    hsv_image = hsv_image.reshape((-1, 3))\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(hsv_image)\n    \n    # Get the cluster centers\n    cluster_centers = kmeans.cluster_centers_\n    \n    # Get the cluster labels\n    labels = kmeans.labels_\n    \n    # Create a new image with the cluster centers\n    new_image = np.zeros((image.shape[0], image.shape[1], 3), dtype=np.uint8)\n    for i in range(image.shape[0]):\n        for j in range(image.shape[1]):\n            new_image[i, j] = cluster_centers[labels[i * image.shape[1] + j]]\n    \n    # Save the new image\n    cv2.imwrite('output.jpg', new_image)\n"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    kmeans = KMeans(\n        n_clusters=n_clusters,\n        random_state=random_state,\n        n_init=n_init,\n    )\n    kmeans.fit(P)\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n    plt.scatter(P[:, 0], P[:, 1], c=labels)\n    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n    return labels, plt.gca()\n\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\n\ndef task_func(points, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    # Generate a random point\n    point = np.random.rand(2)\n    # Create a Voronoi diagram\n    vor = Voronoi(points)\n    # Plot the Voronoi diagram\n    voronoi_plot_2d(vor, show_points=False, show_vertices=False, line_alpha=0.6)\n    plt.scatter(point[0], point[1], color='red')\n    plt.show()\n    return point\n\n"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    \n    \"\"\"\n    # Create the destination directory if it does not exist\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    # Get a list of all files in the source directory\n    files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Iterate over each file\n    for file in files:\n        # Get the filename without the extension\n        filename = os.path.splitext(os.path.basename(file))[0]\n\n        # Copy the file to the destination directory\n        shutil.copy2(file, os.path.join(dest_dir, f\"{filename}.{ext}\"))\n\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\n\ndef task_func(json_str):\n    \"\"\"\n    \n    \"\"\"\n    # Load JSON string into a Python dictionary\n    data = json.loads(json_str)\n\n    # Extract the 'name' and 'age' fields from the dictionary\n    name = data['name']\n    age = data['age']\n\n    # Use regular expression to extract the age from the name\n    age_from_name = re.search(r'\\d+', name).group()\n\n    # Compare the age from the name with the actual age\n    if int(age_from_name) == age:\n        return f\"Name: {name}, Age: {age}, Age from name: {age_from_name}\"\n    else:\n        return f\"Name: {name}, Age: {age}, Age from name: {age_from_name} (mismatch)\"\n\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    try:\n        # Run the script\n        process = subprocess.Popen([script_path], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        # Wait for the process to finish\n        output, error = process.communicate(timeout=timeout)\n        # Check if the process finished successfully\n        if process.returncode == 0:\n            # Return the output\n            return {\"status\": \"success\", \"output\": output.decode(\"utf-8\")}\n        else:\n            # Return the error\n            return {\"status\": \"error\", \"error\": error.decode(\"utf-8\")}\n    except subprocess.TimeoutExpired:\n        # Return the error\n        return {\"status\": \"error\", \"error\": \"Timeout expired\"}\n    except Exception as e:\n        # Return the error\n        return {\"status\": \"error\", \"error\": str(e)}\n\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    df = pd.DataFrame({\n        \"category\": np.random.choice(CATEGORIES, N),\n        \"value\": np.random.rand(N)\n    })\n    return df\n\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    # Generate random data\n    x = np.arange(start_time, end_time, step)\n    y = np.random.rand(len(x)) + trend * x\n\n    # Create a DataFrame\n    df = pd.DataFrame({'x': x, 'y': y})\n\n    # Calculate the mean and standard deviation of y\n    mean_y = df['y'].mean()\n    std_y = df['y'].std()\n\n    # Calculate the z-score of y\n    z_score = (df['y'] - mean_y) / std_y\n\n    # Create a new column 'z_score' in the DataFrame\n    df['z_score'] = z_score\n\n    # Return the DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(random_seed)\n    # Convert epoch milliseconds to datetime\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    # Generate a random product\n    product = random.choice(products)\n    # Generate a random order value\n    order_value = random.randint(1, 100)\n    # Generate a random order status\n    order_status = random.choice([\"pending\", \"shipped\", \"delivered\", \"cancelled\"])\n    # Generate a random order date\n    order_date = dt + pd.DateOffset(days=random.randint(-30, 30))\n    # Generate a random order time\n    order_time = dt + pd.Timedelta(\n        seconds=random.randint(0, 24 * 60 * 60), microseconds=random.randint(0, 1e6)\n    )\n    # Generate a random order quantity\n    order_quantity = random.randint(1, 100)\n    # Generate a random order total\n    order_total = order_value * order_quantity\n    # Generate a random order tax\n    order_tax = random.randint(0, 100)\n    # Generate a random order discount\n    order_discount = random.randint(0, 100)\n    # Generate a random order subtotal\n    order_subtotal = order_total - order_tax - order_discount\n    # Generate a random order currency\n    order_currency = random.choice([\"USD\", \"EUR\", \"GBP\", \"JPY\", \"CNY\"])\n    # Generate a random order payment method\n    order_payment_method = random.choice([\"credit_card\", \"paypal\", \"bank_transfer\"])\n    # Generate a random order payment status\n    order_payment_status = random.choice([\"paid\", \"pending\", \"failed\"])\n    # Generate a random order payment date\n    order_payment_date = dt + pd.DateOffset(days=random.randint(-30, 30))\n    # Generate a random order payment time\n    order_payment_time = dt + pd.Timedelta(\n        seconds=random.randint(0, 24 * 60 * 60), microseconds=random.randint(0, 1e6)\n    )\n    # Generate a random order payment amount\n    order_payment_amount = random.randint(1, 100)\n    # Generate a random order payment currency\n   "}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    \n    \"\"\"\n    # Load json string into a dictionary\n    data = json.loads(json_str)\n    \n    # Create a new Excel file\n    wb = xlwt.Workbook()\n    ws = wb.add_sheet(sheet_name)\n    \n    # Write data to the Excel file\n    for i, (key, value) in enumerate(data.items()):\n        ws.write(i, 0, key)\n        ws.write(i, 1, value)\n    \n    # Save the Excel file\n    wb.save(filename)\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    \n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Generate a list of dates in the past\n    dates = [datetime.now() - timedelta(days=x) for x in range(days_in_past)]\n    \n    # Generate a list of random numbers\n    random_numbers = [random.random() for _ in range(days_in_past)]\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'date': dates,\n        'random_number': random_numbers\n    })\n    \n    # Plot the DataFrame\n    sns.set()\n    sns.lineplot(data=df, x='date', y='random_number')\n    plt.show()\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(random_seed)\n    pd.set_option(\"display.max_columns\", None)\n    pd.set_option(\"display.max_rows\", None)\n\n    # Generate random data\n    data = {\n        \"Date\": [\n            datetime(2022, 1, 1) + pd.DateOffset(days=i) for i in range(days_in_past)\n        ],\n        \"AAPL\": np.random.rand(days_in_past),\n        \"GOOGL\": np.random.rand(days_in_past),\n        \"MSFT\": np.random.rand(days_in_past),\n        \"AMZN\": np.random.rand(days_in_past),\n        \"FB\": np.random.rand(days_in_past),\n    }\n\n    # Create DataFrame\n    df = pd.DataFrame(data)\n\n    # Calculate moving averages\n    df[\"MA_5\"] = df[stock_names].rolling(window=5).mean()\n    df[\"MA_10\"] = df[stock_names].rolling(window=10).mean()\n\n    # Calculate moving standard deviations\n    df[\"SD_5\"] = df[stock_names].rolling(window=5).std()\n    df[\"SD_10\"] = df[stock_names].rolling(window=10).std()\n\n    # Calculate Bollinger Bands\n    df[\"BB_5\"] = df[\"MA_5\"] + 2 * df[\"SD_5\"]\n    df[\"BB_10\"] = df[\"MA_10\"] + 2 * df[\"SD_10\"]\n\n    # Calculate RSI\n    delta = df[stock_names].diff().dropna()\n    up, down = delta.copy(), delta.copy()\n    up[up < 0] = 0\n    down[down > 0] = 0\n    roll_up1 = up.rolling(window=14).mean()\n    roll_down1 = down.rolling(window=14).mean().abs()\n    RS = roll_up1 / roll_down1\n    RSI = 100.0 - (100.0 / (1.0 + RS))\n\n    # Add RSI to DataFrame\n    df[\"RSI\"] = RSI\n\n    return df\n\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    \n    \"\"\"\n    # Read the first file\n    with open(file_path1, 'r') as file1:\n        reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n        data1 = list(reader1)\n\n    # Read the second file\n    with open(file_path2, 'r') as file2:\n        reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n        data2 = list(reader2)\n\n    # Find the differences between the two files\n    differences = ndiff(data1, data2)\n\n    # Print the differences\n    for line in differences:\n        if line[0] == '-':\n            print(f\"Removed: {line[2:]}\")\n        elif line[0] == '+':\n            print(f\"Added: {line[2:]}\")\n        elif line[0] == '?':\n            print(f\"Changed: {line[2:]}\")\n\n    # Create a new DataFrame with the differences\n    df = pd.DataFrame(differences, columns=['Type', 'Line', 'Value'])\n\n    # Save the DataFrame to a new CSV file\n    df.to_csv('differences.csv', index=False)\n\n"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the column\n    mean = data[column].mean()\n    \n    # Calculate the standard deviation of the column\n    std = data[column].std()\n    \n    # Calculate the variance of the column\n    var = data[column].var()\n    \n    # Calculate the skewness of the column\n    skew = data[column].skew()\n    \n    # Calculate the kurtosis of the column\n    kurt = data[column].kurt()\n    \n    # Return the calculated statistics\n    return pd.Series([mean, std, var, skew, kurt], index=['mean', 'std', 'var', 'skew', 'kurt'])\n\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the column\n    mean = data[column].mean()\n    \n    # Calculate the standard deviation of the column\n    std = data[column].std()\n    \n    # Calculate the variance of the column\n    var = data[column].var()\n    \n    # Calculate the skewness of the column\n    skew = data[column].skew()\n    \n    # Calculate the kurtosis of the column\n    kurt = data[column].kurt()\n    \n    # Return the calculated statistics\n    return pd.Series([mean, std, var, skew, kurt], index=['mean', 'std', 'var', 'skew', 'kurt'])\n\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    \n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Convert JSON to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create a dictionary to store the count of each word\n    word_count = defaultdict(int)\n    \n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        # Iterate over each word in the text\n        for word in row['text'].split():\n            # Increment the count of the word in the dictionary\n            word_count[word] += 1\n    \n    # Create a new DataFrame with the word counts\n    word_count_df = pd.DataFrame(list(word_count.items()), columns=['word', 'count'])\n    \n    # Sort the DataFrame by count in descending order\n    word_count_df = word_count_df.sort_values(by='count', ascending=False)\n    \n    # Return the top 10 most common words\n    return word_count_df.head(10)\n\n"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    # Extract the first row as column names\n    column_names = data[0]\n\n    # Extract the rest of the data\n    data = data[1:]\n\n    # Convert the data into a pandas DataFrame\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Count the number of occurrences of each value in the 'Name' column\n    name_counts = df['Name'].value_counts()\n\n    # Print the top 5 most common names\n    print(name_counts.nlargest(5))\n\n    # Plot a bar chart of the top 5 most common names\n    plt.figure(figsize=(10, 6))\n    name_counts.nlargest(5).plot(kind='bar')\n    plt.title('Top 5 Most Common Names')\n    plt.xlabel('Name')\n    plt.ylabel('Count')\n    plt.show()\n\n    # Count the number of occurrences of each value in the 'Age' column\n    age_counts = df['Age'].value_counts()\n\n    # Print the top 5 most common ages\n    print(age_counts.nlargest(5))\n\n    # Plot a bar chart of the top 5 most common ages\n    plt.figure(figsize=(10, 6))\n    age_counts.nlargest(5).plot(kind='bar')\n    plt.title('Top 5 Most Common Ages')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.show()\n\n    # Count the number of occurrences of each value in the 'Country' column\n    country_counts = df['Country'].value_counts()\n\n    # Print the top 5 most common countries\n    print(country_counts.nlargest(5))\n\n    # Plot a bar chart of the top 5 most common countries\n    plt.figure(figsize=(10, 6))\n    country_counts.nlargest(5).plot(kind='bar')\n    plt.title('Top 5 Most Common Countries')\n    plt.xlabel('Country')\n    plt.ylabel('Count')\n    plt.show()\n\n    # Count the number of occurrences of each value in the 'City' column\n    city_counts = df['City'].value_counts()\n\n    # Print the top 5 most common cities\n    print(city_counts.nlargest(5))\n\n   "}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # Create a new column 'total' that is the sum of 'A' and 'B'\n    df['total'] = df['A'] + df['B']\n    \n    # Group by 'total' and count the number of occurrences\n    counts = df.groupby('total')['total'].count()\n    \n    # Create a bar plot of the counts\n    ax = sns.barplot(x=counts.index, y=counts.values)\n    \n    return counts, ax\n\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, bins=4):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean and standard deviation of each column\n    means = df.mean(axis=0)\n    stds = df.std(axis=0)\n    \n    # Create a dictionary to store the results\n    results = {}\n    \n    # Iterate over each column\n    for col in df.columns:\n        # Calculate the z-score for each value in the column\n        z_scores = (df[col] - means[col]) / stds[col]\n        \n        # Count the number of values in each bin\n        counts = Counter(np.digitize(z_scores, bins=bins))\n        \n        # Calculate the probability of each bin\n        probabilities = [count / len(z_scores) for count in counts.values()]\n        \n        # Store the results in the dictionary\n        results[col] = probabilities\n    \n    return results\n\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\n\ndef task_func(a, b):\n    \"\"\"\n    \n    \"\"\"\n    return a + b\n\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Convert the 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Group the data by 'date' and calculate the mean of 'value'\n    df_grouped = df.groupby('date')['value'].mean().reset_index()\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df_grouped['date'], df_grouped['value'], marker='o')\n    plt.title('Mean Value Over Time')\n    plt.xlabel('Date')\n    plt.ylabel('Mean Value')\n    plt.grid(True)\n    plt.show()\n\n    # Return the grouped data\n    return df_grouped\n\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Calculate the mean of each column\n    mean = df.mean()\n    \n    # Calculate the standard deviation of each column\n    std = df.std()\n    \n    # Calculate the variance of each column\n    var = df.var()\n    \n    # Calculate the median of each column\n    median = df.median()\n    \n    # Calculate the mode of each column\n    mode = df.mode()\n    \n    # Calculate the range of each column\n    range = df.max() - df.min()\n    \n    # Calculate the interquartile range (IQR) of each column\n    iqr = df.quantile(0.75) - df.quantile(0.25)\n    \n    # Calculate the skewness of each column\n    skew = df.skew()\n    \n    # Calculate the kurtosis of each column\n    kurt = df.kurt()\n    \n    # Print the results\n    print(\"Mean: \", mean)\n    print(\"Standard Deviation: \", std)\n    print(\"Variance: \", var)\n    print(\"Median: \", median)\n    print(\"Mode: \", mode)\n    print(\"Range: \", range)\n    print(\"Interquartile Range (IQR): \", iqr)\n    print(\"Skewness: \", skew)\n    print(\"Kurtosis: \", kurt)\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    \n    \"\"\"\n    # Generate a random array of length array_length\n    x = np.linspace(0, 10, array_length)\n    # Generate a random noise array of length array_length\n    noise = np.random.normal(0, noise_level, array_length)\n    # Generate a random array of length array_length\n    y = np.sin(x) + noise\n    return x, y\n\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(csv_file, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    # Extract the first column\n    first_column = [row[0] for row in data]\n\n    # Count the frequency of each word\n    word_freq = Counter(first_column)\n\n    # Get the top 10 most common words\n    top_10 = word_freq.most_common(10)\n\n    # Print the top 10 most common words\n    print(top_10)\n\n    # Plot a bar chart of the top 10 most common words\n    words, freq = zip(*top_10)\n    plt.bar(words, freq)\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('Top 10 Most Common Words')\n    plt.show()\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef task_func(size=1000):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random data\n    np.random.seed(0)\n    data = np.random.normal(0, 1, size)\n\n    # Calculate the mean and standard deviation\n    mean = np.mean(data)\n    std = np.std(data)\n\n    # Calculate the skewness\n    skewness = stats.skew(data)\n\n    # Calculate the kurtosis\n    kurtosis = stats.kurtosis(data)\n\n    # Print the results\n    print(f\"Mean: {mean}\")\n    print(f\"Standard Deviation: {std}\")\n    print(f\"Skewness: {skewness}\")\n    print(f\"Kurtosis: {kurtosis}\")\n\n    # Plot the histogram\n    plt.hist(data, bins=30, density=True)\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of Random Data\")\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Generate a random key\n    key = get_random_bytes(32)\n    # Encrypt the key\n    cipher = AES.new(key, AES.MODE_EAX)\n    ciphertext, tag = cipher.encrypt_and_digest(b\"secret message\")\n    # Save the key and the encrypted data\n    with open(\"key.bin\", \"wb\") as f:\n        f.write(key)\n    with open(\"encrypted.bin\", \"wb\") as f:\n        f.write(cipher.nonce + tag + ciphertext)\n    # Decrypt the key\n    with open(\"key.bin\", \"rb\") as f:\n        key = f.read()\n    cipher = AES.new(key, AES.MODE_EAX)\n    with open(\"encrypted.bin\", \"rb\") as f:\n        encrypted = f.read()\n    nonce, tag, ciphertext = encrypted[:16], encrypted[16:32], encrypted[32:]\n    cipher.verify(tag)\n    plaintext = cipher.decrypt_and_verify(ciphertext, tag)\n    # Print the decrypted message\n    print(plaintext.decode())\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Generate a random 128-bit IV.\n    iv = os.urandom(16)\n    # Generate a random 256-bit key.\n    key = os.urandom(32)\n    # Create a new AES-CBC cipher context with the given key and IV.\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n    encryptor = cipher.encryptor()\n    # Pad the plaintext to the appropriate length.\n    padder = padding.PKCS7(128).padder()\n    with open(file_path, 'rb') as f:\n        data = f.read()\n    padder.update(data)\n    padder.finalize()\n    padded_data = padder.padded_data()\n    # Encrypt the padded data.\n    ct = encryptor.update(padded_data) + encryptor.finalize()\n    # Encode the IV and key as base64 strings.\n    iv_b64 = b64encode(iv).decode('utf-8')\n    key_b64 = b64encode(key).decode('utf-8')\n    # Return the IV and key as a string, followed by the ciphertext.\n    return iv_b64 + key_b64 + ct\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Download the webpage\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf-8')\n    \n    # Parse the HTML\n    doc = pq(html)\n    \n    # Extract the data\n    title = doc('title').text()\n    date = doc('.date').text()\n    text = doc('.text').text()\n    \n    # Convert the date to a datetime object\n    date = datetime.strptime(date, '%Y-%m-%d')\n    \n    # Return the data as a dictionary\n    return {\n        'title': title,\n        'date': date,\n        'text': text\n    }\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate random data\n    data = []\n    for _ in range(hours):\n        row = {\n            'Time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'Temperature': randint(20, 30),\n            'Humidity': randint(50, 80),\n            'Pressure': randint(950, 1050)\n        }\n        data.append(row)\n\n    # Write data to CSV file\n    filename = f'{output_dir}/data_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n    with open(filename, 'w', newline='') as csvfile:\n        fieldnames = ['Time', 'Temperature', 'Humidity', 'Pressure']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return filename\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate random data\n    data = []\n    for _ in range(hours * 60):\n        timestamp = datetime.now() - datetime.timedelta(minutes=randint(0, hours * 60))\n        vehicle_type = VEHICLE_TYPES[randint(0, len(VEHICLE_TYPES) - 1)]\n        data.append({\n            'timestamp': timestamp,\n            'vehicle_type': vehicle_type\n        })\n\n    # Save data to CSV\n    with open(os.path.join(output_dir, 'data.csv'), 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['timestamp', 'vehicle_type'])\n        writer.writeheader()\n        writer.writerows(data)\n\n    # Save data to JSON\n    with open(os.path.join(output_dir, 'data.json'), 'w') as jsonfile:\n        json.dump(data, jsonfile)\n\n    # Save data to Excel\n    df = pd.DataFrame(data)\n    df.to_excel(os.path.join(output_dir, 'data.xlsx'), index=False)\n\n    # Plot data\n    plt.figure(figsize=(10, 6))\n    plt.bar([x['vehicle_type'] for x in data], [1] * len(data))\n    plt.xlabel('Vehicle Type')\n    plt.ylabel('Count')\n    plt.title('Vehicle Type Distribution')\n    plt.savefig(os.path.join(output_dir, 'distribution.png'))\n\n    return data\n\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate random weather conditions\n    weather_conditions = [WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)] for _ in range(hours)]\n\n    # Generate random temperature values\n    temperature_values = [randint(-20, 40) for _ in range(hours)]\n\n    # Generate random wind speed values\n    wind_speed_values = [randint(0, 100) for _ in range(hours)]\n\n    # Generate random precipitation values\n    precipitation_values = [randint(0, 100) for _ in range(hours)]\n\n    # Generate random humidity values\n    humidity_values = [randint(0, 100) for _ in range(hours)]\n\n    # Generate random pressure values\n    pressure_values = [randint(0, 100) for _ in range(hours)]\n\n    # Generate random date and time values\n    date_time_values = [datetime.now() + timedelta(hours=i) for i in range(hours)]\n\n    # Create a CSV file\n    with open(os.path.join(output_dir, 'weather_data.csv'), 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Date', 'Time', 'Weather Condition', 'Temperature', 'Wind Speed', 'Precipitation', 'Humidity', 'Pressure'])\n        for i in range(hours):\n            writer.writerow([date_time_values[i].strftime('%Y-%m-%d'), date_time_values[i].strftime('%H:%M:%S'), weather_conditions[i], temperature_values[i], wind_speed_values[i], precipitation_values[i], humidity_values[i], pressure_values[i]])\n\n    # Copy the CSV file to the output directory\n    shutil.copy(os.path.join(output_dir, 'weather_data.csv'), os.path.join(output_dir, 'weather_data_copy.csv'))\n\n    return output_dir\n\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the total score\n    total_score = goals * 3 - penalties * PENALTY_COST\n    \n    # Return the total score\n    return total_score\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\n\ndef task_func(rows):\n    \"\"\"\n    \n    \"\"\"\n    # Create a DataFrame from the given rows\n    df = pd.DataFrame(rows, columns=COLUMNS)\n\n    # Perform some operations on the DataFrame\n    df['Column2'] = df['Column2'].astype(str)\n    df['Column3'] = df['Column3'].astype(str)\n\n    # Return the modified DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\ndef task_func(num_students):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random student IDs\n    student_ids = sample(range(1, num_students + 1), num_students)\n    \n    # Generate random scores\n    scores = np.random.randint(0, 101, size=num_students)\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Student ID': student_ids,\n        'Score': scores\n    })\n    \n    # Sort the DataFrame by score in descending order\n    df = df.sort_values(by='Score', ascending=False)\n    \n    # Print the top 5 students\n    print(df.head(5))\n    \n    # Plot a histogram of scores\n    plt.hist(df['Score'], bins=10, edgecolor='black')\n    plt.xlabel('Score')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Scores')\n    plt.show()\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\n\ndef task_func(array, target_value):\n    \"\"\"\n    \n    \"\"\"\n    return np.sum(np.abs(array - target_value))\n\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    \n    \"\"\"\n    # Remove non-alphanumeric characters and convert to lowercase\n    texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n\n    # Tokenize the text\n    tokens = [nltk.word_tokenize(text) for text in texts]\n\n    # Remove stopwords\n    tokens = [[word for word in token if word not in STOPWORDS] for token in tokens]\n\n    # Join the tokens back into strings\n    texts = [' '.join(token) for token in tokens]\n\n    # Vectorize the text\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform(texts)\n\n    # Perform NMF\n    nmf = NMF(n_components=num_topics)\n    nmf.fit(vectors)\n\n    # Get the feature names (i.e., the words in the vocabulary)\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Get the topic weights\n    topic_weights = nmf.components_\n\n    # Get the topic words\n    topic_words = []\n    for i in range(num_topics):\n        topic_words.append(' '.join([feature_names[j] for j in range(len(feature_names)) if topic_weights[i, j] > 0]))\n\n    return topic_words\n\n"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    \n    \"\"\"\n    # Tokenize the text\n    tokens = [ALPHANUMERIC.sub(' ', text).split() for text in texts]\n    # Remove stopwords\n    if stopwords is not None:\n        tokens = [[word for word in token if word not in stopwords] for token in tokens]\n    # Create a Word2Vec model\n    model = Word2Vec(tokens, min_count=1, vector_size=100, window=5, workers=4)\n    # Get the word vectors\n    word_vectors = model.wv\n    # Return the word vectors\n    return word_vectors\n\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    \n    \"\"\"\n    # Load the JSON file\n    with open(path, 'r') as f:\n        data = json.load(f)\n    \n    # Convert the JSON data to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv('output.csv', index=False)\n    \n    # Copy the CSV file to the output directory\n    shutil.copy('output.csv', 'output_directory')\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Get the current date and time\n    now = datetime.now()\n    # Get the current date\n    current_date = now.strftime(\"%Y-%m-%d\")\n    # Get the current time\n    current_time = now.strftime(\"%H:%M:%S\")\n    # Get the current day of the week\n    current_day = now.strftime(\"%A\")\n    # Get the current month\n    current_month = now.strftime(\"%B\")\n    # Get the current year\n    current_year = now.strftime(\"%Y\")\n    # Get the current hour\n    current_hour = now.strftime(\"%H\")\n    # Get the current minute\n    current_minute = now.strftime(\"%M\")\n    # Get the current second\n    current_second = now.strftime(\"%S\")\n    # Get the current day of the month\n    current_day_of_month = now.strftime(\"%d\")\n    # Get the current day of the year\n    current_day_of_year = now.strftime(\"%j\")\n    # Get the current week number\n    current_week_number = now.strftime(\"%U\")\n    # Get the current ISO week number\n    current_iso_week_number = now.strftime(\"%V\")\n    # Get the current day of the year (as a decimal number)\n    current_day_of_year_decimal = now.strftime(\"%j\")\n    # Get the current week number (as a decimal number)\n    current_week_number_decimal = now.strftime(\"%U\")\n    # Get the current ISO week number (as a decimal number)\n    current_iso_week_number_decimal = now.strftime(\"%V\")\n    # Get the current day of the week (as a decimal number)\n    current_day_of_week_decimal = now.strftime(\"%w\")\n    # Get the current day of the week (as a decimal number)\n    current_day_of_week_decimal = now.strftime(\"%w\")\n    # Get the current day of the week (as a decimal number)\n    current_day_of_week_decimal = now.strftime(\"%w\")\n    # Get the current day of the week (as a decimal number)\n    current_day_of_week_decimal = now.strftime(\"%w\")\n    # Get the current day of the week (as a decimal number)\n    current_day_of_week_decimal = now.strftime(\"%w\")\n    # Get the current day of the week (as a decimal number)\n    current_day_of_week_decimal = now.strftime(\"%w\")\n    # Get the current day of the week (as a decimal number)\n    current"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request\n    response = urllib.request.urlopen(url)\n    \n    # Get the content of the response\n    page_content = response.read()\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(page_content, 'html.parser')\n    \n    # Find all the links on the page\n    links = soup.find_all('a')\n    \n    # Create a list to store the scraped data\n    scraped_data = []\n    \n    # Iterate over the links\n    for link in links:\n        # Get the text and href of the link\n        text = link.get_text()\n        href = link.get('href')\n        \n        # If the href is not None, add the data to the list\n        if href is not None:\n            scraped_data.append({\n                'text': text,\n                'href': href\n            })\n    \n    # Write the scraped data to a CSV file\n    with open(CSV_FILE_PATH, 'a', newline='') as csvfile:\n        fieldnames = ['text', 'href']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # If the file is empty, write the header\n        if os.path.getsize(CSV_FILE_PATH) == 0:\n            writer.writeheader()\n        \n        # Write the scraped data\n        writer.writerows(scraped_data)\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into features and target\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean absolute error\n    mae = np.mean(np.abs(y_pred - y_test))\n\n    return mae\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \"\"\"\n    \n    \"\"\"\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Generate a list of random names\n    names = latin_names + other_names\n    np.random.shuffle(names)\n    names = np.repeat(names, 100)\n    \n    # Generate a list of random dates\n    dates = pd.date_range(start='1/1/{}'.format(start_year), end='12/31/{}'.format(end_year), freq='D')\n    np.random.shuffle(dates)\n    dates = np.repeat(dates, 100)\n    \n    # Generate a list of random email addresses\n    emails = [f'{name}@{email_domain}' for name in names]\n    \n    # Generate a list of random phone numbers\n    phone_numbers = [f'{np.random.randint(100, 999)}-{np.random.randint(100, 999)}-{np.random.randint(1000, 9999)}' for _ in range(len(names))]\n    \n    # Generate a list of random addresses\n    addresses = [f'{np.random.randint(1, 1000)} {np.random.choice([\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Lane\"])} {np.random.randint(1, 1000)} {np.random.choice([\"City\", \"Town\", \"Village\"])} {np.random.randint(1, 1000)}' for _ in range(len(names))]\n    \n    # Generate a list of random comments\n    comments = [f'Comment {i}' for i in range(len(names))]\n    \n    # Create a DataFrame\n    df = pd.DataFrame({\n        'Name': names,\n        'Date': dates,\n        'Email': emails,\n        'Phone': phone_numbers,\n        'Address': addresses,\n        'Comment': comments\n    })\n    \n    return df\n"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the input file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n    \n    # Create a dictionary to store the results\n    results = defaultdict(list)\n    \n    # Iterate over the data\n    for item in data:\n        # Extract the relevant information\n        name = item['name']\n        age = item['age']\n        occupation = item['occupation']\n        \n        # Add the information to the results dictionary\n        results[name].append((age, occupation))\n    \n    # Write the results to the output file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Name', 'Age', 'Occupation'])\n        for name, ages in results.items():\n            for age, occupation in ages:\n                writer.writerow([name, age, occupation])\n    \n    return output_file\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of files in the current directory\n    files = os.listdir()\n    \n    # Filter out the files that are not None\n    non_none_files = [file for file in files if file != \"None\"]\n    \n    # Create the target directory if it does not exist\n    target_dir_path = Path(target_dir)\n    if not target_dir_path.exists():\n        target_dir_path.mkdir()\n    \n    # Copy the non-none files to the target directory\n    for file in non_none_files:\n        shutil.copy(file, target_dir)\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    \"\"\"\n    \n    \"\"\"\n    # Create a list to store the results\n    results = []\n\n    # Iterate over each file in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # If it matches, add the filename to the results list\n            results.append(filename)\n\n    # Return the list of matching files\n    return results\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    \n    \"\"\"\n    # Create a list to store the results\n    results = []\n    \n    # Iterate over the files in the directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the pattern\n        if re.match(pattern, filename):\n            # Extract the filename without the version number\n            filename_without_version = re.match(pattern, filename).group(1)\n            \n            # Create a new directory for the file\n            new_directory = os.path.join(directory, filename_without_version)\n            \n            # Check if the directory already exists\n            if not os.path.exists(new_directory):\n                # Create the directory\n                os.makedirs(new_directory)\n                \n                # Extract the zip file\n                with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                    zip_ref.extractall(new_directory)\n                    \n                # Add the result to the list\n                results.append((filename, new_directory))\n    \n    # Return the results\n    return results\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    \n    \"\"\"\n    # Create a temporary directory to store the archive\n    os.mkdir(ARCHIVE_DIR)\n    \n    # Get the list of files to archive\n    files = glob.glob(pattern)\n    \n    # Archive the files\n    for file in files:\n        # Get the filename\n        filename = os.path.basename(file)\n        \n        # Archive the file\n        subprocess.run(['tar', '-czf', os.path.join(ARCHIVE_DIR, filename), file])\n    \n    # Return the archive directory\n    return ARCHIVE_DIR\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    # Extract the team names\n    teams = [row[0] for row in data[1:]]\n\n    # Count the occurrences of each team\n    team_counts = Counter(teams)\n\n    # Calculate the average goals and penalties for each team\n    team_averages = {}\n    for team in team_counts:\n        team_data = [row for row in data[1:] if row[0] == team]\n        team_averages[team] = {\n            'goals': sum(int(row[1]) for row in team_data) / len(team_data),\n            'penalties': sum(int(row[2]) for row in team_data) / len(team_data)\n        }\n\n    # Sort the teams by their average goals in descending order\n    sorted_teams = sorted(team_averages, key=lambda x: team_averages[x]['goals'], reverse=True)\n\n    # Print the sorted teams\n    for team in sorted_teams:\n        print(f\"{team}: {team_averages[team]['goals']} goals, {team_averages[team]['penalties']} penalties\")\n\n    # Return the sorted teams\n    return sorted_teams\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    \n    \"\"\"\n    # Get all files in the source directory\n    files = [f for f in os.listdir(source_dir) if re.match(file_pattern, f)]\n    \n    # Copy files to the target directory\n    for file in files:\n        shutil.copy(os.path.join(source_dir, file), target_dir)\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    \n    \"\"\"\n    # Remove non-alphanumeric characters\n    text1 = ALPHANUMERIC.sub('', text1)\n    text2 = ALPHANUMERIC.sub('', text2)\n\n    # Convert to lowercase\n    text1 = text1.lower()\n    text2 = text2.lower()\n\n    # Calculate the Levenshtein distance\n    distance = ratio(text1, text2)\n\n    # Calculate the similarity\n    similarity = 1 - distance\n\n    return similarity\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    \n    \"\"\"\n    # Generate all possible permutations of the input list\n    perms = list(permutations(numbers))\n    \n    # Initialize an empty list to store the results\n    results = []\n    \n    # Iterate over each permutation\n    for perm in perms:\n        # Calculate the sum of the permutation\n        total = sum(perm)\n        \n        # Check if the sum is a prime number\n        if is_prime(total):\n            # If it is, add the permutation to the results list\n            results.append(perm)\n    \n    # Return the list of permutations that sum to a prime number\n    return results\n"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the source and destination directories exist\n    if not os.path.exists(SOURCE_DIR):\n        raise ValueError(f\"Source directory '{SOURCE_DIR}' does not exist.\")\n    if not os.path.exists(DEST_DIR):\n        raise ValueError(f\"Destination directory '{DEST_DIR}' does not exist.\")\n\n    # Get the list of files in the source directory\n    files = glob.glob(os.path.join(SOURCE_DIR, '*.' + EXTENSIONS[0]))\n    if len(files) == 0:\n        raise ValueError(f\"No files with extension '{EXTENSIONS[0]}' found in '{SOURCE_DIR}'.\")\n\n    # Copy the files to the destination directory\n    for file in files:\n        shutil.copy2(file, DEST_DIR)\n\n    # Wait for a while to simulate a long-running task\n    time.sleep(5)\n"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the z-score for each column\n    z_scores = zscore(data)\n    \n    # Calculate the mean of the z-scores\n    mean_z_scores = np.mean(z_scores, axis=0)\n    \n    # Calculate the standard deviation of the z-scores\n    std_z_scores = np.std(z_scores, axis=0)\n    \n    # Calculate the min and max of the z-scores\n    min_z_scores = np.min(z_scores, axis=0)\n    max_z_scores = np.max(z_scores, axis=0)\n    \n    # Calculate the min and max of the original data\n    min_data = np.min(data, axis=0)\n    max_data = np.max(data, axis=0)\n    \n    # Calculate the min and max of the scaled data\n    min_scaled = np.min(data, axis=0)\n    max_scaled = np.max(data, axis=0)\n    \n    # Calculate the min and max of the normalized data\n    min_normalized = np.min(data, axis=0)\n    max_normalized = np.max(data, axis=0)\n    \n    # Calculate the min and max of the standardized data\n    min_standardized = np.min(data, axis=0)\n    max_standardized = np.max(data, axis=0)\n    \n    # Calculate the min and max of the z-score data\n    min_z_score = np.min(z_scores, axis=0)\n    max_z_score = np.max(z_scores, axis=0)\n    \n    # Calculate the min and max of the scaled data\n    min_scaled_data = np.min(data, axis=0)\n    max_scaled_data = np.max(data, axis=0)\n    \n    # Calculate the min and max of the normalized data\n    min_normalized_data = np.min(data, axis=0)\n    max_normalized_data = np.max(data, axis=0)\n    \n    # Calculate the min and max of the standardized data\n    min_standardized_data = np.min(data, axis=0)\n    max_standardized_data = np.max(data, axis=0)\n    \n    # Calculate the min and max of the z-score data\n    min_z_score_data = np.min(z_scores, axis=0)\n    max_z_score_data = np.max(z_scores, axis=0)\n    \n    # Calculate the min and max of the"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    \n    \"\"\"\n    # Create a DataFrame from the list of tuples\n    df = pd.DataFrame(data_list, columns=['letter', 'number', 'float_number'])\n\n    # Create a list of all possible combinations of the columns\n    all_combinations = list(itertools.product(df['letter'], df['number'], df['float_number']))\n\n    # Create a new DataFrame with the combinations\n    new_df = pd.DataFrame(all_combinations, columns=['letter', 'number', 'float_number'])\n\n    # Return the new DataFrame\n    return new_df\n\n"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    \n    \"\"\"\n    # Create a contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Perform the chi-squared test\n    chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    return chi2_stat, p_value, dof, expected\n\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    rolls = np.random.choice(NUMBERS, size=rolls, replace=True)\n    return rolls\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Create a zip archive\n    with zipfile.ZipFile(archive_name, 'w') as zip_file:\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, source_dir)\n                zip_file.write(file_path, rel_path)\n\n    # Copy the zip archive to the target directory\n    shutil.copy(archive_name, target_dir)\n\n    # Remove the zip archive\n    os.remove(archive_name)\n\n    return target_dir\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Create a new column 'Age' in the dataframe\n    df['Age'] = df['Age'].astype(int)\n    \n    # Create a new column 'Age_Squared' in the dataframe\n    df['Age_Squared'] = df['Age'] ** 2\n    \n    # Create a new column 'Age_Cubed' in the dataframe\n    df['Age_Cubed'] = df['Age'] ** 3\n    \n    # Create a new column 'Age_Quartic' in the dataframe\n    df['Age_Quartic'] = df['Age'] ** 4\n    \n    # Create a new column 'Age_Root' in the dataframe\n    df['Age_Root'] = np.sqrt(df['Age'])\n    \n    # Create a new column 'Age_Log' in the dataframe\n    df['Age_Log'] = np.log(df['Age'])\n    \n    # Create a new column 'Age_Exp' in the dataframe\n    df['Age_Exp'] = np.exp(df['Age'])\n    \n    # Create a new column 'Age_Sin' in the dataframe\n    df['Age_Sin'] = np.sin(df['Age'])\n    \n    # Create a new column 'Age_Cos' in the dataframe\n    df['Age_Cos'] = np.cos(df['Age'])\n    \n    # Create a new column 'Age_Tan' in the dataframe\n    df['Age_Tan'] = np.tan(df['Age'])\n    \n    # Create a new column 'Age_Abs' in the dataframe\n    df['Age_Abs'] = np.abs(df['Age'])\n    \n    # Create a new column 'Age_Sign' in the dataframe\n    df['Age_Sign'] = np.sign(df['Age'])\n    \n    # Create a new column 'Age_Round' in the dataframe\n    df['Age_Round'] = np.round(df['Age'])\n    \n    # Create a new column 'Age_Ceil' in the dataframe\n    df['Age_Ceil'] = np.ceil(df['Age'])\n    \n    # Create a new column 'Age_Floor' in the dataframe\n    df['Age_Floor'] = np.floor(df['Age'])\n    \n    # Create a new column 'Age_Mod' in the dataframe\n    df['Age_Mod'] = df['Age'] %"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate z-scores for each column\n    z_scores = df.apply(zscore)\n    \n    # Select columns with z-scores greater than the threshold\n    selected_cols = z_scores.columns[z_scores.abs().max(axis=0) > z_threshold]\n    \n    # Select the corresponding columns from the original dataframe\n    selected_df = df[selected_cols]\n    \n    return selected_df\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of each column\n    mean_values = df.mean()\n    \n    # Calculate the standard deviation of each column\n    std_values = df.std()\n    \n    # Calculate the variance of each column\n    var_values = df.var()\n    \n    # Calculate the count of each column\n    count_values = df.count()\n    \n    # Calculate the min of each column\n    min_values = df.min()\n    \n    # Calculate the max of each column\n    max_values = df.max()\n    \n    # Calculate the 25th percentile of each column\n    q1_values = df.quantile(0.25)\n    \n    # Calculate the 75th percentile of each column\n    q3_values = df.quantile(0.75)\n    \n    # Calculate the interquartile range (IQR) of each column\n    iqr_values = q3_values - q1_values\n    \n    # Calculate the median of each column\n    median_values = df.median()\n    \n    # Calculate the mode of each column\n    mode_values = df.mode()\n    \n    # Calculate the range of each column\n    range_values = max_values - min_values\n    \n    # Calculate the coefficient of variation (CV) of each column\n    cv_values = std_values / mean_values\n    \n    # Create a dictionary to store the results\n    results = {\n        'mean': mean_values,\n        'std': std_values,\n        'var': var_values,\n        'count': count_values,\n        'min': min_values,\n        'max': max_values,\n        'q1': q1_values,\n        'q3': q3_values,\n        'iqr': iqr_values,\n        'median': median_values,\n        'mode': mode_values,\n        'range': range_values,\n        'cv': cv_values\n    }\n    \n    # Return the results as a tuple\n    return tuple(results.values())\n"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    \n    \"\"\"\n    # Create a new column 'Date' from the 'Date' column\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Set the 'Date' column as the index\n    df.set_index('Date', inplace=True)\n    \n    # Resample the data to monthly frequency\n    df_monthly = df.resample('M').mean()\n    \n    # Plot the data\n    plt.figure(figsize=(10,6))\n    plt.plot(df_monthly.index, df_monthly['Value'], label='Monthly Average')\n    \n    # Add title and labels\n    plt.title('Monthly Average of Value')\n    plt.xlabel('Date')\n    plt.ylabel('Value')\n    \n    # Show the legend and plot\n    plt.legend()\n    plt.show()\n    \n    # Fit an ARIMA model to the data\n    model = ARIMA(df['Value'], order=(1,1,1))\n    model_fit = model.fit()\n    \n    # Print the summary of the model\n    print(model_fit.summary())\n    \n    # Return the residuals and the plot\n    return [model_fit.resid.mean()], plt.gca()\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    # Convert the word to lowercase\n    word = word.lower()\n    \n    # Count the frequency of each character in the word\n    char_freq = Counter(word)\n    \n    # Generate all possible combinations of characters\n    all_combinations = list(itertools.product(string.ascii_lowercase, repeat=len(word)))\n    \n    # Initialize a dictionary to store the results\n    results = {}\n    \n    # Iterate over all combinations\n    for combination in all_combinations:\n        # Convert the combination to a string\n        combination_str = ''.join(combination)\n        \n        # Check if the combination is a valid word\n        if combination_str in word:\n            # Count the frequency of each character in the combination\n            combination_freq = Counter(combination_str)\n            \n            # Check if the frequency of each character in the combination is less than or equal to the frequency in the word\n            if all(combination_freq[char] <= char_freq[char] for char in combination_str):\n                # Add the combination to the results dictionary\n                results[combination_str] = combination_str\n    \n    # Return the results dictionary\n    return results\n\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    \n    \"\"\"\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create a sample dataframe\n    df = pd.DataFrame({\n        'date': date_range,\n        'category': np.random.choice(categories, size=periods),\n        'sales': np.random.randint(0, 100, size=periods)\n    })\n\n    # Group by category and calculate the sum of sales\n    grouped_df = df.groupby('category')['sales'].sum().reset_index()\n\n    # Plot the results\n    plt.figure(figsize=(10, 6))\n    plt.bar(grouped_df['category'], grouped_df['sales'])\n    plt.xlabel('Category')\n    plt.ylabel('Sales')\n    plt.title('Sales by Category')\n    plt.show()\n\n    return grouped_df\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    \n    \"\"\"\n    # Create a date range\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # If sales data is not provided, generate some random data\n    if sales_data is None:\n        sales_data = np.random.randint(0, 100, size=periods)\n    \n    # Create a DataFrame with the date range and sales data\n    df = pd.DataFrame({'date': date_range, 'sales': sales_data})\n    \n    # Create a Linear Regression model\n    model = LinearRegression()\n    \n    # Fit the model to the data\n    model.fit(df[['date']], df['sales'])\n    \n    # Make predictions for the next 5 weeks\n    next_date_range = pd.date_range(start=date_range[-1] + pd.DateOffset(weeks=1), periods=5, freq=freq)\n    next_df = pd.DataFrame({'date': next_date_range})\n    predictions = model.predict(next_df[['date']])\n    \n    # Return the predictions\n    return predictions\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a list of tasks\n    tasks = [f\"Task {i}\" for i in range(1, n_tasks + 1)]\n\n    # Shuffle the tasks\n    random.shuffle(tasks)\n\n    # Assign tasks to employees\n    task_assignments = {}\n    for employee in employees:\n        task_assignments[employee] = tasks[: len(tasks) // len(employees)]\n        tasks = tasks[len(tasks) // len(employees) :]\n\n    # Create a dictionary with task assignments\n    task_dict = {}\n    for employee, tasks in task_assignments.items():\n        task_dict[employee] = [{\"task\": task, \"status\": \"Not Started\"} for task in tasks]\n\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(task_dict)\n\n    # Add a column for the task list\n    df[\"task_list\"] = task_list\n\n    # Add a column for the task number\n    df[\"task_number\"] = df[\"task\"].str.extract(r\"Task (\\d+)\").astype(int)\n\n    # Sort the DataFrame by task number\n    df = df.sort_values(by=\"task_number\")\n\n    # Add a column for the task start date\n    df[\"task_start_date\"] = pd.to_datetime(\"2022-01-01\") + pd.to_timedelta(\n        df[\"task_number\"] * pd.Timedelta(days=1), unit=\"D\"\n    )\n\n    # Add a column for the task end date\n    df[\"task_end_date\"] = df[\"task_start_date\"] + pd.to_timedelta(days=3)\n\n    # Add a column for the task duration\n    df[\"task_duration\"] = (df[\"task_end_date\"] - df[\"task_start_date\"]).dt.days\n\n    # Add a column for the task status\n    df[\"task_status\"] = \"In Progress\"\n\n    # Add a column for the task completion date\n    df[\"task_completion_date\"] = df[\"task_end_date\"]\n\n    # Add a column for the task completion status\n    df[\"task_completion_status\"] = \"Completed\"\n\n    return df\n\n"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n\ndef task_func(mystrings, text):\n    \"\"\"\n    \n    \"\"\"\n    # Split the text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Count the frequency of each word\n    word_freq = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10 = word_freq.most_common(10)\n    \n    # Print the top 10 most common words\n    for word, freq in top_10:\n        print(f\"{word}: {freq}\")\n    \n    # Plot a bar chart of the top 10 most common words\n    words, freqs = zip(*top_10)\n    plt.bar(words, freqs)\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('Top 10 Most Common Words')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    \n    \"\"\"\n    # Create the target directory if it does not exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    # Get the list of files in the source directory\n    files = glob.glob(os.path.join(source_directory, '*'))\n\n    # Create a zip file\n    with zipfile.ZipFile(zip_name, 'w') as zip_file:\n        for file in files:\n            # Get the relative path of the file\n            relative_path = os.path.relpath(file, source_directory)\n            # Add the file to the zip file\n            zip_file.write(file, relative_path)\n\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Create a list of all files in the source directory\n    files = os.listdir(source_directory)\n    \n    # Create a list to store the paths of the files\n    file_paths = []\n    \n    # Iterate over each file in the source directory\n    for file in files:\n        # Get the full path of the file\n        file_path = os.path.join(source_directory, file)\n        \n        # Check if the file is a text file\n        if file.endswith(\".txt\"):\n            # Add the file path to the list\n            file_paths.append(file_path)\n    \n    # Create a list to store the paths of the target files\n    target_file_paths = []\n    \n    # Iterate over each file path\n    for file_path in file_paths:\n        # Get the name of the file\n        file_name = os.path.basename(file_path)\n        \n        # Get the name of the target file\n        target_file_name = file_name.replace(\".txt\", \".docx\")\n        \n        # Get the full path of the target file\n        target_file_path = os.path.join(target_directory, target_file_name)\n        \n        # Add the target file path to the list\n        target_file_paths.append(target_file_path)\n    \n    # Create a list to store the results\n    results = []\n    \n    # Iterate over each file path and target file path\n    for file_path, target_file_path in zip(file_paths, target_file_paths):\n        # Read the text file\n        with open(file_path, \"r\") as file:\n            text = file.read()\n        \n        # Create a new Word document\n        doc = docx.Document()\n        \n        # Add the text to the document\n        doc.add_paragraph(text)\n        \n        # Save the document to the target file path\n        doc.save(target_file_path)\n        \n        # Add the result to the list\n        results.append((file_path, target_file_path))\n    \n    # Return the number of results\n    return len(results)\n\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    \n    \"\"\"\n    # Convert the 'Age' column to numeric\n    df['Age'] = pd.to_numeric(df['Age'], errors='coerce')\n    \n    # Drop rows with missing values in 'Age'\n    df = df.dropna(subset=['Age'])\n    \n    # Scale the 'Age' column\n    scaler = MinMaxScaler()\n    df['Age'] = scaler.fit_transform(df[['Age']])\n    \n    return df\n\n"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    \"\"\"\n    \n    \"\"\"\n    # Create a directory if it doesn't exist\n    Path(directory_path).mkdir(parents=True, exist_ok=True)\n\n    # Get the current date and time\n    current_datetime = datetime.now(timezone.utc)\n\n    # Create a file with the current date and time\n    file_path = os.path.join(directory_path, f\"{current_datetime.strftime('%Y-%m-%d_%H-%M-%S')}.txt\")\n    with open(file_path, \"w\") as file:\n        file.write(\"Hello, World!\")\n\n    return file_path\n\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    \n    \"\"\"\n    # Load the JSON data\n    data = json.loads(json_data)\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(data)\n\n    # Group the data by country and calculate the total population\n    df_grouped = df.groupby('country')['population'].sum().reset_index()\n\n    # Sort the data by population in descending order\n    df_sorted = df_grouped.sort_values(by='population', ascending=False)\n\n    # Write the sorted data to a CSV file\n    file_path = os.path.join(output_dir, file_name)\n    df_sorted.to_csv(file_path, index=False)\n\n    return file_path\n\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    result = {}\n    for predicate in predicates:\n        if predicate(dir_path):\n            result[predicate.__name__] = True\n        else:\n            result[predicate.__name__] = False\n    return result\n\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\n\ndef task_func(hex_string):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the hexadecimal string to bytes\n    bytes_string = bytes.fromhex(hex_string)\n    \n    # Convert the bytes to a base64-encoded string\n    base64_string = base64.b64encode(bytes_string).decode('utf-8')\n    \n    # URL-encode the base64-encoded string\n    url_encoded_string = urllib.parse.quote(base64_string)\n    \n    # Convert the URL-encoded string to a hexadecimal string\n    hex_string = binascii.hexlify(url_encoded_string.encode('utf-8')).decode('utf-8')\n    \n    return hex_string\n\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n    # Check the MD5 checksum\n    md5_checksum = hashlib.md5(open(TARGET_TAR_FILE, \"rb\").read()).hexdigest()\n    if md5_checksum != EXPECTED_MD5_CHECKSUM:\n        raise ValueError(\"MD5 checksum mismatch\")\n\n    # Extract the tar file\n    with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n        tar.extractall()\n\n    # Clean up\n    os.remove(TARGET_TAR_FILE)\n\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\n\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Download the CSV file\n    urllib.request.urlretrieve(url, csv_file_path)\n\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        data = list(reader)\n\n    # Extract the column\n    column = [row[0] for row in data]\n\n    # Count the occurrences of each value\n    counter = collections.Counter(column)\n\n    # Print the result\n    for value, count in counter.items():\n        print(f\"{value}: {count}\")\n\n"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.3'\n    }\n    req = urllib.request.Request(url, headers=headers)\n    response = urllib.request.urlopen(req)\n    html = response.read().decode('utf-8')\n    tree = etree.HTML(html)\n    # print(tree)\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/@href'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/@href')[0])\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()')[0])\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()')[0])\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\"list\"]/ul/li[1]/a/text()'))\n    # print(tree.xpath('//div[@class=\""}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Download the webpage\n    webpage = urllib.request.urlopen(url).read().decode('utf-8')\n    \n    # Find all the links on the webpage\n    links = re.findall(r'<a href=[\\'\"]?([^\\'\" >]+)', webpage)\n    \n    # Count the frequency of each link\n    link_counts = Counter(links)\n    \n    # Plot a bar chart of the link counts\n    plt.bar(link_counts.keys(), link_counts.values())\n    plt.xlabel('Link')\n    plt.ylabel('Count')\n    plt.title('Link Counts')\n    plt.show()\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the file already exists\n    file_name = url.split(\"/\")[-1]\n    file_path = os.path.join(download_path, file_name)\n    if os.path.exists(file_path):\n        print(f\"File {file_name} already exists. Skipping download.\")\n        return\n\n    # Download the file\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        print(f\"Error downloading file: {e}\")\n        return\n\n    # Save the file\n    with open(file_path, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            if chunk:\n                file.write(chunk)\n\n    # Unzip the file\n    try:\n        with ZipFile(file_path, \"r\") as zip_ref:\n            zip_ref.extractall(download_path)\n    except BadZipFile:\n        print(f\"Error unzipping file: {file_path} is not a valid zip file.\")\n        return\n\n    # Remove the zip file\n    os.remove(file_path)\n\n    print(f\"File {file_name} downloaded and unzipped successfully.\")\n\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\n\ndef task_func(url, table_id):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request\n    response = requests.get(url)\n    \n    # If the GET request is successful, the status code will be 200\n    if response.status_code == 200:\n        # Get the content of the response\n        page_content = response.content\n        \n        # Create a BeautifulSoup object and specify the parser\n        soup = BeautifulSoup(page_content, 'html.parser')\n        \n        # Find the table with the specified id\n        table = soup.find('table', id=table_id)\n        \n        # If the table is found, convert it to a pandas DataFrame\n        if table:\n            # Convert the table to a pandas DataFrame\n            df = pd.read_html(StringIO(soup.prettify()))[0]\n            \n            # Return the DataFrame\n            return df\n        else:\n            # If the table is not found, return None\n            return None\n    else:\n        # If the GET request is not successful, return None\n        return None\n\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\n\ndef task_func(url, filename):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    response = requests.get(url, stream=True)\n    with open(DOWNLOAD_DIR / filename, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                file.write(chunk)\n\n    # Unzip the file\n    with zipfile.ZipFile(DOWNLOAD_DIR / filename, \"r\") as zip_ref:\n        zip_ref.extractall(ZIP_DIR)\n\n    # Remove the zip file\n    (DOWNLOAD_DIR / filename).unlink()\n\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the provided URL\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n        return 1\n\n    # Parse the HTML content of the webpage using BeautifulSoup\n    soup = BeautifulSoup(response.text, \"html.parser\")\n\n    # Find all the links on the webpage\n    links = soup.find_all(\"a\")\n\n    # Create a list to store the scraped data\n    scraped_data = []\n\n    # Iterate over each link\n    for link in links:\n        # Get the href attribute of the link\n        href = link.get(\"href\")\n\n        # If the href attribute is not None\n        if href is not None:\n            # Join the base URL with the href attribute to get the absolute URL\n            absolute_url = urljoin(base_url, href)\n\n            # Append the scraped data to the list\n            scraped_data.append({\"url\": absolute_url})\n\n    # Write the scraped data to a CSV file\n    with open(csv_file, \"w\", newline=\"\") as file:\n        writer = csv.DictWriter(file, fieldnames=[\"url\"])\n        writer.writeheader()\n        writer.writerows(scraped_data)\n\n    # Return 0 to indicate success\n    return 0\n\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Connect to the SQLite database\n    conn = sqlite3.connect(database_name)\n    c = conn.cursor()\n\n    # Create table if not exists\n    c.execute('''CREATE TABLE IF NOT EXISTS webpage_data\n                 (title text, url text, date text)''')\n\n    # Send a GET request\n    response = requests.get(webpage_url)\n\n    # If the GET request is successful, the status code will be 200\n    if response.status_code == 200:\n        # Get the content of the response\n        page_content = response.content\n\n        # Create a HTML object\n        tree = html.fromstring(page_content)\n\n        # Find all the title elements\n        title_elements = tree.xpath('//title/text()')\n\n        # Find all the url elements\n        url_elements = tree.xpath('//a/@href')\n\n        # Find all the date elements\n        date_elements = tree.xpath('//time/@datetime')\n\n        # Create a dictionary to store the data\n        data = {\n            \"title\": title_elements,\n            \"url\": url_elements,\n            \"date\": date_elements\n        }\n\n        # Convert the dictionary to a DataFrame\n        df = pd.DataFrame(data)\n\n        # Insert the data into the table\n        for index, row in df.iterrows():\n            c.execute(\"INSERT INTO webpage_data VALUES (?, ?, ?)\",\n                      (row['title'], row['url'], row['date']))\n\n        # Commit the changes\n        conn.commit()\n\n        # Close the connection\n        conn.close()\n\n        # Return the number of rows inserted\n        return df.shape[0]\n\n    else:\n        # If the GET request is not successful, return 0\n        return 0\n\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\n\nIMAGE_PATH = \"image.png\"\n\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    # Open image\n    image = Image.open(filename)\n    \n    # Convert image to text\n    text = pytesseract.image_to_string(image)\n    \n    # Decode text\n    text = text.decode(from_encoding)\n    \n    # Encode text\n    text = text.encode(to_encoding)\n    \n    # Save text to file\n    with codecs.open(\"output.txt\", \"w\", to_encoding) as f:\n        f.write(text.decode(to_encoding))\n    \n    return text\n\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the API\n    response = requests.get(url)\n\n    # Get the encoding of the response\n    if from_encoding is None:\n        from_encoding = chardet.detect(response.content)['encoding']\n\n    # Decode the response content\n    content = response.content.decode(from_encoding)\n\n    # Convert the content to the desired encoding\n    content = content.encode(from_encoding).decode(to_encoding)\n\n    # Return the content\n    return content\n\n"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n\n        # Check if the column exists\n        if column_name not in df.columns:\n            raise ValueError(f\"Column '{column_name}' not found in the CSV file.\")\n\n        # Convert the column to datetime\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n        # Save the updated DataFrame to a new CSV file\n        new_csv_file_path = f\"{os.path.splitext(csv_file_path)[0]}_updated{os.path.splitext(csv_file_path)[1]}\"\n        df.to_csv(new_csv_file_path, index=False)\n\n        print(f\"Updated CSV file saved to: {new_csv_file_path}\")\n\n    except EmptyDataError:\n        print(f\"No data found in the CSV file: {csv_file_path}\")\n    except ValueError as e:\n        print(f\"Error: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    \n    \"\"\"\n    # Create a SSL context\n    context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n    context.load_cert_chain(cert_file, key_file)\n\n    # Wrap the client socket with the SSL context\n    ssl_socket = context.wrap_socket(client_socket, server_side=True)\n\n    # Receive data from the client\n    data = ssl_socket.recv(buffer_size)\n    while data:\n        # Process the data\n        print(f\"Received: {data.decode()}\")\n\n        # Send a response back to the client\n        response = \"Hello, client!\"\n        ssl_socket.send(response.encode())\n\n        # Receive more data from the client\n        data = ssl_socket.recv(buffer_size)\n\n    # Close the SSL socket\n    ssl_socket.close()\n\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    \n    \"\"\"\n    # Create a UDP socket\n    client_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\n    # Set the timeout to 1 second\n    client_socket.settimeout(1)\n\n    # Connect to the server\n    client_socket.connect((server_address, server_port))\n\n    # Create a queue to hold the received data\n    data_queue = queue.Queue()\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list to hold the received data\n    data_list = []\n\n    # Create a list"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\n\ndef task_func(client_socket):\n    \"\"\"\n    \n    \"\"\"\n    while True:\n        try:\n            data = client_socket.recv(BUFFER_SIZE)\n            if not data:\n                break\n            print(f\"Received: {data.decode()}\")\n            response = input(\"Enter your response: \")\n            client_socket.send(response.encode())\n        except Exception as e:\n            print(f\"Error: {e}\")\n            break\n    client_socket.close()\n\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    df = pd.read_csv(file_path)\n\n    # Remove stop words\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in STOP_WORDS]))\n\n    # Vectorize the text\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(df['text'])\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Get the feature counts\n    feature_counts = X.toarray().sum(axis=0)\n\n    # Create a DataFrame\n    df_features = pd.DataFrame({'feature': feature_names, 'count': feature_counts})\n\n    # Sort the DataFrame\n    df_features = df_features.sort_values(by='count', ascending=False)\n\n    # Save the DataFrame\n    if save_path:\n        df_features.to_csv(save_path, index=False)\n\n    # Plot the top 10 features\n    plt.figure(figsize=(10, 6))\n    plt.bar(df_features['feature'][:10], df_features['count'][:10])\n    plt.xlabel('Feature')\n    plt.ylabel('Count')\n    plt.title('Top 10 Features')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\n\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    \n    \"\"\"\n    if animals is None:\n        animals = ['cat', 'dog', 'bird']\n    if foods is None:\n        foods = ['fish', 'chicken', 'beef']\n    # Create a list of all possible combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    # Create a DataFrame with the combinations\n    df = pd.DataFrame(combinations, columns=['Animal', 'Food'])\n    return df\n\n"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    \n    \"\"\"\n    # Convert time strings to datetime objects\n    time_objects = []\n    for time_string in time_strings:\n        time_object = datetime.strptime(time_string, '%Y-%m-%d %H:%M:%S')\n        time_objects.append(time_object)\n\n    # Convert datetime objects to timezone-aware datetime objects\n    timezone = pytz.timezone(timezone)\n    time_objects = [timezone.localize(time_object) for time_object in time_objects]\n\n    # Sort the datetime objects\n    time_objects.sort()\n\n    # Get the time difference between the first and last datetime objects\n    time_diff = time_objects[-1] - time_objects[0]\n\n    # Convert the time difference to seconds\n    time_diff_seconds = time_diff.total_seconds()\n\n    # Calculate the average time difference\n    avg_time_diff = time_diff_seconds / len(time_objects)\n\n    # Return the average time difference\n    return avg_time_diff\n\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_freq = Counter(words)\n    \n    # Get the 10 most common words\n    most_common = word_freq.most_common(10)\n    \n    # Print the 10 most common words\n    for word, freq in most_common:\n        print(f\"{word}: {freq}\")\n    \n    # Plot a bar chart of the 10 most common words\n    words, freqs = zip(*most_common)\n    plt.bar(words, freqs)\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('10 Most Common Words')\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n\ndef task_func(myString):\n    \"\"\"\n    \n    \"\"\"\n    # Remove HTML tags\n    myString = re.sub(r'<.*?>', '', myString)\n    \n    # Remove special characters\n    myString = re.sub(r'[^a-zA-Z0-9\\s]', '', myString)\n    \n    # Remove extra spaces\n    myString = re.sub(r'\\s+', ' ', myString)\n    \n    # Remove leading and trailing spaces\n    myString = myString.strip()\n    \n    return myString\n\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    \n    \"\"\"\n    # Extract the relevant information from the JSON data\n    # For example, let's assume we want to extract the 'name' and 'age' fields\n    name = json_data['name']\n    age = json_data['age']\n\n    # Do some processing on the extracted information\n    # For example, let's assume we want to calculate the person's age in years\n    age_in_years = age\n\n    # Save the processed information to a file\n    # If save_dir is not provided, use the current working directory\n    if save_dir is None:\n        save_dir = os.getcwd()\n    save_path = os.path.join(save_dir, f\"{name}_{age_in_years}.txt\")\n    with open(save_path, 'w') as f:\n        f.write(f\"Name: {name}\\nAge: {age_in_years}\")\n\n    # Return a message indicating the task was completed\n    return f\"Task completed for {name} with age {age_in_years}\"\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request\n    response = requests.get(url)\n    \n    # If the GET request is successful, the status code will be 200\n    if response.status_code == 200:\n        # Get the content of the response\n        page_content = response.content\n        \n        # Create a BeautifulSoup object and specify the parser\n        soup = bs4.BeautifulSoup(page_content, 'html.parser')\n        \n        # Find all the links on the page\n        links = soup.find_all('a')\n        \n        # Create a list to store the links\n        link_list = []\n        \n        # Iterate over the links\n        for link in links:\n            # Get the href attribute of the link\n            href = link.get('href')\n            \n            # If the href attribute is not None\n            if href is not None:\n                # Add the href to the list\n                link_list.append(href)\n        \n        # Write the list to a JSON file\n        with open(output_path, 'w') as f:\n            json.dump(link_list, f)\n        \n        # Return the list of links\n        return link_list\n    \n    else:\n        # If the GET request is not successful, return an empty list\n        return []\n"}
