{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    \n    \"\"\"\n    # Connect to the FTP server\n    ftp = ftplib.FTP(ftp_server)\n    ftp.login(user=ftp_user, passwd=ftp_password)\n    \n    # Change to the specified directory\n    ftp.cwd(ftp_dir)\n    \n    # List the files in the directory\n    files = ftp.nlst()\n    \n    # Download each file\n    for file in files:\n        local_file = open(file, 'wb')\n        ftp.retrbinary('RETR ' + file, local_file.write)\n        local_file.close()\n    \n    # Close the FTP connection\n    ftp.quit()\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    \n    \"\"\"\n    with open(commands_file_path, 'r') as f:\n        commands = f.readlines()\n\n    for command in commands:\n        command = command.strip()\n        if command:\n            try:\n                subprocess.run(command, shell=True, check=True)\n            except subprocess.CalledProcessError as e:\n                print(f\"Error executing command: {command}\")\n                print(f\"Error message: {e}\")\n\n    # Get the list of files in the output directory\n    files = os.listdir(output_dir_path)\n\n    # Create a CSV file to store the file names and their sizes\n    with open('file_sizes.csv', 'w', newline='') as csvfile:\n        fieldnames = ['file_name', 'file_size']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for file in files:\n            file_path = os.path.join(output_dir_path, file)\n            if os.path.isfile(file_path):\n                file_size = os.path.getsize(file_path)\n                writer.writerow({'file_name': file, 'file_size': file_size})\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    \n    \"\"\"\n    try:\n        process = psutil.Process(psutil.Process().pid)\n        children = process.children(recursive=True)\n        for child in children:\n            if child.name() == process_name:\n                return \"Task is running\"\n        return \"Task is not running\"\n    except psutil.NoSuchProcess:\n        return \"Process does not exist\"\n"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # Get all the zip files in the directory\n    zip_files = glob.glob(os.path.join(directory, '*.zip'))\n    \n    # Loop through each zip file\n    for zip_file in zip_files:\n        # Extract the zip file\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall(directory)\n        \n        # Remove the zip file\n        os.remove(zip_file)\n"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u6807\u70b9\u7b26\u53f7\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u6570\u5b57\n    text = re.sub(r'\\d', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u82f1\u6587\n    text = re.sub(r'[a-zA-Z]', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u4e2d\u6587\n    text = re.sub(r'[\\u4e00-\\u9fff]', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u7a7a\u683c\n    text = re.sub(r'\\s', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u6362\u884c\u7b26\n    text = re.sub(r'\\n', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u5236\u8868\u7b26\n    text = re.sub(r'\\t', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u56de\u8f66\u7b26\n    text = re.sub(r'\\r', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u9017\u53f7\n    text = re.sub(r',', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u53e5\u53f7\n    text = re.sub(r'\\.', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u5192\u53f7\n    text = re.sub(r':', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u5206\u53f7\n    text = re.sub(r';', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u5f15\u53f7\n    text = re.sub(r'\"', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u5355\u5f15\u53f7\n    text = re.sub(r\"'\", '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u62ec\u53f7\n    text = re.sub(r'\\(', '', text)\n    text = re.sub(r'\\)', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u65b9\u62ec\u53f7\n    text = re.sub(r'\\[', '', text)\n    text = re.sub(r'\\]', '', text)\n    # \u53bb\u9664\u6587\u672c\u4e2d\u7684\u5927\u62ec\u53f7\n    text = re.sub(r'\\{', '', text)\n    text = re.sub(r'\\}', '', text)\n    "}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, target_column):\n    \"\"\"\n    \n    \"\"\"\n    # Create a Random Forest Classifier\n    clf = RandomForestClassifier(n_estimators=100)\n\n    # Train the model using the training sets\n    clf.fit(df.drop(target_column, axis=1), df[target_column])\n\n    # Predict the response for test dataset\n    y_pred = clf.predict(df.drop(target_column, axis=1))\n\n    # Model Accuracy\n    print(\"Accuracy:\", metrics.accuracy_score(df[target_column], y_pred))\n\n    # Feature Importance\n    importances = clf.feature_importances_\n    indices = np.argsort(importances)[::-1]\n\n    # Print the feature ranking\n    print(\"Feature ranking:\")\n\n    for f in range(df.drop(target_column, axis=1).shape[1]):\n        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n    # Plot the feature importances of the forest\n    plt.figure()\n    plt.title(\"Feature importances\")\n    plt.bar(range(df.drop(target_column, axis=1).shape[1]), importances[indices],\n            color=\"r\", align=\"center\")\n    plt.xticks(range(df.drop(target_column, axis=1).shape[1]), indices)\n    plt.xlim([-1, df.drop(target_column, axis=1).shape[1]])\n    plt.show()\n\n    return clf\n\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(__name__, template_folder=template_folder)\n    app.config['SECRET_KEY'] = secret_key\n    login_manager.init_app(app)\n    login_manager.login_view = 'login'\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password = password\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    users = {\n        1: User(1, 'user1', generate_password_hash('password1')),\n        2: User(2, 'user2', generate_password_hash('password2'))\n    }\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return users.get(int(user_id))\n\n    @app.route('/')\n    def home():\n        return render_template('home.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = load_user(form.username.data)\n            if user and"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the z-scores for the specified column\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers based on the z-score\n    outliers = data[z_scores > outlier_z_score]\n    \n    # Remove outliers from the data\n    cleaned_data = data[z_scores <= outlier_z_score]\n    \n    # Standardize the cleaned data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(cleaned_data)\n    \n    # Plot the original data and the cleaned data\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.scatter(data[column], np.zeros_like(data[column]), label='Original Data')\n    plt.scatter(outliers[column], np.zeros_like(outliers[column]), color='red', label='Outliers')\n    plt.title('Original Data with Outliers')\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(scaled_data[:, 0], scaled_data[:, 1], label='Standardized Data')\n    plt.title('Standardized Data')\n    plt.legend()\n    \n    plt.show()\n    \n    return scaled_data\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    \n    \"\"\"\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n    \n    # Create a scatter plot of the data colored by cluster\n    plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_, cmap='viridis')\n    \n    # Plot the cluster centers\n    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, alpha=0.5)\n    \n    # Show the plot\n    plt.show()\n    \n    return kmeans.labels_\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    \n    \"\"\"\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    principalComponents = pca.fit_transform(data)\n    \n    # Create a DataFrame for the principal components\n    principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n    \n    return principalDf\n"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Load the Iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target\n\n    # Create a pairplot of the Iris dataset\n    sns.pairplot(df, hue='species')\n    plt.show()\n"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    data = {\n        'Date': [datetime(2022, 1, 1) + pd.DateOffset(days=i) for i in range(10)],\n        'Value': [random.randint(1, 100) for _ in range(10)]\n    }\n    df = pd.DataFrame(data)\n    \n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df['Value'])\n    ax.set_title('Random Values Over Time')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    \n    plt.show()\n\ntask_func()\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load the Boston housing dataset\n    boston = pd.read_csv(data_url, sep=\"\\s+\", header=None)\n    boston.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n    \n    # Split the dataset into training and testing sets\n    np.random.seed(seed)\n    train_indices = np.random.choice(boston.index, size=int(len(boston) * 0.8), replace=False)\n    test_indices = boston.index.difference(train_indices)\n    train_set = boston.loc[train_indices]\n    test_set = boston.loc[test_indices]\n    \n    # Perform exploratory data analysis\n    print(boston.describe())\n    sns.pairplot(boston)\n    plt.show()\n    \n    # Implement a linear regression model\n    from sklearn.linear_model import LinearRegression\n    model = LinearRegression()\n    model.fit(train_set.drop(\"MEDV\", axis=1), train_set[\"MEDV\"])\n    \n    # Evaluate the model\n    train_predictions = model.predict(train_set.drop(\"MEDV\", axis=1))\n    test_predictions = model.predict(test_set.drop(\"MEDV\", axis=1))\n    train_mse = np.mean((train_predictions - train_set[\"MEDV\"]) ** 2)\n    test_mse = np.mean((test_predictions - test_set[\"MEDV\"]) ** 2)\n    print(f\"Train MSE: {train_mse}, Test MSE"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the dataframe is empty\n    if df.empty:\n        return \"The dataframe is empty.\"\n    \n    # Check if the dataframe has a datetime index\n    if not isinstance(df.index, pd.DatetimeIndex):\n        return \"The dataframe does not have a datetime index.\"\n    \n    # Check if the dataframe has only one column\n    if len(df.columns) != 1:\n        return \"The dataframe should have only one column.\"\n    \n    # Perform seasonal decomposition\n    decomposition = seasonal_decompose(df, model=decomposition_model, freq=freq)\n    \n    # Plot the results\n    plt.figure(figsize=(12, 8))\n    plt.subplot(411)\n    plt.plot(df, label='Original')\n    plt.legend(loc='best')\n    plt.subplot(412)\n    plt.plot(decomposition.trend, label='Trend')\n    plt.legend(loc='best')\n    plt.subplot(413)\n    plt.plot(decomposition.seasonal,label='Seasonality')\n    plt.legend(loc='best')\n    plt.subplot(414)\n    plt.plot(decomposition.resid, label='Residuals')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n    \n    return decomposition\n"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    \n    # Generate a list of dates between start_date and end_date\n    dates = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n    \n    # Generate a list of random numbers between 1 and 100 for each date\n    numbers = [randint(1, 100) for _ in range(len(dates))]\n    \n    # Create a DataFrame with the dates and numbers\n    df = pd.DataFrame({'Date': dates, 'Number': numbers})\n    \n    # Set the Date column as the index\n    df.set_index('Date', inplace=True)\n    \n    return df\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the directory exists\n    if not os.path.exists(file_dir):\n        print(f\"The directory {file_dir} does not exist.\")\n        return\n\n    # Get a list of all files with the specified extension in the directory\n    file_list = glob.glob(os.path.join(file_dir, '*' + file_ext))\n\n    # Check if any files were found\n    if not file_list:\n        print(f\"No files with the extension {file_ext} were found in the directory {file_dir}.\")\n        return\n\n    # Read the first file in the list and store its data in a DataFrame\n    df = pd.read_csv(file_list[0])\n\n    # Iterate over the remaining files and concatenate their data to the DataFrame\n    for file in file_list[1:]:\n        df = pd.concat([df, pd.read_csv(file)], ignore_index=True)\n\n    # Check if the DataFrame is empty\n    if df.empty:\n        print(\"The DataFrame is empty after reading all the files.\")\n        return\n\n    # Perform the specified task on the DataFrame\n    result = df[my_list]\n\n    # Return the result\n    return result\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    \n    \"\"\"\n    random_seed(seed)\n    for i in range(size):\n        my_list.append(randint(0, 100))\n    return sum(my_list)\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find all the table elements on the page\n    tables = soup.find_all('table')\n    \n    # Create an empty list to store the data\n    data = []\n    \n    # Loop through each table element\n    for table in tables:\n        # Find all the row elements in the table\n        rows = table.find_all('tr')\n        \n        # Loop through each row element\n        for row in rows:\n            # Find all the cell elements in the row\n            cells = row.find_all('td')\n            \n            # If there are cells in the row\n            if cells:\n                # Extract the text from each cell and store it in a list\n                row_data = [cell.get_text() for cell in cells]\n                \n                # Append the row data to the data list\n                data.append(row_data)\n    \n    # Create a pandas DataFrame from the data list\n    df = pd.DataFrame(data)\n    \n    # Return the DataFrame\n    return df\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the 'value' column for each unique 'category'\n    mean_values = df.groupby('category')['value'].mean()\n    \n    # Plot the mean values\n    plt.bar(mean_values.index, mean_values.values)\n    plt.xlabel('Category')\n    plt.ylabel('Mean Value')\n    plt.title('Mean Values by Category')\n    plt.show()\n"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    \n    \"\"\"\n    for ip in IPv4Network(ip_range):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((str(ip), port))\n        if result == 0:\n            print(f\"Port {port} is open on {ip}\")\n        sock.close()\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(log_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the log file\n    with open(log_file, 'r') as file:\n        log_data = file.readlines()\n\n    # Initialize an empty list to store the parsed data\n    parsed_data = []\n\n    # Regular expression pattern to match the log format\n    pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) - (INFO|WARNING|ERROR) - (.+)'\n\n    # Iterate over the log data\n    for line in log_data:\n        # Use the regular expression to match the log line\n        match = re.match(pattern, line)\n        if match:\n            # Extract the timestamp, log level, and log message\n            timestamp = match.group(1)\n            log_level = match.group(2)\n            log_message = match.group(3)\n\n            # Convert the timestamp to a datetime object\n            timestamp = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n\n            # Append the parsed data to the list\n            parsed_data.append((timestamp, log_level, log_message))\n\n    # Create a pandas DataFrame from the parsed data\n    df = pd.DataFrame(parsed_data, columns=['Timestamp', 'Log Level', 'Log Message'])\n\n    # Return the DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u63d0\u53d6\u6587\u672c\u4e2d\u7684\u6570\u5b57\n    numbers = re.findall(r'\\d+', text)\n    numbers = [int(num) for num in numbers]\n\n    # 2. \u8ba1\u7b97\u6570\u5b57\u7684\u9891\u7387\n    freq = {}\n    for num in numbers:\n        if num in freq:\n            freq[num] += 1\n        else:\n            freq[num] = 1\n\n    # 3. \u7ed8\u5236\u76f4\u65b9\u56fe\n    plt.hist(numbers, bins=range(min(numbers), max(numbers)+2), rwidth=rwidth)\n    plt.xlabel('Numbers')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Numbers in Text')\n    plt.show()\n\n    # 4. \u8ba1\u7b97\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\n    mean = np.mean(numbers)\n    std_dev = np.std(numbers)\n\n    return mean, std_dev\n\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\n\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Remove punctuation\n    df['text'] = df['text'].str.translate(str.maketrans('', '', punctuation))\n    \n    # Tokenize the text\n    df['text'] = df['text'].apply(nltk.word_tokenize)\n    \n    # Remove stop words\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n    \n    # Lemmatize the text\n    lemmatizer = nltk.WordNetLemmatizer()\n    df['text'] = df['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n    \n    # Join the tokens back into a string\n    df['text'] = df['text'].apply(lambda x: ' '.join(x))\n    \n    return df\n\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the text_column exists in the dataframe\n    if text_column not in dataframe.columns:\n        raise ValueError(f\"The column '{text_column}' does not exist in the dataframe.\")\n\n    # Remove non-alphabetic characters and convert to lowercase\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x).lower())\n\n    # Remove stopwords\n    dataframe[text_column] = dataframe[text_column].apply("}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random coordinates within the specified range\n    np.random.seed(0)  # For reproducibility\n    lon = np.random.uniform(dic['Lon'][0], dic['Lon'][1], size=len(cities))\n    lat = np.random.uniform(dic['Lat'][0], dic['Lat'][1], size=len(cities))\n    \n    # Create a GeoDataFrame with the random coordinates and city names\n    geometry = [Point(x, y) for x, y in zip(lon, lat)]\n    gdf = gpd.GeoDataFrame(data={'City': cities}, geometry=geometry)\n    \n    return gdf\n"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    \n    \"\"\"\n    set_seed(seed)\n    \n    # Convert the input UTC datetime to the corresponding timezone\n    timezone = pytz.timezone(timezones[cities[randint(0, len(cities) - 1)]])\n    local_datetime = utc_datetime.astimezone(timezone)\n    \n    # Generate a random weather condition\n    weather = weather_conditions[randint(0, len(weather_conditions) - 1)]\n    \n    # Create a DataFrame with the local datetime and weather condition\n    df = pd.DataFrame({\n        'Local Datetime': [local_datetime],\n        'Weather Condition': [weather]\n    })\n    \n    return df\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    return np.random.choice(elements, size=len(elements), replace=False)\n\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the destination directory exists, if not, create it\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the file\n    response = requests.get(url, headers=headers)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file from {url}\")\n\n    # Save the file to the destination directory\n    file_path = os.path.join(destination_directory, url.split('/')[-1])\n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n\n    # Extract the zip file\n    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Remove the zip file\n    os.remove(file_path)\n\n    return file_path\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    image = np.random.randint(range_low, range_high, image_size)\n    return image\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    \n    \"\"\"\n    # Load audio file\n    audio, sr = librosa.load(audio_file)\n    \n    # Compute Mel-frequency cepstral coefficients (MFCCs)\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=L)\n    \n    # Compute chroma features\n    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n    \n    # Compute tonnetz features\n    tonnetz = librosa.feature.tonnetz(y=audio, sr=sr)\n    \n    # Compute MFCCs, chroma, and tonnetz features\n    features = np.concatenate((mfccs, chroma, tonnetz), axis=0)\n    \n    # Reshape features to (1, -1)\n    features = features.reshape(1, -1)\n    \n    # Normalize features\n    features = librosa.util.normalize(features)\n    \n    # Compute the mean and standard deviation of the features\n    mean = np.mean(features, axis=1)\n    std = np.std(features, axis=1)\n    \n    # Compute the weighted mean and standard deviation\n    weighted_mean = np.sum(features * np.arange(features.shape[1]).reshape(-1, 1), axis=1) / np.sum(features, axis=1)\n    weighted_std = np.sqrt(np.sum((features - weighted_mean.reshape(-1, 1))**2 * np.arange(features.shape[1]).reshape(-1, 1), axis=1) / np.sum(features, axis=1))\n    \n    # Compute the median and interquartile range of the features\n    median = np.median(features, axis=1)\n    iqr = np.percentile(features, "}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    return original\n\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n\ndef task_func(original):\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u6570\u636e\u9884\u5904\u7406\n    # 1.1 \u6570\u636e\u6807\u51c6\u5316\n    scaler = preprocessing.StandardScaler().fit(original)\n    standardized = scaler.transform(original)\n    \n    # 1.2 \u6570\u636e\u5f52\u4e00\u5316\n    min_max_scaler = preprocessing.MinMaxScaler().fit(original)\n    normalized = min_max_scaler.transform(original)\n    \n    # 2. \u7ed8\u5236\u539f\u59cb\u6570\u636e\u3001\u6807\u51c6\u5316\u6570\u636e\u3001\u5f52\u4e00\u5316\u6570\u636e\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 3, 1)\n    plt.scatter(original[:, 0], original[:, 1], c='r', label='Original Data')\n    plt.title('Original Data')\n    plt.legend()\n    \n    plt.subplot(1, 3, 2)\n    plt.scatter(standardized[:, 0], standardized[:, 1], c='g', label='Standardized Data')\n    plt.title('Standardized Data')\n    plt.legend()\n    \n    plt.subplot(1, 3, 3)\n    plt.scatter(normalized[:, 0], normalized[:, 1], c='b', label='Normalized Data')\n    plt.title('Normalized Data')\n    plt.legend()\n    \n    plt.show()\n    \n    return standardized, normalized\n\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    \n    \"\"\"\n    # \u8ba1\u7b97\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\n    n = len(data)\n    yf = fftpack.fft(data)\n    xf = np.linspace(0, sample_rate / 2, n // 2)\n    \n    # \u8ba1\u7b97\u529f\u7387\u8c31\u5bc6\u5ea6\n    psd = np.abs(yf[:n // 2]) ** 2 / sample_rate\n    \n    # \u7ed8\u5236\u9891\u8c31\u56fe\n    plt.figure(figsize=(10, 5))\n    plt.plot(xf, psd)\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Power Spectral Density')\n    plt.title('Power Spectrum Density of the Signal')\n    plt.grid(True)\n    plt.show()\n    \n    return xf, psd\n\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    pass\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    \n    \"\"\"\n    # Create a SMTP session\n    server = smtplib.SMTP(smtp_server, smtp_port)\n    server.starttls()\n    \n    # Login to the SMTP server\n    server.login(smtp_username, smtp_password)\n    \n    # Create a message\n    msg = MIMEText(\"This is a test email sent from a Python script.\")\n    msg['Subject'] = 'Test Email'\n    msg['From'] = smtp_username\n    msg['To'] = smtp_username\n    \n    # Send the email\n    server.sendmail(smtp_username, smtp_username, msg.as_string())\n    \n    # Close the SMTP session\n    server.quit()\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the directory exists\n    if not os.path.exists(directory):\n        return \"Directory does not exist\"\n    \n    # Check if the file exists in the directory\n    filepath = os.path.join(directory, filename)\n    if not os.path.isfile(filepath):\n        return \"File does not exist in the directory\"\n    \n    # Read the file and count the occurrences of each word\n    with open(filepath, 'r') as f:\n        text = f.read().lower()\n        words = text.split()\n        word_counts = Counter(words)\n    \n    # Save the word counts to a JSON file\n    json_filename = filename.split('.')[0] + '.json'\n    json_filepath = os.path.join(directory, json_filename)\n    with open(json_filepath, 'w') as f:\n        json.dump(word_counts, f)\n    \n    return f\"Word counts saved to {json_filename}\"\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    \n    \"\"\"\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Set 'Date' as the index\n    df.set_index('Date', inplace=True)\n    \n    # Resample the data to daily frequency and fill missing values with the last known value\n    df_resampled = df.resample('D').ffill()\n    \n    # Calculate the rolling mean and standard deviation\n    df_resampled['Rolling Mean'] = df_resampled['Value'].rolling(window=7).mean()\n    df_resampled['Rolling Std'] = df_resampled['Value'].rolling(window=7).std()\n    \n    # Plot the original data and the rolling mean\n    if plot:\n        plt.figure(figsize=(12, 6))\n        plt.plot(df_resampled['Value'], label='Original Data')\n        plt.plot(df_resampled['Rolling Mean'], label='Rolling Mean (7 days)')\n        plt.fill_between(df_resampled.index, df_resampled['Rolling Mean'] - df_resampled['Rolling Std'], \n                         df_resampled['Rolling Mean'] + df_resampled['Rolling Std'], color='gray', alpha=0.2, label='Rolling Std')\n        plt.xlabel('Date')\n        plt.ylabel('Value')\n        plt.title('Original Data and Rolling Mean with Standard Deviation')\n        plt.legend()\n        plt.show()\n    \n    return df_resampled\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    \n    \"\"\"\n    # Generate report data\n    report_data = {field: [random.randint(50, 100) for _ in range(100)] for field in FIELDS + additional_fields}\n    report_data['Student'] = STUDENTS\n    report_data['Total'] = [sum(row) for row in zip(*report_data.values())]\n    report_data['Average'] = [mean(row) for row in zip(*report_data.values())]\n    report_data = pd.DataFrame(report_data)\n    \n    # Sort the report data by total marks in descending order\n    report_data = report_data.sort_values(by='Total', ascending=False)\n    \n    # Reset the index of the report data\n    report_data = report_data.reset_index(drop=True)\n    \n    return report_data\n"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random data\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f'Person_{random.randint(1, 1000)}'\n        age = random.randint(18, 60)\n        height = round(random.uniform(150, 200), 2)\n        weight = round(random.uniform(50, 100), 2)\n        data.append([name, age, height, weight])\n\n    # Write data to CSV file\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    # Calculate average age, height, and weight\n    ages = [row[1] for row in data]\n    heights = [row[2] for row in data]\n    weights = [row[3] for row in data]\n\n    avg_age = mean(ages)\n    avg_height = mean(heights)\n    avg_weight = mean(weights)\n\n    return avg_age, avg_height, avg_weight\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef task_func(directory):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u5f53\u524d\u65f6\u95f4\n    now = datetime.now()\n    # \u683c\u5f0f\u5316\u5f53\u524d\u65f6\u95f4\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    # \u83b7\u53d6\u5f53\u524d\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\n    files = os.listdir(directory)\n    # \u904d\u5386\u6240\u6709\u6587\u4ef6\n    for file in files:\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u5b8c\u6574\u8def\u5f84\n        file_path = os.path.join(directory, file)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u521b\u5efa\u65f6\u95f4\n        create_time = os.path.getctime(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u521b\u5efa\u65f6\u95f4\u5b57\u7b26\u4e32\n        create_time_str = datetime.fromtimestamp(create_time).strftime(\"%Y-%m-%d %H:%M:%S\")\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u4fee\u6539\u65f6\u95f4\n        modify_time = os.path.getmtime(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u4fee\u6539\u65f6\u95f4\u5b57\u7b26\u4e32\n        modify_time_str = datetime.fromtimestamp(modify_time).strftime(\"%Y-%m-%d %H:%M:%S\")\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u5927\u5c0f\n        file_size = os.path.getsize(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u6269\u5c55\u540d\n        file_ext = os.path.splitext(file_path)[1]\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u540d\u79f0\n        file_name = os.path.basename(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u76ee\u5f55\n        file_dir = os.path.dirname(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u5b8c\u6574\u8def\u5f84\n        file_full_path = os.path.abspath(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n        file_abs_path = os.path.realpath(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\n        file_rel_path = os.path.relpath(file_path, directory)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\n        file_abs_path = os.path.abspath(file_path)\n        # \u83b7\u53d6\u6587\u4ef6\u7684\u76f8\u5bf9\u8def\u5f84\n        file_rel_path = os.path"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\n\ndef task_func(file_list):\n    \"\"\"\n    \n    \"\"\"\n    for file in file_list:\n        print(f\"Processing file: {file}\")\n        # Perform some processing on the file\n        time.sleep(1)  # Simulate some processing time\n\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of all files in the directory\n    file_list = glob.glob(os.path.join(directory_path, '*'))\n    \n    # Iterate over the file list\n    for file_path in file_list:\n        # Check if the file is a text file\n        if file_path.endswith('.txt'):\n            # Open the file and read its contents\n            with open(file_path, 'r') as file:\n                file_contents = file.read()\n                \n            # Check if the file contains the word \"hello\"\n            if \"hello\" in file_contents:\n                print(f\"Found 'hello' in {file_path}\")\n                \n            # Check if the file contains the word \"world\"\n            if \"world\" in file_contents:\n                print(f\"Found 'world' in {file_path}\")\n"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    \n    \"\"\"\n    # Create a boxplot for the specified column\n    sns.boxplot(x=col, data=df)\n    plt.title(f'Boxplot of {col}')\n    plt.show()\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    \n    \"\"\"\n    if wait:\n        subprocess.call([sys.executable, script_path] + list(args))\n    else:\n        subprocess.Popen([sys.executable, script_path] + list(args))\n\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    \n    \"\"\"\n    # Load the excel file\n    data = pd.read_excel(file_location, sheet_name=sheet_name)\n    \n    # Calculate the mean of each column\n    mean_values = data.mean()\n    \n    # Calculate the standard deviation of each column\n    std_dev_values = data.std()\n    \n    # Calculate the skewness of each column\n    skewness_values = data.skew()\n    \n    # Calculate the kurtosis of each column\n    kurtosis_values = data.kurtosis()\n    \n    # Create a dictionary to store the results\n    results = {\n        'Mean': mean_values,\n        'Standard Deviation': std_dev_values,\n        'Skewness': skewness_values,\n        'Kurtosis': kurtosis_values\n    }\n    \n    # Create a DataFrame from the results dictionary\n    results_df = pd.DataFrame(results)\n    \n    # Print the results\n    print(results_df)\n    \n    # Save the results to a CSV file\n    results_df.to_csv('results.csv', index=True)\n    \n    # Plot the results\n    results_df.plot(kind='bar', figsize=(10, 5))\n    plt.title('Statistical Analysis of the Data')\n    plt.xlabel('Statistics')\n    plt.ylabel('Values')\n    plt.show()\n    \n    # Save the plot to a PNG file\n    plt.savefig('results.png')\n    \n    # Return the results DataFrame\n    return results_df\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef task_func(activities):\n    \"\"\"\n    \n    \"\"\"\n    activity_counts = defaultdict(int)\n    for activity in activities:\n        activity_counts[activity] += 1\n    return activity_counts\n\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    \n    \"\"\"\n    random.seed(seed)\n    \n    # \u83b7\u53d6\u6e90\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\n    files = os.listdir(src_dir)\n    \n    # \u968f\u673a\u9009\u62e910%\u7684\u6587\u4ef6\n    selected_files = random.sample(files, int(len(files) * 0.1))\n    \n    # \u521b\u5efa\u76ee\u6807\u76ee\u5f55\n    os.makedirs(dest_dir, exist_ok=True)\n    \n    # \u5c06\u9009\u4e2d\u7684\u6587\u4ef6\u590d\u5236\u5230\u76ee\u6807\u76ee\u5f55\n    for file in selected_files:\n        src_file_path = os.path.join(src_dir, file)\n        dest_file_path = os.path.join(dest_dir, file)\n        shutil.copy(src_file_path, dest_file_path)\n    \n    return f\"\u6210\u529f\u4ece{src_dir}\u590d\u5236\u4e86{len(selected_files)}\u4e2a\u6587\u4ef6\u5230{dest_dir}\"\n\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u6307\u5b9a\u76ee\u5f55\u4e0b\u6240\u6709xlsx\u6587\u4ef6\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # \u904d\u5386\u6240\u6709xlsx\u6587\u4ef6\n    for file in xlsx_files:\n        # \u52a0\u8f7dxlsx\u6587\u4ef6\n        wb = load_workbook(file)\n        \n        # \u904d\u5386\u6240\u6709\u5de5\u4f5c\u8868\n        for sheet in wb.sheetnames:\n            # \u83b7\u53d6\u5de5\u4f5c\u8868\n            ws = wb[sheet]\n            \n            # \u904d\u5386\u6240\u6709\u5355\u5143\u683c\n            for row in ws.iter_rows():\n                for cell in row:\n                    # \u68c0\u67e5\u5355\u5143\u683c\u662f\u5426\u5305\u542b\u6570\u5b57\n                    if re.search(r'\\d', str(cell.value)):\n                        # \u6253\u5370\u5305\u542b\u6570\u5b57\u7684\u5355\u5143\u683c\u4f4d\u7f6e\u548c\u503c\n                        print(f'File: {file}, Sheet: {sheet}, Row: {row.row}, Column: {cell.column}, Value: {cell.value}')\n\n"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    \n    \"\"\"\n    # Generate random samples from a normal distribution\n    samples = np.random.normal(0, 1, sample_size)\n    \n    # Generate time points\n    time_points = np.linspace(0, 1, sample_size)\n    \n    # Compute the Fourier transform of the samples\n    fourier_transform = np.fft.fft(samples)\n    \n    # Compute the frequency axis\n    frequency_axis = np.fft.fftfreq(sample_size, 1/sample_size)\n    \n    # Compute the magnitude spectrum\n    magnitude_spectrum = np.abs(fourier_transform)\n    \n    # Compute the phase spectrum\n    phase_spectrum = np.angle(fourier_transform)\n    \n    # Plot the magnitude spectrum\n    plt.figure(figsize=(10, 5))\n    plt.plot(frequency_axis, magnitude_spectrum)\n    plt.xlabel('Frequency')\n    plt.ylabel('Magnitude')\n    plt.title('Magnitude Spectrum')\n    plt.grid(True)\n    plt.show()\n    \n    # Plot the phase spectrum\n    plt.figure(figsize=(10, 5))\n    plt.plot(frequency_axis, phase_spectrum)\n    plt.xlabel('Frequency')\n    plt.ylabel('Phase')\n    plt.title('Phase Spectrum')\n    plt.grid(True)\n    plt.show()\n    \n    # Compute the power spectrum\n    power_spectrum = magnitude_spectrum**2\n    \n    # Plot the power spectrum\n    plt.figure(figsize=(10, 5))\n    plt.plot(frequency_axis, power_spectrum)\n    plt.xlabel('Frequency')\n    plt.ylabel('Power')\n    plt.title('Power Spectrum')\n    plt.grid(True)\n    plt"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    \n    \"\"\"\n    app = Flask(app_name)\n    app.config['MAIL_SERVER'] = 'smtp.gmail.com'\n    app.config['MAIL_PORT'] = 587\n    app.config['MAIL_USE_TLS'] = True\n    app.config['MAIL_USERNAME'] = os.environ.get('MAIL_USERNAME')\n    app.config['MAIL_PASSWORD'] = os.environ.get('MAIL_PASSWORD')\n    mail = Mail(app)\n    return mail\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    \n    \"\"\"\n    # Read the Excel file\n    df = pd.read_excel(excel_file_path)\n    \n    # Check if the column exists in the DataFrame\n    if column_name in df.columns:\n        # Calculate the sum of the specified column\n        column_sum = df[column_name].sum()\n        \n        # Create a new DataFrame with the sum\n        result_df = pd.DataFrame({column_name: [column_sum]})\n        \n        # Save the result to a new Excel file\n        result_file_path = os.path.join(os.path.dirname(excel_file_path), f\"{file_name}_sum.xlsx\")\n        result_df.to_excel(result_file_path, index=False)\n        \n        print(f\"Sum of {column_name} has been calculated and saved to {result_file_path}.\")\n    else:\n        print(f\"Column '{column_name}' does not exist in the Excel file.\")\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Create a sequential model\n    model = Sequential()\n\n    # Add a dense layer with 64 units and ReLU activation\n    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n\n    # Add a dense layer with 1 unit and sigmoid activation\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Compile the model with SGD optimizer and binary cross-entropy loss\n    model.compile(optimizer=SGD(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n    print('Test accuracy:', accuracy)\n\n    # Plot the training and testing loss\n    plt.plot(model.history.history['loss'])\n    plt.plot(model.history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.show()\n\n    # Plot the training and testing accuracy\n    plt.plot(model.history.history['accuracy'])\n    plt.plot(model.history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accur"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into training and testing sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\n    # Define the model\n    model = keras.Sequential([\n        keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Train the model\n    model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n    # Evaluate the model\n    Y_pred = model.predict(X_test)\n\n    # Compute the ROC curve and the AUC\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Character"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    \n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path)\n    \n    # Convert the image to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Reshape the image to a 2D array of pixels\n    image_reshaped = image_rgb.reshape((image_rgb.shape[0] * image_rgb.shape[1], 3))\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(image_reshaped)\n    \n    # Get the cluster labels for each pixel\n    labels = kmeans.predict(image_reshaped)\n    \n    # Create a new image with the same dimensions as the original\n    new_image = np.zeros((image_rgb.shape[0], image_rgb.shape[1], 3), dtype=np.uint8)\n    \n    # Assign the color of the centroid to each pixel\n    for i in range(image_rgb.shape[0]):\n        for j in range(image_rgb.shape[1]):\n            new_image[i, j] = kmeans.cluster_centers_[labels[i * image_rgb.shape[1] + j]]\n    \n    # Save the new image\n    cv2.imwrite('new_image.jpg', cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR))\n"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    kmeans.fit(P)\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    # Plotting\n    fig, ax = plt.subplots()\n    ax.scatter(P[:, 0], P[:, 1], c=labels, cmap='viridis')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200)\n    ax.set_title('KMeans Clustering')\n    ax.set_xlabel('P[0]')\n    ax.set_ylabel('P[1]')\n\n    return labels, ax\n\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\n\ndef task_func(points, seed=0):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    vor = Voronoi(points)\n    voronoi_plot_2d(vor)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(dest_dir):\n        os.makedirs(dest_dir)\n\n    for file in glob.glob(os.path.join(src_dir, '*.' + ext)):\n        shutil.copy(file, dest_dir)\n\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\n\ndef task_func(json_str):\n    \"\"\"\n    \n    \"\"\"\n    # \u5c06json\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u5b57\u5178\n    json_dict = json.loads(json_str)\n    # \u83b7\u53d6\u5b57\u5178\u4e2d\u7684'data'\u952e\u5bf9\u5e94\u7684\u503c\n    data = json_dict['data']\n    # \u83b7\u53d6'data'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'items'\u952e\u5bf9\u5e94\u7684\u503c\n    items = data['items']\n    # \u521b\u5efa\u4e00\u4e2a\u7a7a\u5217\u8868\uff0c\u7528\u4e8e\u5b58\u50a8\u5904\u7406\u540e\u7684\u6570\u636e\n    result = []\n    # \u904d\u5386'items'\u952e\u5bf9\u5e94\u7684\u503c\n    for item in items:\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\n        item_data = item['item']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'title'\u952e\u5bf9\u5e94\u7684\u503c\n        title = item_data['title']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'description'\u952e\u5bf9\u5e94\u7684\u503c\n        description = item_data['description']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'price'\u952e\u5bf9\u5e94\u7684\u503c\n        price = item_data['price']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'image'\u952e\u5bf9\u5e94\u7684\u503c\n        image = item_data['image']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'link'\u952e\u5bf9\u5e94\u7684\u503c\n        link = item_data['link']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'shop'\u952e\u5bf9\u5e94\u7684\u503c\n        shop = item_data['shop']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'shop_link'\u952e\u5bf9\u5e94\u7684\u503c\n        shop_link = item_data['shop_link']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'sales'\u952e\u5bf9\u5e94\u7684\u503c\n        sales = item_data['sales']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'comment_num'\u952e\u5bf9\u5e94\u7684\u503c\n        comment_num = item_data['comment_num']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'location'\u952e\u5bf9\u5e94\u7684\u503c\n        location = item_data['location']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'cate'\u952e\u5bf9\u5e94\u7684\u503c\n        cate = item_data['cate']\n        # \u83b7\u53d6'item'\u952e\u5bf9\u5e94\u7684\u503c\u4e2d\u7684'cate_link'\u952e\u5bf9\u5e94\u7684\u503c\n        cate_"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    start_time = time.time()\n    process = subprocess.Popen(script_path, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    try:\n        stdout, stderr = process.communicate(timeout=timeout)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        return {\n            \"status\": \"success\",\n            \"stdout\": stdout.decode(\"utf-8\"),\n            \"stderr\": stderr.decode(\"utf-8\"),\n            \"elapsed_time\": elapsed_time\n        }\n    except subprocess.TimeoutExpired:\n        process.kill()\n        return {\n            \"status\": \"timeout\",\n            \"stdout\": \"\",\n            \"stderr\": \"Task timed out after {} seconds\".format(timeout),\n            \"elapsed_time\": timeout\n        }\n    except Exception as e:\n        return {\n            \"status\": \"error\",\n            \"stdout\": \"\",\n            \"stderr\": str(e),\n            \"elapsed_time\": 0\n        }\n\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    data = np.random.rand(N, len(CATEGORIES))\n    data = data / data.sum(axis=1, keepdims=True)\n    df = pd.DataFrame(data, columns=CATEGORIES)\n    return df\n\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(seed)\n    \n    # Generate time series data\n    time_series = pd.date_range(start=start_time, end=end_time, freq=step)\n    \n    # Generate random values\n    values = np.random.randn(len(time_series))\n    \n    # Apply trend\n    if trend == 'linear':\n        trend_values = np.linspace(0, 1, len(time_series))\n        values += trend_values\n    elif trend == 'quadratic':\n        trend_values = np.linspace(0, 1, len(time_series)) ** 2\n        values += trend_values\n    elif trend == 'exponential':\n        trend_values = np.exp(np.linspace(0, 1, len(time_series)))\n        values += trend_values\n    \n    # Create DataFrame\n    df = pd.DataFrame({'time': time_series, 'value': values})\n    \n    return df\n\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    \n    \"\"\"\n    random.seed(random_seed)\n    \n    # Convert epoch milliseconds to datetime\n    dt = datetime.fromtimestamp(epoch_milliseconds / 1000)\n    \n    # Generate random sales data\n    sales_data = {\n        \"product\": random.choice(products),\n        \"quantity\": random.randint(1, 10),\n        \"price\": round(random.uniform(10, 100), 2),\n        \"sale_date\": dt,\n    }\n    \n    return sales_data\n\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    \n    \"\"\"\n    # \u5c06json\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3apandas DataFrame\n    df = pd.read_json(json_str)\n    \n    # \u521b\u5efa\u4e00\u4e2a\u65b0\u7684ExcelWriter\u5bf9\u8c61\n    writer = pd.ExcelWriter(filename, engine='xlwt')\n    \n    # \u5c06DataFrame\u5199\u5165Excel\u6587\u4ef6\n    df.to_excel(writer, sheet_name=sheet_name, index=False)\n    \n    # \u4fdd\u5b58Excel\u6587\u4ef6\n    writer.save()\n    \n    return filename\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    \n    \"\"\"\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Generate a list of dates for the last week\n    dates = [datetime.now() - timedelta(days=i) for i in range(days_in_past)]\n    \n    # Generate a list of random temperatures for each date\n    temperatures = [random.uniform(20, 30) for _ in range(days_in_past)]\n    \n    # Create a pandas DataFrame with the dates and temperatures\n    df = pd.DataFrame({'date': dates, 'temperature': temperatures})\n    \n    # Convert the date column to a string format\n    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n    \n    # Plot the temperature over time\n    sns.lineplot(x='date', y='temperature', data=df)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the DataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(random_seed)\n    stock_data = {}\n    for stock in stock_names:\n        stock_data[stock] = np.random.randint(100, 500, days_in_past)\n    df = pd.DataFrame(stock_data)\n    df.index = pd.date_range(start=datetime.now(), periods=days_in_past, freq=\"D\")\n    return df\n\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV files\n    df1 = pd.read_csv(file_path1, delimiter=delimiter, quotechar=quotechar)\n    df2 = pd.read_csv(file_path2, delimiter=delimiter, quotechar=quotechar)\n\n    # Find the columns that are in both dataframes\n    common_columns = list(set(df1.columns) & set(df2.columns))\n\n    # Find the rows that are in both dataframes\n    common_rows = df1.merge(df2, on=common_columns, how='inner')\n\n    # Find the rows that are in df1 but not in df2\n    unique_rows_df1 = df1[~df1.merge(df2, on=common_columns, how='left', indicator=True)['_merge'] == 'left_only']\n\n    # Find the rows that are in df2 but not in df1\n    unique_rows_df2 = df2[~df2.merge(df1, on=common_columns, how='left', indicator=True)['_merge'] == 'left_only']\n\n    # Find the rows that are in both dataframes but have different values\n    diff_rows = []\n    for index, row in common_rows.iterrows():\n        diff = ndiff(row.values, df2.loc[df2[common_columns].apply(tuple, axis=1) == tuple(row[common_columns]), :].values[0])\n        if any(d.startswith('-') or d.startswith('+') for d in diff):\n            diff_rows.append(row)\n\n    # Print the results\n    print(\"Common rows:\")\n    print(common_rows)\n    print(\"\\nUnique rows in df1:\")\n    print(unique_rows_df1)\n    print(\"\\nUnique rows in df2:\")\n    print(unique_rows_df2)\n    print(\"\\nRows"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # Your code here\n    pass\n\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    \n    \"\"\"\n    # Your code here\n    pass\n\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    \n    \"\"\"\n    with open(input_file, \"r\") as f:\n        data = json.load(f)\n\n    # Create a dictionary to store the results\n    results = defaultdict(list)\n\n    # Iterate over the data\n    for item in data:\n        # Extract the relevant information\n        name = item[\"name\"]\n        age = item[\"age\"]\n        gender = item[\"gender\"]\n        height = item[\"height\"]\n        weight = item[\"weight\"]\n\n        # Add the information to the results dictionary\n        results[\"name\"].append(name)\n        results[\"age\"].append(age)\n        results[\"gender\"].append(gender)\n        results[\"height\"].append(height)\n        results[\"weight\"].append(weight)\n\n    # Convert the results dictionary to a pandas DataFrame\n    df = pd.DataFrame(results)\n\n    # Print the DataFrame\n    print(df)\n\n    # Calculate the average age, height, and weight\n    avg_age = df[\"age\"].mean()\n    avg_height = df[\"height\"].mean()\n    avg_weight = df[\"weight\"].mean()\n\n    # Print the averages\n    print(\"Average age:\", avg_age)\n    print(\"Average height:\", avg_height)\n    print(\"Average weight:\", avg_weight)\n\n    # Calculate the standard deviation of age, height, and weight\n    std_age = df[\"age\"].std()\n    std_height = df[\"height\"].std()\n    std_weight = df[\"weight\"].std()\n\n    # Print the standard deviations\n    print(\"Standard deviation of age:\", std_age)\n    print(\"Standard deviation of height:\", std_height)\n    print(\"Standard deviation of weight:\", std_weight)\n\n    # Calculate the median age, height, and weight\n    median_age = df[\"age\"].median()\n    median_height = df[\"height\"].median()\n    median_weight = df[\"weight\"].median()\n\n    # Print the medians"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    data = pd.read_csv(file_path)\n    \n    # Count the occurrences of each value in the 'Category' column\n    category_counts = data['Category'].value_counts()\n    \n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.bar(category_counts.index, category_counts.values)\n    plt.xlabel('Category')\n    plt.ylabel('Count')\n    plt.title('Count of Each Category in the Dataset')\n    plt.xticks(rotation=45)\n    plt.show()\n\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    \n    \"\"\"\n    # Your code here\n    pass\n\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, bins=4):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the frequency of each value in the dataframe\n    freq = df.value_counts()\n    \n    # Calculate the mean and standard deviation of the frequency\n    mean_freq = freq.mean()\n    std_freq = freq.std()\n    \n    # Calculate the z-scores for each frequency\n    z_scores = (freq - mean_freq) / std_freq\n    \n    # Calculate the bin edges for the z-scores\n    bin_edges = norm.ppf(np.linspace(0, 1, bins + 1))\n    \n    # Assign each z-score to a bin\n    binned_freq = pd.cut(z_scores, bin_edges, labels=False)\n    \n    # Calculate the frequency of each bin\n    bin_freq = binned_freq.value_counts()\n    \n    # Calculate the percentage of each bin\n    bin_percentage = bin_freq / len(freq) * 100\n    \n    return bin_percentage\n\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\n\ndef task_func(a, b):\n    \"\"\"\n    \n    \"\"\"\n    return a * b\n\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Convert the 'Date' column to datetime format\n    data['Date'] = pd.to_datetime(data['Date'])\n    \n    # Set the 'Date' column as the index\n    data.set_index('Date', inplace=True)\n    \n    # Resample the data to daily frequency and fill missing values with the previous day's value\n    data = data.resample('D').ffill()\n    \n    # Calculate the moving average of the 'Close' column with a window size of 30 days\n    data['MA30'] = data['Close'].rolling(window=30).mean()\n    \n    # Calculate the moving average of the 'Close' column with a window size of 100 days\n    data['MA100'] = data['Close'].rolling(window=100).mean()\n    \n    # Plot the 'Close' and moving averages\n    plt.figure(figsize=(10, 5))\n    plt.plot(data['Close'], label='Close')\n    plt.plot(data['MA30'], label='MA30')\n    plt.plot(data['MA100'], label='MA100')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.title('Stock Price with Moving Averages')\n    plt.legend()\n    plt.show()\n    \n    # Calculate the percentage change in the 'Close' column\n    data['Percentage Change'] = data['Close'].pct_change()\n    \n    # Print the first 5 rows of the data\n    print(data.head())\n\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Your code here\n    pass\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    \n    \"\"\"\n    x = np.linspace(0, 10, array_length)\n    y = np.sin(x) + np.random.normal(0, noise_level, array_length)\n    \n    return x, y\n\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the CSV file\n    with open(csv_file, 'r', encoding='utf-8') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip the header row\n        data = [row[0] for row in reader]\n\n    # Normalize the text and count the frequency of each character\n    normalized_data = [unicodedata.normalize('NFKD', char).encode('ASCII', 'ignore').decode('ASCII') for char in data]\n    char_frequency = Counter(normalized_data)\n\n    # Plot the frequency distribution\n    plt.figure(figsize=(10, 6))\n    plt.bar(char_frequency.keys(), char_frequency.values())\n    plt.xlabel('Characters')\n    plt.ylabel('Frequency')\n    plt.title('Frequency Distribution of Characters in the CSV File')\n    plt.show()\n\n    # Return the most common character\n    most_common_char = char_frequency.most_common(1)[0][0]\n    return most_common_char\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef task_func(size=1000):\n    \"\"\"\n    \n    \"\"\"\n    np.random.seed(0)\n    x = np.random.randn(size)\n    y = np.random.randn(size)\n    return x, y\n\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # Generate RSA keys\n    (pubkey, privkey) = rsa.newkeys(1024)\n\n    # Generate AES key\n    aes_key = get_random_bytes(16)\n\n    # Encrypt AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pubkey)\n\n    # Encrypt message with AES key\n    message = \"Hello, World!\"\n    cipher = AES.new(aes_key, AES.MODE_EAX)\n    ciphertext, tag = cipher.encrypt_and_digest(message.encode())\n\n    # Combine encrypted AES key, ciphertext, and tag into a single message\n    encrypted_message = b64encode(encrypted_aes_key + cipher.nonce + tag + ciphertext)\n\n    # Decrypt message with RSA private key\n    decrypted_aes_key = rsa.decrypt(encrypted_aes_key, privkey)\n\n    # Decrypt ciphertext with AES key\n    cipher = AES.new(decrypted_aes_key, AES.MODE_EAX, nonce=cipher.nonce)\n    decrypted_message = cipher.decrypt_and_verify(ciphertext, tag).decode()\n\n    print(\"Original message:\", message)\n    print(\"Encrypted message:\", encrypted_message.decode())\n    print(\"Decrypted message:\", decrypted_message)\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Generate RSA key pair\n    (pubkey, privkey) = rsa.newkeys(2048)\n\n    # Read the file\n    with open(file_path, 'rb') as f:\n        data = f.read()\n\n    # Encrypt the data using RSA\n    encrypted_data = rsa.encrypt(data, pubkey)\n\n    # Generate a random 16-byte key for AES\n    key = os.urandom(16)\n\n    # Generate a random 12-byte nonce for AES\n    nonce = os.urandom(12)\n\n    # Create a cipher object\n    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce), backend=default_backend())\n\n    # Encrypt the data using AES\n    ciphertext, tag = cipher.encryptor().update(encrypted_data) + cipher.encryptor().finalize()\n\n    # Create a dictionary with the encrypted data, key, nonce, and tag\n    encrypted_dict = {\n        'encrypted_data': b64encode(encrypted_data).decode('utf-8'),\n        'key': b64encode(key).decode('utf-8'),\n        'nonce': b64encode(nonce).decode('utf-8'),\n        'tag': b64encode(tag).decode('utf-8')\n    }\n\n    return encrypted_dict\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n    response = urllib.request.urlopen(url)\n    html = response.read().decode('utf-8')\n    \n    # \u4f7f\u7528PyQuery\u89e3\u6790HTML\n    doc = pq(html)\n    \n    # \u63d0\u53d6\u6570\u636e\n    data = []\n    for item in doc('div.item').items():\n        title = item('h2').text()\n        link = item('h2 a').attr('href')\n        publish_time = item('div.meta span.time').text()\n        publish_time = datetime.strptime(publish_time, '%Y-%m-%d %H:%M:%S')\n        data.append({'title': title, 'link': link, 'publish_time': publish_time})\n    \n    # \u5c06\u6570\u636e\u8f6c\u6362\u4e3aDataFrame\n    df = pd.DataFrame(data)\n    \n    # \u4fdd\u5b58\u6570\u636e\u5230CSV\u6587\u4ef6\n    df.to_csv('news.csv', index=False)\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data for each sensor\n    for sensor in SENSORS:\n        # Create a CSV file for the sensor\n        filename = os.path.join(output_dir, f'{sensor}.csv')\n        with open(filename, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Timestamp', 'Value'])\n\n            # Generate data for the last 'hours'\n            for i in range(hours):\n                # Generate a random value for the sensor\n                value = randint(0, 100) if sensor == 'Humidity' else randint(1000, 5000)\n\n                # Generate a timestamp for the current hour\n                timestamp = datetime.now().replace(minute=0, second=0, microsecond=0) - timedelta(hours=hours-i-1)\n\n                # Write the data to the CSV file\n                writer.writerow([timestamp.strftime('%Y-%m-%d %H:%M:%S'), value])\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate data\n    data = []\n    for hour in range(hours):\n        for vehicle_type in VEHICLE_TYPES:\n            data.append({\n                'hour': hour,\n                'vehicle_type': vehicle_type,\n                'count': randint(0, 100)\n            })\n\n    # Write data to CSV file\n    csv_file = os.path.join(output_dir, 'traffic.csv')\n    with open(csv_file, 'w', newline='') as f:\n        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    # Plot data\n    df = pd.read_csv(csv_file)\n    df.pivot_table(index='hour', columns='vehicle_type', values='count').plot(kind='bar', stacked=True)\n    plt.title('Traffic by Vehicle Type and Hour')\n    plt.xlabel('Hour')\n    plt.ylabel('Count')\n    plt.savefig(os.path.join(output_dir, 'traffic.png'))\n\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    \n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    weather_data = []\n    for i in range(hours):\n        weather_data.append({\n            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n            'temperature': randint(20, 35),\n            'humidity': randint(30, 60),\n            'condition': WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        })\n\n    # Write weather data to CSV file\n    with open(os.path.join(output_dir, 'weather_data.csv'), 'w', newline='') as csvfile:\n        fieldnames = ['timestamp', 'temperature', 'humidity', 'condition']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        for data in weather_data:\n            writer.writerow(data)\n\n    # Copy weather data to another directory\n    shutil.copy(os.path.join(output_dir, 'weather_data.csv'), os.path.join(output_dir, 'backup', 'weather_data_backup.csv'))\n\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    \n    \"\"\"\n    return goals * 1000 - penalties * PENALTY_COST\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\n\ndef task_func(rows):\n    \"\"\"\n    \n    \"\"\"\n    return rows.sum()\n\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\ndef task_func(num_students):\n    \"\"\"\n    \n    \"\"\"\n    # Generate a list of student IDs\n    student_ids = list(range(1, num_students + 1))\n    \n    # Generate a list of grades for each student\n    grades = [sample(range(60, 101), 5) for _ in range(num_students)]\n    \n    # Create a DataFrame to store the student IDs and grades\n    df = pd.DataFrame({'Student ID': student_ids, 'Grades': grades})\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df['Grades'].apply(np.mean)\n    \n    # Calculate the overall average grade\n    overall_average = df['Average Grade'].mean()\n    \n    # Print the DataFrame and the overall average grade\n    print(df)\n    print(f\"Overall Average Grade: {overall_average:.2f}\")\n    \n    # Plot the average grades\n    plt.bar(df['Student ID'], df['Average Grade'])\n    plt.xlabel('Student ID')\n    plt.ylabel('Average Grade')\n    plt.title('Average Grades by Student')\n    plt.show()\n    \n    return df, overall_average\n\n\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\n\ndef task_func(array, target_value):\n    \"\"\"\n    \n    \"\"\"\n    return np.sum(np.abs(array - target_value))\n\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    \n    \"\"\"\n    # Preprocess the texts\n    texts = [ALPHANUMERIC.sub(' ', text).lower() for text in texts]\n    texts = [' '.join([word for word in text.split() if word not in STOPWORDS]) for text in texts]\n\n    # Vectorize the texts\n    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n    tfidf = vectorizer.fit_transform(texts)\n\n    # Apply NMF\n    nmf = NMF(n_components=num_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    # Print the topics\n    for topic_idx, topic in enumerate(nmf.components_):\n        print(f\"Topic #{topic_idx + 1}:\")\n        print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n        print()\n\n    return nmf, feature_names\n\n"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    \n    \"\"\"\n    # Tokenize the texts\n    tokenized_texts = [nltk.word_tokenize(text) for text in texts]\n    \n    # Remove stopwords\n    if stopwords:\n        tokenized_texts = [[word for word in text if word not in stopwords] for text in tokenized_texts]\n    \n    # Remove non-alphanumeric characters\n    tokenized_texts = [[ALPHANUMERIC.sub('', word) for word in text] for text in tokenized_texts]\n    \n    # Remove empty strings\n    tokenized_texts = [[word for word in text if word] for text in tokenized_texts]\n    \n    # Convert to lowercase\n    tokenized_texts = [[word.lower() for word in text] for text in tokenized_texts]\n    \n    # Train Word2Vec model\n    model = Word2Vec(tokenized_texts, min_count=1)\n    \n    return model\n\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    \n    \"\"\"\n    # \u8bfb\u53d6json\u6587\u4ef6\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # \u521b\u5efa\u4e00\u4e2a\u7a7a\u7684DataFrame\n    df = pd.DataFrame()\n    \n    # \u904d\u5386json\u6570\u636e\uff0c\u5c06\u6570\u636e\u6dfb\u52a0\u5230DataFrame\u4e2d\n    for item in data:\n        df = df.append(item, ignore_index=True)\n    \n    # \u8fd4\u56deDataFrame\n    return df\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u5f53\u524d\u65f6\u95f4\n    now = datetime.now()\n    # \u683c\u5f0f\u5316\u5f53\u524d\u65f6\u95f4\n    formatted_now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    # \u6253\u5370\u5f53\u524d\u65f6\u95f4\n    print(formatted_now)\n\n    # \u83b7\u53d6\u5f53\u524d\u5de5\u4f5c\u76ee\u5f55\n    current_dir = os.getcwd()\n    # \u6253\u5370\u5f53\u524d\u5de5\u4f5c\u76ee\u5f55\n    print(current_dir)\n\n    # \u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6\uff0c\u5e76\u5199\u5165\u5f53\u524d\u65f6\u95f4\n    with open('time.txt', 'w') as f:\n        f.write(formatted_now)\n\n    # \u6253\u5f00\u5e76\u8bfb\u53d6csv\u6587\u4ef6\n    with open('data.csv', 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            print(row)\n\n    # \u751f\u6210\u4e00\u4e2a\u968f\u673a\u6570\n    random_num = random.randint(1, 100)\n    # \u6253\u5370\u968f\u673a\u6570\n    print(random_num)\n\n    # \u521b\u5efa\u4e00\u4e2a\u65b0\u6587\u4ef6\uff0c\u5e76\u5199\u5165\u968f\u673a\u6570\n    with open('random.txt', 'w') as f:\n        f.write(str(random_num))\n\n    # \u8fd4\u56de\u4efb\u52a1\u7ed3\u679c\n    return \"Task completed successfully\"\n"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the CSV file already exists\n    if os.path.exists(CSV_FILE_PATH):\n        # If it does, append to the existing file\n        with open(CSV_FILE_PATH, 'a', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            # Scrape the data\n            scraped_data = scrape_data(url)\n            # Write the scraped data to the CSV file\n            writer.writerow(scraped_data)\n    else:\n        # If it doesn't, create a new file and write the scraped data to it\n        with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            # Write the header row\n            writer.writerow(['Header 1', 'Header 2', 'Header 3'])\n            # Scrape the data\n            scraped_data = scrape_data(url)\n            # Write the scraped data to the CSV file\n            writer.writerow(scraped_data)\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    \"\"\"\n    \n    \"\"\"\n    # Split the data into features and target\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on the testing set\n    y_pred = model.predict(X_test)\n\n    # Calculate the mean squared error\n    mse = np.mean((y_test - y_pred) ** 2)\n\n    return mse\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \"\"\"\n    \n    \"\"\"\n    \n    # Set the random seed for reproducibility\n    np.random.seed(rng_seed)\n    \n    # Create a list of dates between the start and end years\n    dates = pd.date_range(start=f'{start_year}-01-01', end=f'{end_year}-12-31')\n    \n    # Create a list of email addresses using the domain and latin names\n    emails = [f'{name.lower().replace(\" \", \"_\")}@{email_domain}' for name in latin_names]\n    \n    # Create a list of phone numbers using a regular expression\n    phone_numbers = [re.sub(r'\\D', '', np.random.choice(['555-555-5555', '555-555-5556', '555-555-5557'])) for _ in range(len(latin_names))]\n    \n    # Create a list of addresses using a regular expression\n    addresses = [re.sub(r'\\D', '', np.random.choice(['123 Main St', '456 Elm St', '789 Pine St'])) for _ in range(len(latin_names))]\n    \n    # Create a list of cities using a regular expression\n    cities = [re.sub(r'\\D', '', np.random.choice(['New York', 'Los Angeles', 'Chicago'])) for _ in range(len(latin_names))]\n    \n    # Create a list of states using a regular expression\n    states = [re.sub(r'\\D', '', np.random.choice(['NY', 'CA', 'IL'])) for _ in range(len"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    \n    \"\"\"\n    # Read the input file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Initialize a dictionary to store the frequency of each word\n    word_freq = defaultdict(int)\n\n    # Iterate over the data and count the frequency of each word\n    for item in data:\n        for word in item['text'].split():\n            word_freq[word] += 1\n\n    # Write the word frequency to the output file\n    with open(output_file, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Word', 'Frequency'])\n        for word, freq in word_freq.items():\n            writer.writerow([word, freq])\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    \n    \"\"\"\n    # Get the source directory from kwargs\n    source_dir = kwargs.get(\"source_dir\")\n    \n    # Check if source directory exists\n    if not os.path.exists(source_dir):\n        print(f\"Source directory '{source_dir}' does not exist.\")\n        return\n    \n    # Create target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Iterate over all files in the source directory\n    for file_name in os.listdir(source_dir):\n        file_path = os.path.join(source_dir, file_name)\n        \n        # Check if it's a file and not a directory\n        if os.path.isfile(file_path):\n            # Copy the file to the target directory\n            shutil.copy(file_path, target_dir)\n            print(f\"Copied '{file_name}' to '{target_dir}'\")\n        else:\n            print(f\"Skipped '{file_name}' as it's a directory.\")\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    \"\"\"\n    \n    \"\"\"\n    \n    # Check if the directory exists\n    if not os.path.exists(directory):\n        raise ValueError(f\"The directory '{directory}' does not exist.\")\n\n    # Check if the pattern is a valid regular expression\n    try:\n        re.compile(pattern)\n    except re.error:\n        raise ValueError(f\"The pattern '{pattern}' is not a valid regular expression.\")\n\n    # Get a list of all files in the directory\n    files = os.listdir(directory)\n\n    # Filter the files using the regular expression\n    matching_files = [file for file in files if re.match(pattern, file)]\n\n    # Read the contents of each matching file and store the data in a list of dictionaries\n    data = []\n    for file in matching_files:\n        with open(os.path.join(directory, file), 'r') as f:\n            reader = csv.DictReader(f)\n            data.extend(list(reader))\n\n    return data\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u6307\u5b9a\u76ee\u5f55\u4e0b\u7684\u6240\u6709zip\u6587\u4ef6\n    zip_files = [f for f in os.listdir(directory) if f.endswith('.zip')]\n    \n    # \u904d\u5386zip\u6587\u4ef6\n    for zip_file in zip_files:\n        # \u83b7\u53d6zip\u6587\u4ef6\u540d\n        zip_file_name = os.path.join(directory, zip_file)\n        \n        # \u521b\u5efa\u4e00\u4e2aZipFile\u5bf9\u8c61\n        with zipfile.ZipFile(zip_file_name, 'r') as z:\n            # \u83b7\u53d6zip\u6587\u4ef6\u4e2d\u7684\u6240\u6709\u6587\u4ef6\n            file_list = z.namelist()\n            \n            # \u904d\u5386\u6587\u4ef6\u5217\u8868\n            for file in file_list:\n                # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u6587\u4ef6\u540d\n                match = re.match(pattern, file)\n                if match:\n                    # \u83b7\u53d6\u5339\u914d\u5230\u7684\u6587\u4ef6\u540d\n                    file_name = match.group(1)\n                    \n                    # \u521b\u5efa\u65b0\u6587\u4ef6\u540d\n                    new_file_name = f'{file_name}.txt'\n                    \n                    # \u6253\u5f00zip\u6587\u4ef6\u4e2d\u7684\u6587\u4ef6\n                    with z.open(file) as f:\n                        # \u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\n                        content = f.read().decode('utf-8')\n                        \n                        # \u5c06\u6587\u4ef6\u5185\u5bb9\u5199\u5165\u65b0\u6587\u4ef6\n                        with open(os.path.join(directory, new_file_name), 'w', encoding='utf-8') as new_f:\n                            new_f.write(content)\n                            \n                            print(f'{file_name}.txt \u5df2\u521b\u5efa')\n                            \n                    # \u5220\u9664zip\u6587\u4ef6\u4e2d\u7684\u6587\u4ef6\n                    z.extract(file, path=directory)\n                    os.remove(zip_file_name)\n                    \n                    print(f'{zip_file} \u5df2\u5904\u7406')\n                    \n                    break\n                    \n            else:\n                print(f'{zip_file} \u4e2d\u672a\u627e\u5230\u5339\u914d\u7684\u6587\u4ef6')\n                \n                # "}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    \n    \"\"\"\n    # Get the list of files matching the pattern\n    files = glob.glob(pattern)\n    \n    # Iterate over the files\n    for file in files:\n        # Get the file name\n        file_name = os.path.basename(file)\n        \n        # Create the destination path\n        dest_path = os.path.join(ARCHIVE_DIR, file_name)\n        \n        # Copy the file to the destination path\n        subprocess.run(['cp', file, dest_path])\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    \n    \"\"\"\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        data = list(reader)\n\n    # Calculate the total goals and penalties for each team\n    team_totals = Counter()\n    for row in data:\n        team_totals[row['team']] += int(row['goals']) + int(row['penalties'])\n\n    # Find the team with the highest total score\n    highest_team = max(team_totals.items(), key=lambda x: x[1])\n\n    return highest_team\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    \n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if re.match(file_pattern, file):\n                source_file = os.path.join(root, file)\n                target_file = os.path.join(target_dir, file)\n                shutil.copy2(source_file, target_file)\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    \n    \"\"\"\n    # Tokenize and remove non-alphanumeric characters\n    tokens1 = ALPHANUMERIC.sub('', text1).split()\n    tokens2 = ALPHANUMERIC.sub('', text2).split()\n    \n    # Count the frequency of each word\n    counter1 = Counter(tokens1)\n    counter2 = Counter(tokens2)\n    \n    # Calculate the Jaccard similarity\n    intersection = set(counter1.keys()) & set(counter2.keys())\n    union = set(counter1.keys()) | set(counter2.keys())\n    jaccard_similarity = len(intersection) / len(union)\n    \n    # Calculate the Levenshtein similarity\n    levenshtein_similarity = ratio(text1, text2)\n    \n    return jaccard_similarity, levenshtein_similarity\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the sum of all numbers\n    total_sum = sum(numbers)\n    \n    # Calculate the product of all numbers\n    total_product = reduce(lambda x, y: x*y, numbers)\n    \n    # Calculate the factorial of all numbers\n    total_factorial = math.factorial(total_sum)\n    \n    # Calculate the sum of the squares of all numbers\n    total_square_sum = sum([num**2 for num in numbers])\n    \n    # Calculate the sum of the cubes of all numbers\n    total_cube_sum = sum([num**3 for num in numbers])\n    \n    # Calculate the sum of the fourth powers of all numbers\n    total_fourth_power_sum = sum([num**4 for num in numbers])\n    \n    # Calculate the sum of the fifth powers of all numbers\n    total_fifth_power_sum = sum([num**5 for num in numbers])\n    \n    # Calculate the sum of the sixth powers of all numbers\n    total_sixth_power_sum = sum([num**6 for num in numbers])\n    \n    # Calculate the sum of the seventh powers of all numbers\n    total_seventh_power_sum = sum([num**7 for num in numbers])\n    \n    # Calculate the sum of the eighth powers of all numbers\n    total_eighth_power_sum = sum([num**8 for num in numbers])\n    \n    # Calculate the sum of the ninth powers of all numbers\n    total_ninth_power_sum = sum([num**9 for num in numbers])\n    \n    # Calculate the sum of the tenth powers of all numbers\n    total_tenth_power_sum = sum([num**10 for num in numbers])\n    \n    # Calculate the sum of the eleventh powers of all numbers\n    total_eleventh_power_sum = sum([num**11 for num in numbers])\n    \n    # Calculate the sum of the twelfth powers of all numbers\n    total_twelfth_power_sum"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the source directory exists\n    if not os.path.exists(SOURCE_DIR):\n        warnings.warn(f\"Source directory '{SOURCE_DIR}' does not exist.\")\n        return\n\n    # Check if the destination directory exists, if not, create it\n    if not os.path.exists(DEST_DIR):\n        os.makedirs(DEST_DIR)\n\n    # Get a list of all files in the source directory with the specified extensions\n    files = glob.glob(os.path.join(SOURCE_DIR, f\"*.{EXTENSIONS}\"))\n\n    # Copy each file to the destination directory\n    for file in files:\n        shutil.copy(file, DEST_DIR)\n\n    # Print a message indicating the completion of the task\n    print(f\"Task completed. {len(files)} files copied from '{SOURCE_DIR}' to '{DEST_DIR}'.\")\n"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef task_func(data):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the z-score for each column in the dataframe\n    z_scores = data.apply(zscore)\n    \n    # Identify columns with z-scores greater than 3 or less than -3\n    outliers = z_scores[(z_scores > 3) | (z_scores < -3)]\n    \n    # Remove outliers from the dataframe\n    cleaned_data = data.drop(outliers.index)\n    \n    # Normalize the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    normalized_data = pd.DataFrame(scaler.fit_transform(cleaned_data), columns=cleaned_data.columns)\n    \n    return normalized_data\n\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    \n    \"\"\"\n    # 1. Create a pandas DataFrame from the given list of tuples.\n    df = pd.DataFrame(data_list, columns=['alpha', 'beta', 'gamma'])\n\n    # 2. Add a new column 'delta' to the DataFrame, which is the sum of 'beta' and 'gamma'.\n    df['delta'] = df['beta'] + df['gamma']\n\n    # 3. Sort the DataFrame by the 'delta' column in descending order.\n    df_sorted = df.sort_values(by='delta', ascending=False)\n\n    # 4. Create a new DataFrame 'df_new' that contains only the 'alpha' and 'delta' columns.\n    df_new = df_sorted[['alpha', 'delta']]\n\n    # 5. Add a new column 'epsilon' to the 'df_new' DataFrame, which is the product of 'alpha' and 'delta'.\n    df_new['epsilon'] = df_new['alpha'] * df_new['delta']\n\n    # 6. Return the 'df_new' DataFrame.\n    return df_new\n\n"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the columns exist in the dataframe\n    if col1 not in data.columns or col2 not in data.columns:\n        return \"Error: One or both of the columns do not exist in the dataframe.\"\n    \n    # Check if the columns are categorical\n    if not data[col1].dtype.name.startswith('category') or not data[col2].dtype.name.startswith('category'):\n        return \"Error: One or both of the columns are not categorical.\"\n    \n    # Create a contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    \n    # Perform the chi-square test\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    # Return the result\n    return chi2, p, dof, expected\n\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Simulating dice rolls\n    results = np.random.choice(NUMBERS, size=rolls, replace=True)\n    \n    # Calculating frequency of each number\n    frequency = np.bincount(results)\n    \n    # Plotting the frequency distribution\n    plt.bar(NUMBERS, frequency, color='skyblue')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title(f'Frequency Distribution of {rolls} Dice Rolls')\n    plt.show()\n    \n    # Calculating and returning the mean and median\n    mean = np.mean(results)\n    median = np.median(results)\n    return mean, median\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    \n    \"\"\"\n    # Check if the source directory exists\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' does not exist.\")\n\n    # Check if the target directory exists, if not, create it\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create a zip file in the target directory\n    zip_file_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through the source directory and add all files to the zip file\n        for root, dirs, files in os.walk(source_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, source_dir))\n\n    return zip_file_path\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    \n    \"\"\"\n    # Create a new column 'total_sales' by summing 'sales' and 'profit' columns\n    df['total_sales'] = df['sales'] + df['profit']\n    \n    # Create a new column 'total_expenses' by summing 'rent' and 'utilities' columns\n    df['total_expenses'] = df['rent'] + df['utilities']\n    \n    # Create a new column 'profit_margin' by calculating the profit margin\n    df['profit_margin'] = df['total_sales'] / df['total_expenses']\n    \n    # Create a new column 'revenue_per_employee' by calculating the revenue per employee\n    df['revenue_per_employee'] = df['total_sales'] / df['employees']\n    \n    # Create a new column 'expenses_per_employee' by calculating the expenses per employee\n    df['expenses_per_employee'] = df['total_expenses'] / df['employees']\n    \n    # Create a new column 'profit_per_employee' by calculating the profit per employee\n    df['profit_per_employee'] = df['profit_margin'] * df['revenue_per_employee']\n    \n    # Create a new column 'profit_per_employee_margin' by calculating the profit per employee margin\n    df['profit_per_employee_margin'] = df['profit_per_employee'] / df['expenses_per_employee']\n    \n    # Create a new column 'profit_per_employee_margin_adjusted' by calculating the profit per employee margin adjusted\n    df['profit_per_employee_margin_adjusted'] = df['profit_per_employee_margin'] * (1 - df['employee_turnover_rate'])\n    \n    # Create a new column 'profit_per_employee_margin_adjusted_adjusted' by calculating the profit per employee margin adjusted adjusted\n    df['profit_per_em"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the z-score for each column in the DataFrame\n    z_scores = df.apply(zscore)\n    \n    # Identify columns where the absolute z-score is greater than the threshold\n    outliers = z_scores.abs() > z_threshold\n    \n    # Filter the DataFrame to only include rows where there are no outliers in any column\n    clean_df = df[~outliers.any(axis=1)]\n    \n    return clean_df\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    \n    \"\"\"\n    # Calculate the mean of the 'score' column\n    mean_score = df['score'].mean()\n    \n    # Calculate the median of the 'score' column\n    median_score = df['score'].median()\n    \n    # Calculate the mode of the 'score' column\n    mode_score = df['score'].mode()[0]\n    \n    # Calculate the standard deviation of the 'score' column\n    std_dev_score = df['score'].std()\n    \n    # Calculate the variance of the 'score' column\n    variance_score = df['score'].var()\n    \n    # Calculate the range of the 'score' column\n    range_score = df['score'].max() - df['score'].min()\n    \n    # Calculate the interquartile range of the 'score' column\n    iqr_score = df['score'].quantile(0.75) - df['score'].quantile(0.25)\n    \n    # Calculate the skewness of the 'score' column\n    skewness_score = df['score'].skew()\n    \n    # Calculate the kurtosis of the 'score' column\n    kurtosis_score = df['score'].kurt()\n    \n    # Calculate the correlation between 'score' and 'age' columns\n    corr_score_age = df['score'].corr(df['age'])\n    \n    # Calculate the correlation between 'score' and 'gender' columns\n    corr_score_gender = df['score'].corr(df['gender'])\n    \n    # Calculate the correlation between 'score' and 'class' columns\n    corr_score_class = df['score'].corr(df['class'])\n    \n    # Calculate the correlation between 'score' and 'subject' columns\n    corr_score_subject = df['score'].corr(df['subject'])\n    \n    # Calculate the correlation between 'score' and 'school' columns\n    corr_score_school = df['score'].corr"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    \n    \"\"\"\n    # 1. \u7ed8\u5236\u539f\u59cb\u6570\u636e\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['value'])\n    ax.set_title('Original Data')\n    \n    # 2. \u62df\u5408ARIMA\u6a21\u578b\n    model = ARIMA(df['value'], order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    \n    # 3. \u7ed8\u5236ARIMA\u6a21\u578b\u62df\u5408\u7ed3\u679c\n    ax2 = ax.twinx()\n    ax2.plot(df.index, model_fit.fittedvalues, color='red')\n    ax2.set_title('ARIMA Model Fitted')\n    \n    # 4. \u8ba1\u7b97\u6b8b\u5dee\n    residuals = pd.DataFrame(model_fit.resid)\n    residuals.plot(kind='kde', ax=ax.twinx())\n    ax.set_title('Residuals')\n    \n    # 5. \u8ba1\u7b97\u6b8b\u5dee\u7684\u7edf\u8ba1\u6458\u8981\n    print(residuals.describe())\n    \n    return residuals.describe(), ax\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    word = word.lower()\n    word = word.translate(str.maketrans('', '', string.punctuation))\n    word = ''.join(word.split())\n    \n    counter = Counter(word)\n    \n    return dict(counter)\n\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    \n    \"\"\"\n    # Generate a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with dates as index and categories as columns\n    sales_data = pd.DataFrame(index=dates, columns=categories)\n    \n    # Populate the DataFrame with random sales data\n    for category in categories:\n        sales_data[category] = np.random.randint(100, 1000, size=periods)\n    \n    # Plot the sales data\n    sales_data.plot(kind='bar', stacked=True)\n    plt.xlabel('Date')\n    plt.ylabel('Sales')\n    plt.title('Weekly Sales Data by Category')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate and print the total sales for each category\n    total_sales = sales_data.sum()\n    print(\"Total Sales by Category:\")\n    print(total_sales)\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    \n    \"\"\"\n    # Generate a date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Create a DataFrame with dates as the index\n    df = pd.DataFrame(index=dates)\n    \n    # Generate some random sales data\n    np.random.seed(0)\n    df['sales'] = np.random.randint(100, 500, size=periods)\n    \n    # Fit a linear regression model to the data\n    model = LinearRegression()\n    model.fit(df.index.to_frame(), df['sales'])\n    \n    # Predict sales for the next 4 weeks\n    future_dates = pd.date_range(start=dates[-1], periods=4, freq=freq)\n    future_df = pd.DataFrame(index=future_dates)\n    future_sales = model.predict(future_df.index.to_frame())\n    \n    # Add the predicted sales to the DataFrame\n    df['predicted_sales'] = np.concatenate((df['sales'].values, future_sales))\n    \n    return df\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    \n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n\n    # Create a DataFrame to store the tasks\n    task_df = pd.DataFrame(columns=[\"task_id\", \"task_name\", \"assigned_to\", \"due_date\"])\n\n    # Generate tasks\n    for i in range(n_tasks):\n        task_id = i + 1\n        task_name = f\"Task {task_id}\"\n        assigned_to = random.choice(employees)\n        due_date = datetime.now() + pd.DateOffset(days=random.randint(1, 30))\n\n        # Add the task to the DataFrame\n        task_df = task_df.append(\n            {\"task_id\": task_id, \"task_name\": task_name, \"assigned_to\": assigned_to, \"due_date\": due_date},\n            ignore_index=True,\n        )\n\n    return task_df\n\n"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n\ndef task_func(mystrings, text):\n    \"\"\"\n    \n    \"\"\"\n    # Task 1\n    mystrings = [s.strip() for s in mystrings]\n    mystrings = [s.lower() for s in mystrings]\n    mystrings = [re.sub(r'\\W+', ' ', s) for s in mystrings]\n    mystrings = [s.split() for s in mystrings]\n    \n    # Task 2\n    text = text.lower()\n    text = re.sub(r'\\W+', ' ', text)\n    text = text.split()\n    \n    # Task 3\n    counter = Counter(mystrings)\n    for word in text:\n        if word in counter:\n            counter[word] += 1\n        else:\n            counter[word] = 1\n    \n    # Task 4\n    words = list(counter.keys())\n    counts = list(counter.values())\n    \n    # Task 5\n    plt.figure(figsize=(10, 5))\n    plt.bar(words, counts)\n    plt.xlabel('Words')\n    plt.ylabel('Counts')\n    plt.title('Word Frequency')\n    plt.xticks(rotation=45)\n    plt.show()\n    \n    return counter\n\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u6e90\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\n    file_list = glob.glob(os.path.join(source_directory, '*'))\n    \n    # \u521b\u5efa\u76ee\u6807\u76ee\u5f55\uff08\u5982\u679c\u4e0d\u5b58\u5728\uff09\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n    \n    # \u521b\u5efazip\u6587\u4ef6\n    with zipfile.ZipFile(os.path.join(target_directory, zip_name), 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # \u5c06\u6587\u4ef6\u6dfb\u52a0\u5230zip\u6587\u4ef6\u4e2d\n        for file in file_list:\n            zipf.write(file, os.path.basename(file))\n\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u6e90\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\n    source_files = os.listdir(source_directory)\n    \n    # \u904d\u5386\u6e90\u6587\u4ef6\n    for file in source_files:\n        # \u83b7\u53d6\u6587\u4ef6\u8def\u5f84\n        file_path = os.path.join(source_directory, file)\n        \n        # \u5224\u65ad\u662f\u5426\u4e3adocx\u6587\u4ef6\n        if file_path.endswith('.docx'):\n            # \u6253\u5f00docx\u6587\u4ef6\n            doc = docx.Document(file_path)\n            \n            # \u83b7\u53d6\u6240\u6709\u6bb5\u843d\n            paragraphs = doc.paragraphs\n            \n            # \u904d\u5386\u6bb5\u843d\n            for para in paragraphs:\n                # \u83b7\u53d6\u6bb5\u843d\u6587\u672c\n                text = para.text\n                \n                # \u5224\u65ad\u6587\u672c\u662f\u5426\u5305\u542b\u7279\u5b9a\u5173\u952e\u8bcd\n                if '\u5173\u952e\u8bcd' in text:\n                    # \u83b7\u53d6\u6587\u4ef6\u540d\n                    file_name = os.path.basename(file_path)\n                    \n                    # \u521b\u5efa\u76ee\u6807\u6587\u4ef6\u8def\u5f84\n                    target_file_path = os.path.join(target_directory, file_name)\n                    \n                    # \u5c06\u5305\u542b\u7279\u5b9a\u5173\u952e\u8bcd\u7684docx\u6587\u4ef6\u590d\u5236\u5230\u76ee\u6807\u76ee\u5f55\n                    os.replace(file_path, target_file_path)\n                    \n                    # \u8fd4\u56de\u5df2\u5904\u7406\u6587\u4ef6\u6570\u91cf\n                    return 1\n    \n    # \u5982\u679c\u6ca1\u6709\u627e\u5230\u5305\u542b\u7279\u5b9a\u5173\u952e\u8bcd\u7684\u6587\u4ef6\uff0c\u8fd4\u56de0\n    return 0\n\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    \n    \"\"\"\n    # 1. Calculate the mean of each column\n    mean_values = df.mean()\n    \n    # 2. Calculate the standard deviation of each column\n    std_dev_values = df.std()\n    \n    # 3. Calculate the skewness of each column\n    skewness_values = df.skew()\n    \n    # 4. Calculate the kurtosis of each column\n    kurtosis_values = df.kurtosis()\n    \n    # 5. Calculate the median of each column\n    median_values = df.median()\n    \n    # 6. Calculate the mode of each column\n    mode_values = df.mode().iloc[0]\n    \n    # 7. Calculate the range of each column\n    range_values = df.max() - df.min()\n    \n    # 8. Calculate the interquartile range (IQR) of each column\n    iqr_values = df.quantile(0.75) - df.quantile(0.25)\n    \n    # 9. Calculate the variance of each column\n    variance_values = df.var()\n    \n    # 10. Calculate the coefficient of variation (CV) of each column\n    cv_values = df.std() / df.mean()\n    \n    # 11. Calculate the z-score of each value in the dataframe\n    z_scores = (df - df.mean()) / df.std()\n    \n    # 12. Calculate the percentage change of each value in the dataframe\n    pct_change = df.pct_change()\n    \n    # 13. Calculate the rolling mean of each column\n    rolling_mean = df.rolling(window=5).mean()\n    \n    # 14. Calculate the rolling median of each column\n    rolling_median = df.rolling(window=5).median()\n    \n    # 15. Calculate the"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u5f53\u524d\u65f6\u95f4\n    now = datetime.now(timezone.utc)\n    # \u83b7\u53d6\u76ee\u5f55\u8def\u5f84\n    path = Path(directory_path)\n    # \u83b7\u53d6\u76ee\u5f55\u7684\u521b\u5efa\u65f6\u95f4\n    create_time = path.stat().st_ctime\n    # \u8ba1\u7b97\u76ee\u5f55\u521b\u5efa\u65f6\u95f4\u4e0e\u5f53\u524d\u65f6\u95f4\u7684\u5dee\u503c\n    diff = now - datetime.fromtimestamp(create_time, timezone.utc)\n    # \u8fd4\u56de\u76ee\u5f55\u521b\u5efa\u65f6\u95f4\u4e0e\u5f53\u524d\u65f6\u95f4\u7684\u5dee\u503c\n    return diff.total_seconds()\n\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    \n    \"\"\"\n    # Load the JSON data\n    data = json.loads(json_data)\n\n    # Create a DataFrame from the JSON data\n    df = pd.DataFrame(data)\n\n    # Calculate the population density\n    df[\"density\"] = df[\"population\"] / df[\"area\"]\n\n    # Round the density to two decimal places\n    df[\"density\"] = df[\"density\"].round(2)\n\n    # Sort the DataFrame by population in descending order\n    df = df.sort_values(by=\"population\", ascending=False)\n\n    # Reset the index of the DataFrame\n    df = df.reset_index(drop=True)\n\n    # Save the DataFrame to a CSV file\n    df.to_csv(os.path.join(output_dir, file_name), index=False)\n\n    return df\n\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    \n    \"\"\"\n    result = {}\n    for predicate in predicates:\n        result[predicate] = []\n    for root, dirs, files in os.walk(dir_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            for predicate in predicates:\n                if re.match(predicate, file_path):\n                    result[predicate].append(file_path)\n    return result\n\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\n\ndef task_func(hex_string):\n    \"\"\"\n    \n    \"\"\"\n    # \u5c06\u5341\u516d\u8fdb\u5236\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u5b57\u8282\u5bf9\u8c61\n    byte_obj = binascii.unhexlify(hex_string)\n    \n    # \u5c06\u5b57\u8282\u5bf9\u8c61\u8f6c\u6362\u4e3aBase64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\n    base64_string = base64.b64encode(byte_obj).decode('utf-8')\n    \n    # \u5c06Base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3aURL\u5b89\u5168\u7684Base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\n    url_safe_base64_string = base64_string.replace('+', '-').replace('/', '_')\n    \n    # \u5c06URL\u5b89\u5168\u7684Base64\u7f16\u7801\u7684\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3aUnicode\u5b57\u7b26\u4e32\n    unicode_string = urllib.parse.unquote(url_safe_base64_string)\n    \n    # \u5c06Unicode\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3aUTF-8\u7f16\u7801\u7684\u5b57\u8282\u5bf9\u8c61\n    utf8_byte_obj = unicode_string.encode('utf-8')\n    \n    # \u5c06UTF-8\u7f16\u7801\u7684\u5b57\u8282\u5bf9\u8c61\u8f6c\u6362\u4e3a\u5341\u516d\u8fdb\u5236\u5b57\u7b26\u4e32\n    hex_string = binascii.hexlify(utf8_byte_obj).decode('utf-8')\n    \n    return hex_string\n\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n\n    # Verify the MD5 checksum\n    md5_checksum = hashlib.md5(open(TARGET_TAR_FILE, 'rb').read()).hexdigest()\n    if md5_checksum != EXPECTED_MD5_CHECKSUM:\n        raise ValueError(\"The MD5 checksum of the downloaded file does not match the expected value.\")\n\n    # Extract the tar file\n    with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n        tar.extractall()\n\n    # Remove the tar file\n    os.remove(TARGET_TAR_FILE)\n\n    print(\"Task completed successfully.\")\n\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\n\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    \n    \"\"\"\n    # Download the CSV file from the given URL\n    urllib.request.urlretrieve(url, csv_file_path)\n\n    # Read the CSV file and extract the specified column\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        column_data = [row[column_name] for row in reader]\n\n    # Count the occurrences of each unique value in the column\n    value_counts = collections.Counter(column_data)\n\n    # Print the value counts\n    for value, count in value_counts.items():\n        print(f\"{value}: {count}\")\n\n"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n    response = urllib.request.urlopen(url)\n    html = response.read()\n    # \u89e3\u6790\u7f51\u9875\n    selector = etree.HTML(html)\n    # \u83b7\u53d6\u6240\u6709tr\u6807\u7b7e\n    tr_list = selector.xpath('//table[@class=\"table table-striped\"]/tr')\n    # \u521b\u5efa\u4e00\u4e2a\u7a7a\u5217\u8868\uff0c\u7528\u4e8e\u5b58\u50a8\u6570\u636e\n    data_list = []\n    # \u904d\u5386\u6240\u6709tr\u6807\u7b7e\n    for tr in tr_list:\n        # \u83b7\u53d6\u6240\u6709td\u6807\u7b7e\n        td_list = tr.xpath('td')\n        # \u521b\u5efa\u4e00\u4e2a\u7a7a\u5217\u8868\uff0c\u7528\u4e8e\u5b58\u50a8\u6bcf\u884c\u6570\u636e\n        row_data = []\n        # \u904d\u5386\u6240\u6709td\u6807\u7b7e\n        for td in td_list:\n            # \u83b7\u53d6td\u6807\u7b7e\u7684\u6587\u672c\u5185\u5bb9\n            row_data.append(td.xpath('text()')[0])\n        # \u5c06\u6bcf\u884c\u6570\u636e\u6dfb\u52a0\u5230data_list\u4e2d\n        data_list.append(row_data)\n    # \u521b\u5efa\u4e00\u4e2apandas DataFrame\n    df = pd.DataFrame(data_list, columns=['Name', 'Age', 'Position', 'Country'])\n    # \u8fd4\u56deDataFrame\n    return df\n\n"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(url):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u7f51\u9875\u5185\u5bb9\n    with urllib.request.urlopen(url) as response:\n        html = response.read().decode('utf-8')\n\n    # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u6240\u6709\u5355\u8bcd\n    words = re.findall(r'\\b\\w+\\b', html)\n\n    # \u8ba1\u7b97\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\n    word_count = Counter(words)\n\n    # \u83b7\u53d6\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u5355\u8bcd\n    most_common_word = word_count.most_common(1)[0][0]\n\n    # \u83b7\u53d6\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u5355\u8bcd\u7684\u6b21\u6570\n    most_common_count = word_count.most_common(1)[0][1]\n\n    # \u7ed8\u5236\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u5206\u5e03\u56fe\n    plt.figure(figsize=(10, 5))\n    plt.bar(word_count.keys(), word_count.values())\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Word Frequency Distribution')\n    plt.show()\n\n    return most_common_word, most_common_count\n\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the download path exists, if not, create it\n    if not os.path.exists(download_path):\n        os.makedirs(download_path)\n\n    # Download the file\n    file_name = url.split(\"/\")[-1]\n    file_path = os.path.join(download_path, file_name)\n    response = requests.get(url, stream=True)\n    with open(file_path, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n\n    # Unzip the file\n    try:\n        with ZipFile(file_path, 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n    except BadZipFile:\n        print(f\"{file_name} is not a valid zip file.\")\n\n    # Remove the zip file\n    os.remove(file_path)\n\n    return file_path.replace(\".zip\", \"\")\n\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\n\ndef task_func(url, table_id):\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the table with the specified ID\n    table = soup.find('table', {'id': table_id})\n    \n    # Convert the table to a pandas DataFrame\n    df = pd.read_html(StringIO(str(table)))[0]\n    \n    return df\n\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\n\ndef task_func(url, filename):\n    \"\"\"\n    \n    \"\"\"\n    # Download the file\n    response = requests.get(url)\n    if response.status_code == 200:\n        # Save the file\n        file_path = DOWNLOAD_DIR / filename\n        with open(file_path, \"wb\") as f:\n            f.write(response.content)\n        print(f\"File {filename} downloaded successfully.\")\n    else:\n        print(f\"Failed to download file {filename}. Status code: {response.status_code}\")\n\n    # Unzip the file\n    if filename.endswith(\".zip\"):\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n        print(f\"File {filename} unzipped successfully.\")\n    else:\n        print(f\"File {filename} is not a zip file.\")\n\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n        return 1\n    \n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Find all the links on the page\n    links = soup.find_all(\"a\")\n    \n    # Extract the href attribute of each link\n    hrefs = [link.get(\"href\") for link in links]\n    \n    # Filter out the links that are not absolute URLs\n    absolute_hrefs = [href for href in hrefs if href and href.startswith(\"http\")]\n    \n    # Filter out the links that are not within the base URL\n    filtered_hrefs = [href for href in absolute_hrefs if href.startswith(base_url)]\n    \n    # Write the filtered links to a CSV file\n    with open(csv_file, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Link\"])\n        writer.writerows([[href] for href in filtered_hrefs])\n    \n    # Return the number of filtered links\n    return len(filtered_hrefs)\n\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the webpage\n    response = requests.get(webpage_url)\n    \n    # Parse the HTML content of the webpage\n    tree = html.fromstring(response.content)\n    \n    # Extract the data from the webpage\n    data = tree.xpath('//div[@class=\"data\"]/text()')\n    \n    # Create a pandas DataFrame from the extracted data\n    df = pd.DataFrame(data, columns=['Data'])\n    \n    # Connect to the SQLite database\n    conn = sqlite3.connect(database_name)\n    \n    # Create a table in the database to store the data\n    conn.execute('''CREATE TABLE IF NOT EXISTS my_table\n                     (Data TEXT)''')\n    \n    # Insert the data into the table\n    for index, row in df.iterrows():\n        conn.execute(\"INSERT INTO my_table VALUES (?)\", (row['Data'],))\n    \n    # Commit the changes to the database\n    conn.commit()\n    \n    # Close the database connection\n    conn.close()\n    \n    # Return the number of rows inserted into the table\n    return len(df)\n\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\n\nIMAGE_PATH = \"image.png\"\n\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    with codecs.open(filename, \"r\", encoding=from_encoding) as f:\n        text = f.read()\n    with codecs.open(filename, \"w\", encoding=to_encoding) as f:\n        f.write(text)\n\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    \n    \"\"\"\n    # Send GET request\n    response = requests.get(url)\n    \n    # Get encoding from response\n    if from_encoding is None:\n        from_encoding = chardet.detect(response.content)[\"encoding\"]\n    \n    # Decode response content\n    decoded_content = response.content.decode(from_encoding)\n    \n    # Load JSON data\n    data = json.loads(decoded_content)\n    \n    # Process data\n    processed_data = process_data(data)\n    \n    # Return processed data\n    return processed_data\n\n"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        # Read the CSV file into a DataFrame\n        df = pd.read_csv(csv_file_path)\n        \n        # Check if the column exists in the DataFrame\n        if column_name not in df.columns:\n            return \"Column not found in the CSV file.\"\n        \n        # Convert the specified column to datetime objects\n        df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n        \n        # Get the current date and time\n        current_date_time = datetime.now()\n        \n        # Create a new column with the current date and time\n        df['Current_Date_Time'] = current_date_time\n        \n        # Save the modified DataFrame back to the CSV file\n        df.to_csv(csv_file_path, index=False)\n        \n        return \"Column converted to datetime and current date and time added successfully.\"\n    \n    except FileNotFoundError:\n        return \"CSV file not found.\"\n    except EmptyDataError:\n        return \"CSV file is empty.\"\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    \n    \"\"\"\n    context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    context.load_cert_chain(cert_file, key_file)\n    \n    with context.wrap_socket(client_socket, server_side=True) as sock:\n        while True:\n            data = sock.recv(buffer_size)\n            if not data:\n                break\n            sock.sendall(data)\n\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    \n    \"\"\"\n    # Create a UDP socket\n    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as sock:\n        # Bind the socket to the server address and port\n        sock.bind((server_address, server_port))\n\n        # Create a queue to store incoming messages\n        message_queue = queue.Queue()\n\n        # Create a timer to stop the task after the specified duration\n        end_time = datetime.now() + timedelta(seconds=run_duration)\n\n        while datetime.now() < end_time:\n            # Wait for data to be available to read or write\n            readable, writable, exceptional = select.select([sock], [sock], [sock])\n\n            # Handle incoming messages\n            for s in readable:\n                if s is sock:\n                    data, address = sock.recvfrom(buffer_size)\n                    message_queue.put((data, address))\n\n            # Handle outgoing messages\n            for s in writable:\n                if s is sock:\n                    try:\n                        data, address = message_queue.get_nowait()\n                        sock.sendto(data, address)\n                    except queue.Empty:\n                        pass\n\n            # Handle exceptions\n            for s in exceptional:\n                if s is sock:\n                    sock.close()\n                    break\n\n        # Close the socket\n        sock.close()\n\n"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\n\ndef task_func(client_socket):\n    \"\"\"\n    \n    \"\"\"\n    # TODO: Implement the task function to handle the client's request\n    pass\n\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    \n    \"\"\"\n    # Load the data\n    data = pd.read_csv(file_path)\n    \n    # Extract the text\n    text = data['text'].tolist()\n    \n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n    \n    # Fit and transform the text\n    X = vectorizer.fit_transform(text)\n    \n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Get the word counts\n    word_counts = X.sum(axis=0)\n    \n    # Create a DataFrame with the word counts\n    word_count_df = pd.DataFrame(word_counts, columns=['count'])\n    word_count_df['word'] = feature_names\n    \n    # Sort the DataFrame by word count\n    word_count_df = word_count_df.sort_values('count', ascending=False)\n    \n    # Plot the word counts\n    plt.figure(figsize=(10, 5))\n    plt.bar(word_count_df['word'][:10], word_count_df['count'][:10])\n    plt.xlabel('Words')\n    plt.ylabel('Count')\n    plt.title('Top 10 Most Frequent Words')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Save the plot if a save path is provided\n    if save_path:\n        plt.savefig(save_path)\n    \n    # Show the plot\n    plt.show()\n    \n    # Return the DataFrame\n    return word_count_df\n\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\n\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    \n    \"\"\"\n    if animals is None:\n        animals = ['cat', 'dog', 'bird', 'fish']\n    if foods is None:\n        foods = ['fish', 'chicken', 'beef', 'pizza']\n    \n    # Create a DataFrame with animals and foods\n    df = pd.DataFrame({'animals': animals, 'foods': foods})\n    \n    # Create a new column with combinations of animals and foods\n    df['combinations'] = df.apply(lambda row: f\"{row['animals']} eats {row['foods']}\", axis=1)\n    \n    # Create a new column with combinations of animals and foods in reverse order\n    df['reverse_combinations'] = df.apply(lambda row: f\"{row['foods']} is eaten by {row['animals']}\", axis=1)\n    \n    # Create a new column with combinations of animals and foods in both orders\n    df['both_combinations'] = df.apply(lambda row: f\"{row['animals']} eats {row['foods']} and {row['foods']} is eaten by {row['animals']}\", axis=1)\n    \n    return df\n\n"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    \n    \"\"\"\n    # Convert time strings to datetime objects\n    time_objects = [datetime.strptime(time_string, \"%Y-%m-%d %H:%M:%S\") for time_string in time_strings]\n    \n    # Set the timezone\n    timezone = pytz.timezone(timezone)\n    \n    # Localize the time objects to the specified timezone\n    localized_times = [time.replace(tzinfo=pytz.utc).astimezone(timezone) for time in time_objects]\n    \n    # Convert the localized times to numpy arrays\n    localized_times_array = np.array(localized_times)\n    \n    # Return the numpy array of localized times\n    return localized_times_array\n\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    \n    \"\"\"\n    # 1. Remove all punctuation and convert the text to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    \n    # 2. Split the text into words\n    words = text.split()\n    \n    # 3. Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # 4. Plot a bar chart of the word frequencies\n    plt.bar(word_counts.keys(), word_counts.values())\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Word Frequencies')\n    plt.show()\n    \n    # 5. Return the most common word and its frequency\n    most_common_word, frequency = word_counts.most_common(1)[0]\n    return most_common_word, frequency\n\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n\ndef task_func(myString):\n    \"\"\"\n    \n    \"\"\"\n    # Check if the input is a valid URL\n    try:\n        url = urlparse(myString)\n        if url.scheme and url.netloc:\n            pass\n        else:\n            return \"Invalid URL\"\n    except ValueError:\n        return \"Invalid URL\"\n\n    # Send a GET request to the URL\n    response = requests.get(myString)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        # Parse the HTML content of the page\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Find all the links on the page\n        links = soup.find_all('a')\n\n        # Extract the href attribute of each link\n        hrefs = [link.get('href') for link in links]\n\n        # Filter out any None values and duplicates\n        hrefs = list(set(filter(None, hrefs)))\n\n        # Return the list of unique href attributes\n        return hrefs\n    else:\n        return \"Failed to retrieve the page\"\n\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    \n    \"\"\"\n    # \u83b7\u53d6\u5f53\u524d\u65f6\u95f4\n    now = datetime.now()\n    # \u683c\u5f0f\u5316\u5f53\u524d\u65f6\u95f4\n    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # \u83b7\u53d6json_data\u4e2d\u7684unknown_key\u7684\u503c\n    unknown_value = json_data.get(unknown_key)\n    \n    # \u6253\u5370\u5f53\u524d\u65f6\u95f4\u3001unknown_key\u7684\u503c\u548cjson_data\n    print(f\"\u5f53\u524d\u65f6\u95f4: {now_str}\")\n    print(f\"{unknown_key}\u7684\u503c: {unknown_value}\")\n    print(f\"json_data: {json_data}\")\n    \n    # \u5982\u679csave_dir\u4e0d\u4e3a\u7a7a\uff0c\u5219\u5c06json_data\u4fdd\u5b58\u4e3ajson\u6587\u4ef6\n    if save_dir:\n        file_path = os.path.join(save_dir, f\"{unknown_key}_{now_str}.json\")\n        with open(file_path, 'w') as f:\n            json.dump(json_data, f, ensure_ascii=False, indent=4)\n        print(f\"json_data\u5df2\u4fdd\u5b58\u4e3a{file_path}\")\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    \n    \"\"\"\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Parse the HTML content of the page\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    \n    # Find all the links on the page\n    links = soup.find_all('a')\n    \n    # Extract the href attribute of each link\n    hrefs = [link.get('href') for link in links]\n    \n    # Filter out any None values and duplicates\n    hrefs = list(set(filter(None, hrefs)))\n    \n    # Write the hrefs to a JSON file\n    with open(output_path, 'w') as f:\n        json.dump(hrefs, f)\n    \n    return hrefs\n"}
