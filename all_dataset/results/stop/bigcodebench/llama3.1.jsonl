{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    \"\"\"\n    Download all files from specific directory on FTP server using wget subprocess.\n    \n    Args:\n    ftp_server (str): FTP server address. Default 'ftp.dlptest.com'.\n    ftp_user (str): FTP server username. Default 'dlpuser'.\n    ftp_password (str): FTP server password. Default 'rNrKYTX9g7z3RgJRmxWuGHbeu'.\n    ftp_dir (str): directory path on FTP server from files need downloaded. Default '/ftp/test'.\n    \n    Returns:\n    List[str]: A list filenames were attempted downloaded from FTP server.\n    \n    Raises:\n    Exception: \n        - If failure connecting FTP server. Outputs message \"Failed connect FTP server {ftp_server}: {str(e)}\"\n        - If failure logging into FTP server. Outputs message \"Failed log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\"\n        - If failure changing specified directory. Outputs message \"Failed change directory {ftp_dir} on server {ftp_server}: {str(e)}\"\n    \n    Requirements:\n    - subprocess\n    - ftplib\n    - os\n\n    Example:\n    >>> task_func()\n    ['file1.txt', 'file2.jpg', ...]\n    \"\"\"\n    # Connect to FTP server\n    try:\n        ftp = ftplib.FTP(ftp_server)\n    except Exception as e:\n        print(f\"Failed connect FTP server {ftp_server}: {str(e)}\")\n        return []\n    \n    # Login to FTP server\n    try:\n        ftp.login(user=ftp_user, passwd=ftp_password)\n    except Exception as e:\n        print(f\"Failed log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        ftp.quit()\n        return []\n    \n    # Change directory\n    try:\n        ftp.cwd(ftp_dir)\n    except Exception as e:\n        print(f\"Failed change directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n        ftp.quit()\n        return []\n    \n    # Get list of files in directory\n    files = ftp.nlst()\n    \n    # Download files using wget subprocess\n    downloaded_files = []\n    for file in files:\n        # Construct wget command\n        cmd = f\"wget -r -nH -nd -N -P /tmp {ftp_server}/{ftp_dir}/{file}\"\n        \n        # Run wget command\n        try:\n            subprocess.run(cmd, shell=True, check=True)\n            downloaded_files.append(file)\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed download file {file} from FTP server {ftp_server}: {str(e)}\")\n    \n    # Quit FTP connection\n    ftp.quit()\n    \n    return downloaded_files\n"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    \"\"\"\n    Execute list shell commands read from CSV file save outputs separate files.\n    Each command's output written unique file specified output directory.\n    If command fails, error message along with exit code appended respective output file.\n\n    Parameters:\n    - commands_file_path (str): Path CSV file containing shell commands first column.\n                                file should not have headers.\n    - output_dir_path (str): Path where outputs commands will saved. If directory does not exist,\n                             will created.\n\n    Requirements:\n    - subprocess\n    - csv\n    - os\n\n    Raises:\n    - FileNotFoundError: If commands_file_path does not exist.\n\n    Returns:\n    - list str: A list paths output files created output directory, each named as\n                   'command_X_output.txt', where X command index. If command execution fails,\n                   output file will contain descriptive error message exit code.\n\n    Example:\n    >>> task_func(\"commands.csv\", \"/path/to/output_directory\")\n    ['/path/to/output_directory/command_1_output.txt', '/path/to/output_directory/command_2_output.txt', ...]\n    \"\"\"\n    # Check if commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"File not found: {commands_file_path}\")\n\n    # Create output directory if it does not exist\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    # Read commands from CSV file\n    with open(commands_file_path, 'r') as commands_file:\n        csv_reader = csv.reader(commands_file)\n        commands = [row[0] for row in csv_reader]\n\n    # Execute each command and save output\n    output_files = []\n    for i, command in enumerate(commands):\n        output_file_path = os.path.join(output_dir_path, f\"command_{i+1}_output.txt\")\n        try:\n            # Execute command and capture output\n            output = subprocess.check_output(command, shell=True)\n            # Save output to file\n            with open(output_file_path, 'wb') as output_file:\n                output_file.write(output)\n        except subprocess.CalledProcessError as e:\n            # If command fails, save error message and exit code to file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(f\"Command failed with exit code {e.returncode}: {e.output.decode('utf-8')}\")\n        output_files.append(output_file_path)\n\n    return output_files\n"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    \"\"\"\n    Check if particular process running based on its name. If not running, start using process name as command. \n    If running, terminate process restart by executing process name as command.\n\n    Parameters:\n    - process_name (str): name process check manage. This should executable as command.\n\n    Returns:\n    - str: A message indicating action taken:\n        - \"Process not found. Starting <process_name>.\"\n        - \"Process found. Restarting <process_name>.\"\n\n    Requirements:\n    - subprocess\n    - psutil\n    - time\n\n    Example:\n    >>> task_func('notepad')\n    \"Process not found. Starting notepad.\"\n    OR\n    >>> task_func('notepad')\n    \"Process found. Restarting notepad.\"\n    \"\"\"\n    # Check if process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process found, terminate and restart\n            print(f\"Process found. Restarting {process_name}.\")\n            # Terminate process\n            proc.terminate()\n            # Wait for process to terminate\n            time.sleep(1)\n            # Restart process\n            subprocess.run([process_name])\n            return f\"Process found. Restarting {process_name}.\"\n    # Process not found, start it\n    print(f\"Process not found. Starting {process_name}.\")\n    subprocess.run([process_name])\n    return f\"Process not found. Starting {process_name}.\"\n"}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    \"\"\"\n    Zips all files (not including subdirectories) located specified directory returns path created zip file.\n    \n    Parameters:\n    directory (str): directory path containing files zipped.\n    \n    Returns:\n    str: path generated zip file. Returns None if directory does not contain files.\n    \n    Raises:\n    FileNotFoundError: if specified directory does not exist\n\n    Requirements:\n    - os\n    - glob\n    - zipfile\n    \n    Notes:\n    - zip name always 'files.zip'\n\n    Example:\n    >>> path = task_func('/path/to/files')\n    >>> isinstance(path, str)\n    True\n    \"\"\"\n    # Check if directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n    \n    # Check if directory contains files\n    if not glob.glob(os.path.join(directory, '*')):\n        return None\n    \n    # Create zip file\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zip_file:\n        for file in glob.glob(os.path.join(directory, '*')):\n            if os.path.isfile(file):\n                zip_file.write(file, os.path.basename(file))\n    \n    return zip_path\n"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    Create word cloud from text after removing URLs plot it.\n\n    Parameters:\n    - text (str): text analyze.\n\n    Returns:\n    WordCloud object: generated word cloud.\n    Raises:\n    ValueError(\"No words available generate word cloud after removing URLs.\"): If no words available generate word cloud after removing URLs.\n\n    Requirements:\n    - re\n    - wordcloud.WordCloud\n    - matplotlib.pyplot\n\n    Example:\n    >>> print(task_func('Visit https://www.python.org for more info. Python great. I love Python.').words_)\n    {'Python': 1.0, 'Visit': 0.5, 'info': 0.5, 'great': 0.5, 'love': 0.5}\n    >>> print(task_func('Check out this link: http://www.example.com. Machine learning fascinating.').words_)\n    {'Check': 1.0, 'link': 1.0, 'Machine': 1.0, 'learning': 1.0, 'fascinating': 1.0}\n    \"\"\"\n    # Remove URLs from text\n    text = re.sub(r'http\\S+', '', text)\n\n    # Create word cloud\n    wordcloud = WordCloud().generate(text)\n\n    # Plot word cloud\n    plt.figure(figsize=(10, 8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n\n    # Return word cloud\n    return wordcloud\n\n"}
{"task_id": "BigCodeBench/37", "solution": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, target_column):\n    \"\"\"\n    Train random forest classifier perform classification rows dataframe with respect column interest plot bar plot feature importance each column dataframe.\n    - xlabel bar plot should 'Feature Importance Score', ylabel 'Features' title 'Visualizing Important Features'.\n    - Sort feature importances descending order.\n    - Use feature importances on x-axis feature names on y-axis.\n\n    Parameters:\n    - df (pandas.DataFrame) : Dataframe containing data classify.\n    - target_column (str) : Name target column.\n\n    Returns:\n    - sklearn.model.RandomForestClassifier : random forest classifier trained on input data.\n    - matplotlib.axes.Axes: Axes object plotted data.\n\n    Requirements:\n    - pandas\n    - sklearn.ensemble\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> import pandas as pd\n    >>> data = pd.DataFrame({\"X\" : [-1, 3, 5, -4, 7, 2], \"label\": [0, 1, 1, 0, 1, 1]})\n    >>> model, ax = task_func(data, \"label\")\n    >>> print(data.head(2))\n       X  label\n    0 -1      0\n    1  3      1\n    >>> print(model)\n    RandomForestClassifier(random_state=42)\n    \"\"\"\n    # Create a copy of the dataframe to avoid modifying the original dataframe\n    df_copy = df.copy()\n\n    # Define the features (X) and the target (y)\n    X = df_copy.drop(target_column, axis=1)\n    y = df_copy[target_column]\n\n    # Train a random forest classifier\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X, y)\n\n    # Get the feature importances\n    feature_importances = model.feature_importances_\n\n    # Create a bar plot of the feature importances\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=X.columns, y=feature_importances)\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n\n    return model, plt.gca()  # Return the axes object\n\n"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\ndef task_func(secret_key, template_folder):\n    \"\"\"\n    Creates Flask application with configured user authentication using Flask-Login.\n    It defines routes for login, logout, protected page. user authentication\n    managed with simple User class login form using Flask-WTF. application\n    uses dynamic configuration for security template rendering.\n\n    Parameters:\n        secret_key (str): A secret key for application use for session management.\n        template_folder (str): path directory containing Flask templates.\n\n    Requirements:\n    - flask\n    - flask_login\n    - flask_wtf\n    - wtforms\n    - wtforms.validators\n    - werkzeug.security\n\n    Returns:\n        Flask: A Flask application instance configured for user authentication.\n\n    Examples:\n    >>> app = task_func('mysecretkey', 'templates')\n    >>> 'login' [rule.endpoint for rule app.url_map.iter_rules()]\n    True\n    >>> app.config['SECRET_KEY'] == 'mysecretkey'\n    True\n    \"\"\"\n    app = Flask(__name__)\n    app.config['SECRET_KEY'] = secret_key\n    app.config['TEMPLATES_FOLDER'] = template_folder\n    login_manager.init_app(app)\n\n    class User(UserMixin):\n        def __init__(self, id, username, password):\n            self.id = id\n            self.username = username\n            self.password = generate_password_hash(password)\n\n        def check_password(self, password):\n            return check_password_hash(self.password, password)\n\n    @login_manager.user_loader\n    def load_user(user_id):\n        return User.query.get(int(user_id))\n\n    @app.route('/')\n    def index():\n        return render_template('index.html')\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = User.query.filter_by(username=form.username.data).first()\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n            else:\n                return 'Invalid username or password'\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('index'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return render_template('protected.html', username=current_user.username)\n\n    return app\n"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies removes outliers from specified column dataset based on Z-score.\n    It standardizes column, calculates Z-scores, removes data points where Z-score exceeds threshold.\n    function also visualizes data before after outlier removal.\n\n    Parameters:\n    data (ndarray): dataset.\n    column (int): index column analyze for outliers.\n    outlier_z_score (float): Z-score threshold identify outliers.\n\n    Returns:\n    tuple: A tuple containing original data, data without outliers, indices outliers.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n    - sklearn.preprocessing.StandardScaler\n    \n    Notes:\n    function plots two scatter plots: 'Data with Outliers' shows original data including outliers,\n    while 'Data without Outliers' displays data after removing outliers based on provided Z-score threshold.\n    This visual comparison helps illustrate impact outlier removal on dataset.\n    \n    Examples:\n    >>> data = np.array([[14, 25], [1, 22], [7, 8], [100, 200]])\n    >>> column = 1\n    >>> len(task_func(data, column, 3.0))\n    3\n    >>> isinstance(task_func(data, column, 3.0)[0], np.ndarray)\n    True\n    >>> isinstance(task_func(data, column, 3.0)[1], np.ndarray)\n    True\n    >>> isinstance(task_func(data, column, 3.0)[2], tuple)\n    True\n    \"\"\"\n    # Standardize column\n    scaler = StandardScaler()\n    data[:, column] = scaler.fit_transform(data[:, column].reshape(-1, 1)).flatten()\n    \n    # Calculate Z-scores\n    z_scores = stats.zscore(data[:, column])\n    \n    # Identify outliers\n    outliers = np.where(np.abs(z_scores) > outlier_z_score)[0]\n    \n    # Remove outliers\n    data_without_outliers = np.delete(data, outliers, axis=0)\n    \n    # Visualize data\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.scatter(data[:, 0], data[:, column], c='blue', alpha=0.5)\n    plt.title('Data with Outliers')\n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers[:, 0], data_without_outliers[:, column], c='blue', alpha=0.5)\n    plt.title('Data without Outliers')\n    plt.show()\n    \n    return data, data_without_outliers, outliers\n"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    \"\"\"\n    Perform K-means clustering on dataset generate scatter plot visualizing clusters their centroids.\n\n    Parameters:\n        data (pd.DataFrame): dataset clustered, where rows samples columns features.\n        n_clusters (int): number clusters form. Must greater than 1. Defaults 3.\n\n    Returns:\n        tuple: \n            - np.ndarray: An array cluster labels assigned each sample.\n            - plt.Axes: An Axes object with scatter plot showing clusters centroids.\n\n    Raises:\n        ValueError: If 'data' not pd.DataFrame.\n        ValueError: If 'n_clusters' not integer greater than 1.\n\n    Requirements:\n        - numpy\n        - pandas\n        - matplotlib\n        - sklearn\n    \n    Example:\n    >>> np.random.seed(42)\n    >>> data = pd.DataFrame(np.random.rand(100, 2), columns=['Feature1', 'Feature2'])\n    >>> _, ax = task_func(data, 3)\n    >>> ax.get_title()\n    'K-Means Clustering'\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame.\")\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Number of clusters must be an integer greater than 1.\")\n\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(data)\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    fig, ax = plt.subplots()\n    ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=labels, cmap='viridis')\n    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.5)\n    ax.set_title('K-Means Clustering')\n    return labels, ax\n"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on dataset record result.\n    Also, generates scatter plot transformed data.\n\n    Parameters:\n    data (DataFrame): dataset.\n    n_components (int): number principal components calculate. Default 2.\n\n    Returns:\n    DataFrame: transformed data with principal components.\n    Axes: matplotlib Axes object containing scatter plot.\n\n    Raises:\n    ValueError: If n_components not positive integer.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n    - sklearn.decomposition\n\n    Example:\n    >>> data = pd.DataFrame([[14, 25], [1, 22], [7, 8]], columns=['Column1', 'Column2'])\n    >>> transformed_data, plot = task_func(data)\n    \"\"\"\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer.\")\n\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Create DataFrame from transformed data\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    # Add original data to DataFrame\n    transformed_df['Original'] = data.iloc[:, 0]\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df['Original'], transformed_df['PC1'], label='PC1')\n    ax.scatter(transformed_df['Original'], transformed_df['PC2'], label='PC2')\n    ax.set_xlabel('Original Data')\n    ax.set_ylabel('Principal Component')\n    ax.set_title('Scatter Plot of Transformed Data')\n    ax.legend()\n\n    return transformed_df, ax\n"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    \"\"\"\n    Draws seaborn pair plot iris dataset using Arial font.\n\n    This function sets global font Arial for better readability visual appeal. It generates pair plot from iris dataset, where each subplot represents relationship between two features, colored by species. plot includes title 'Iris Dataset Pair Plot' labels for each feature on axes.\n\n    Parameters:\n    None\n\n    Returns:\n        plt.Figure: A matplotlib Figure object containing seaborn pair plot iris dataset. plot has 'Iris Dataset Pair Plot' as its title. Each subplot's axes labeled with corresponding feature names, such as 'sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'.\n\n    Requirements:\n        - matplotlib.pyplot\n        - pandas\n        - seaborn\n        - sklearn.datasets\n\n    Example:\n        >>> fig = task_func()\n        >>> type(fig)\n        <class 'matplotlib.figure.Figure'>\n    \"\"\"\n    # Set global font to Arial for better readability\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Load iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target\n\n    # Map species to string for better visualization\n    species_map = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n    df['species'] = df['species'].map(species_map)\n\n    # Create pair plot\n    plt.figure(figsize=(10, 8))\n    sns.pairplot(df, hue='species', markers=['o', 's', 'D'], palette='husl')\n    plt.title('Iris Dataset Pair Plot')\n    plt.show()\n\n    # Return figure object\n    return plt.gcf()\n"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    \"\"\"\n    Generates plot random time series data for past 30 days with reproducibility \n    controlled by optional seed parameter.\n\n    plot styled with Arial font for better readability.\n\n    Parameters:\n        seed (int, optional): Seed for random number generator ensure reproducibility. Defaults 42.\n\n    Returns:\n        matplotlib.axes.Axes: Axes object containing line plot time series data. \n                              plot will have 'Date' as x-axis label, 'Value' as y-axis label, \n                              'Random Time Series Data' as title.\n\n    Raises:\n        ValueError: If issue generating data or plot.\n\n    Requirements:\n        - matplotlib.pyplot\n        - pandas\n        - random\n        - datetime\n\n    Example:\n        >>> ax = task_func()\n        >>> ax.get_title()\n        'Random Time Series Data'\n        >>> ax.get_xlabel()\n        'Date'\n        >>> ax.get_ylabel()\n        'Value'\n    \"\"\"\n    # Set seed for reproducibility\n    random.seed(seed)\n\n    # Generate random time series data for past 30 days\n    start_date = datetime.now() - timedelta(days=30)\n    dates = [start_date + timedelta(days=i) for i in range(31)]\n    values = [random.random() for _ in range(31)]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Date': dates, 'Value': values})\n\n    # Plot data\n    try:\n        ax = df.plot(x='Date', y='Value', figsize=(10, 6), fontsize=12, fontname='Arial')\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        return ax\n    except Exception as e:\n        raise ValueError(f\"Failed to generate plot: {str(e)}\")\n"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    Draw correlation heatmap Boston Housing dataset using Seaborn, with option save specified file.\n\n    Parameters:\n        seed (int, optional): Random seed for reproducibility. Defaults 42.\n    font should family sans-serif Arial.\n\n    Returns:\n        matplotlib.axes.Axes: Axes object containing heatmap plot.\n\n    Raises:\n        ValueError: If error occurs generating or saving plot.\n\n    Requirements:\n        - matplotlib\n        - os\n        - pandas\n        - seaborn\n        - numpy \n\n    Example:\n        >>> ax = task_func()\n        >>> type(ax)\n        <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Load Boston Housing dataset\n    df = pd.read_csv(data_url)\n\n    # Create correlation matrix\n    corr_matrix = df.corr()\n\n    # Create heatmap\n    plt.figure(figsize=(10, 8))\n    sns.set(font=\"Arial\")\n    ax = sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", square=True, linecolor=\"white\", linewidths=0.5)\n    ax.set_title(\"Boston Housing Dataset Correlation Heatmap\")\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"Feature\")\n\n    # Save plot to file\n    try:\n        plt.savefig(\"boston_housing_corr_heatmap.png\", bbox_inches=\"tight\")\n    except Exception as e:\n        print(f\"Error saving plot: {e}\")\n        raise\n\n    return ax\n"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    \"\"\"\n    Decomposes time series 'value' column DataFrame into trend, seasonality, residuals.\n\n    Parameters:\n    df (DataFrame): DataFrame with columns 'group', 'date', 'value'.\n    freq (str, optional): Frequency time series data. Defaults 'D' (daily).\n    decomposition_model (str, optional): Type decomposition model. \n        Options 'additive' or 'multiplicative'. Defaults 'multiplicative'.\n\n    Returns:\n    tuple: A tuple containing decomposition result (DecomposeResult object) matplotlib Axes object.\n\n    Raises:\n    ValueError: If 'df' not DataFrame, lacks required columns, or contains invalid data types.\n    ValueError: If 'freq' not valid frequency string.\n    ValueError: If 'decomposition_model' not 'additive' or 'multiplicative'.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - statsmodels.tsa.seasonal\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     \"group\": [\"A\"] * 14,\n    ...     \"date\": pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"2022-01-04\", \n    ...                            \"2022-01-05\", \"2022-01-06\", \"2022-01-07\", \"2022-01-08\",\n    ...                            \"2022-01-09\", \"2022-01-10\", \"2022-01-11\", \"2022-01-12\", \n    ...                            \"2022-01-13\", \"2022-01-14\"]),\n    ...     \"value\": [10, 12, 13, 15, 17, 16, 14, 13, 12, 15, 17, 18, 20, 19],\n    ... })\n    >>> result, ax = task_func(df, freq='D', decomposition_model='multiplicative')\n    >>> plt.show()  # This will display plot with title 'Time Series Decomposition' y-axis labeled 'Value'\n    \"\"\"\n    # Check if 'df' is DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n\n    # Check if 'df' has required columns\n    required_columns = ['group', 'date', 'value']\n    if not all(col in df.columns for col in required_columns):\n        raise ValueError(\"DataFrame 'df' must contain columns 'group', 'date', 'value'.\")\n\n    # Check if 'df' contains valid data types\n    if not all(df[col].dtype in [int, float] for col in required_columns):\n        raise ValueError(\"DataFrame 'df' must contain numeric data types for columns 'group', 'date', 'value'.\")\n\n    # Check if 'freq' is valid frequency string\n    if freq not in ['D', 'W', 'M', 'Q', 'Y']:\n        raise ValueError(\"Invalid frequency string. Options: 'D', 'W', 'M', 'Q', 'Y'.\")\n\n    # Check if 'decomposition_model' is valid\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid decomposition model. Options: 'additive', 'multiplicative'.\")\n\n    # Set index to 'date' column\n    df.set_index('date', inplace=True)\n\n    # Resample data to specified frequency\n    df_resampled = df.resample(freq).mean()\n\n    # Perform decomposition\n    decomposition = seasonal_decompose(df_resampled['value'], model=decomposition_model)\n\n    # Extract trend, seasonality, residuals\n    trend = decomposition.trend\n    seasonality = decomposition.seasonal\n    residuals = decomposition.resid\n\n    # Create plot\n    fig, ax = plt.subplots(4, 1, figsize=(10, 12))\n    ax[0].plot(df_resampled['value'], label='Original')\n    ax[0].legend(loc='best')\n    ax[0].set_title('Original')\n    ax[1].plot(trend, label='Trend')\n    ax[1].legend(loc='best')\n    ax[1].set_title('Trend')\n    ax[2].plot(seasonality, label='Seasonality')\n    ax[2].legend(loc='best')\n    ax[2].set_title('Seasonality')\n    ax[3].plot(residuals, label='Residuals')\n    ax[3].legend(loc='best')\n    ax[3]."}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate pandas Series random dates within specified date range, \n    including both start_date end_date, with optional seed for reproducibility.\n    \n    function creates series dates randomly selected between specified start \n    end dates, inclusive. It allows specifying seed for random number generator ensure \n    reproducible results, making suitable for simulations or tests requiring consistency.\n    \n    Parameters:\n    - start_date (datetime.datetime, optional): start date range. Defaults January 1, 2020.\n    - end_date (datetime.datetime, optional): end date range. Defaults December 31, 2020.\n    - seed (int, optional): Seed for random number generator ensure reproducibility. Default 42.\n    \n    Returns:\n    - pandas.Series: A Series object containing random dates within specified range, with each \n      date being datetime.datetime object. series length matches number days \n      specified range.\n    \n    Raises:\n    - ValueError: If 'start_date' or 'end_date' not datetime.datetime instance, or if 'start_date' \n      later than 'end_date'.\n\n    Note:\n    start_date end_date inclusive, meaning both dates considered as potential values \n    generated series. default seed value 42, ensuring results reproducible by default \n    unless different seed specified by user.\n    \n    Requirements:\n    - pandas\n    - datetime\n    - random\n    \n    Example:\n    >>> dates = task_func(seed=123)\n    >>> print(dates.head())  # Prints first 5 dates from series\n    0   2020-01-27\n    1   2020-05-17\n    2   2020-02-14\n    3   2020-07-27\n    4   2020-05-16\n    dtype: datetime64[ns]\n    \"\"\"\n    # Check if start_date and end_date are datetime objects\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"Both start_date and end_date must be datetime objects.\")\n    \n    # Check if start_date is not later than end_date\n    if start_date > end_date:\n        raise ValueError(\"start_date cannot be later than end_date.\")\n    \n    # Set seed for random number generator\n    random_seed(seed)\n    \n    # Generate random dates within specified range\n    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n    \n    # Randomly select dates from generated range\n    random_dates = dates.sample(frac=1).unique()\n    \n    return pd.Series(random_dates)\n"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    \"\"\"\n    Modify list by adding element '12', concatenate number CSV files \n    from directory into single DataFrame. number files concatenated \n    determined by sum numbers list.\n\n    Parameters:\n    my_list (list): input list, modified place.\n    file_dir (str, optional): directory search for CSV files. Defaults './data_files/'.\n    file_ext (str, optional): file extension files concatenate. Defaults '.csv'.\n\n    Returns:\n    DataFrame: A pandas DataFrame concatenating data from selected CSV files.\n\n    Raises:\n    TypeError: If 'my_list' not list.\n    FileNotFoundError: If no files found specified directory.\n\n    Requirements:\n    - pandas\n    - os\n    - glob\n\n    Example:\n    >>> create_dummy_csv()\n    >>> my_list = [1, 2, 3]\n    >>> df = task_func(my_list)\n    >>> print(df.head())\n       A  B\n    0  0  3\n    1  1  4\n    2  2  5\n    3  0  3\n    4  1  4\n    >>> tearDown_dummy()\n    \"\"\"\n    # Check if 'my_list' is list\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input 'my_list' must be list.\")\n\n    # Add element '12' to 'my_list'\n    my_list.append(12)\n\n    # Calculate number of files to concatenate\n    num_files = sum(my_list)\n\n    # Get list of CSV files in directory\n    csv_files = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n\n    # Check if any files found\n    if not csv_files:\n        raise FileNotFoundError(\"No files found in specified directory.\")\n\n    # Concatenate CSV files into DataFrame\n    df = pd.concat([pd.read_csv(file) for file in csv_files[:num_files]], ignore_index=True)\n\n    return df\n"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint,seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    \"\"\"\n    Enhances 'my_list' by appending number 12, generates list random integers based \n    on sum elements 'my_list', limited by 'size'. It measures time taken for this process \n    plots histogram generated random numbers.\n\n    size random numbers list determined by sum numbers 'my_list', with \n    upper limit set by 'size'. random integers within range 1 100, inclusive.\n\n    Parameters:\n    - my_list (list): input list containing numeric elements.\n    - size (int): Maximum size limit for generated list random numbers. Default 100.\n    - seed (int): Seed value for random number generator for reproducibility. Default 100.\n\n    Returns:\n    - tuple: A tuple containing time taken generate list (in seconds, as float) \n      matplotlib Axes object for histogram. histogram's x-axis labeled 'Number', \n      representing range random integers, y-axis labeled 'Frequency', representing \n      frequency each integer generated list.\n\n    Raises:\n    - TypeError: If 'my_list' not list.\n    - ValueError: If 'my_list' contains elements not numeric (int or float).\n\n    histogram plots distribution random numbers generated, with number range (1-100) \n    on x-axis count (frequency) each number on y-axis.\n\n    Requirements:\n    - random\n    - time\n    - matplotlib.pyplot\n\n    Example:\n    >>> my_list = [2, 3, 5]\n    >>> time_taken, ax = task_func(my_list)\n    >>> print(type(time_taken))  # Example output: <class 'float'>\n    <class 'float'>\n    >>> ax.get_title()  # Returns 'Histogram Random Numbers'\n    'Histogram Random Numbers'\n    \"\"\"\n    if not isinstance(my_list, list):\n        raise TypeError(\"Input 'my_list' must be a list.\")\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"Input 'my_list' must contain only numeric elements.\")\n    random_seed(seed)\n    start_time = time.time()\n    total_sum = sum(my_list)\n    if total_sum > size:\n        total_sum = size\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    end_time = time.time()\n    time_taken = end_time - start_time\n    fig, ax = plt.subplots()\n    ax.hist(random_numbers, bins=100, edgecolor='black')\n    ax.set_title('Histogram Random Numbers')\n    ax.set_xlabel('Number')\n    ax.set_ylabel('Frequency')\n    return time_taken, ax\n"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    \"\"\"\n    Scrape first table from web page extract data into Pandas DataFrame.\n\n    This function scrapes first table found on specified web page URL extracts data into DataFrame,\n    where each row DataFrame corresponds table row (<tr>) from web page, each column represents\n    data contained within table data elements (<td>) row. DataFrame's columns named after the\n    table's header row (<th> elements), if present. If table lacks headers, DataFrame's columns remain unnamed.\n\n    Parameters:\n    - url (str): URL webpage scrape. Defaults 'http://example.com'.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing scraped table data, with rows corresponding table rows and\n      columns named after table headers, if available.\n\n    Raises:\n    - ConnectionError: If issue connecting URL.\n    - requests.HTTPError: If HTTP request URL fails.\n    - ValueError: If no table data found on page or if page content cannot parsed.\n\n    Note: Assumes webpage contains at least one table attempts parse first table encountered.\n\n    Requirements:\n    - pandas\n    - requests\n    - bs4\n\n    Example:\n    >>> df = task_func('https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)')\n    >>> print(df)\n                                                       0\n    0                                                   \n    1  Largest economies world by GDP (nominal...\n    \"\"\"\n    try:\n        # Send HTTP request to URL\n        response = requests.get(url)\n        # Check if request was successful\n        response.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        # Handle request exceptions\n        print(f\"Request failed: {e}\")\n        return None\n\n    try:\n        # Parse HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.text, 'html.parser')\n        # Find first table on page\n        table = soup.find('table')\n        if table is None:\n            # If no table found, raise ValueError\n            raise ValueError(\"No table data found on page\")\n        # Extract table data into DataFrame\n        df = pd.read_html(str(table))[0]\n        return df\n    except Exception as e:\n        # Handle any other exceptions\n        print(f\"Error parsing page content: {e}\")\n        return None\n"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draw histograms numeric columns DataFrame return plots.\n\n    Each histogram represents distribution values one numeric column,\n    with column name as plot title, 'Value' as x-axis label, 'Frequency' as y-axis label.\n\n    Parameters:\n    - df (DataFrame): DataFrame containing data.\n\n    Returns:\n    - list: A list Matplotlib Axes objects, each representing histogram for numeric column.\n\n    Raises:\n    - ValueError: If input not non-empty DataFrame or if no numeric columns DataFrame.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'A': np.random.normal(0, 1, 100), 'B': np.random.exponential(1, 100)})\n    >>> axes = task_func(df)\n    >>> for ax axes:\n    ...     plt.show()\n    \"\"\"\n    # Check if input is non-empty DataFrame\n    if df.empty:\n        raise ValueError(\"Input must be non-empty DataFrame\")\n\n    # Check if DataFrame contains numeric columns\n    if not df.select_dtypes(include=[np.number]).empty:\n        # Get numeric columns\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n\n        # Initialize list to store Axes objects\n        axes = []\n\n        # Iterate over numeric columns\n        for col in numeric_cols:\n            # Create histogram for current column\n            ax = df[col].hist(bins=10, figsize=(8, 6))\n\n            # Set plot title and labels\n            ax.set_title(col)\n            ax.set_xlabel('Value')\n            ax.set_ylabel('Frequency')\n\n            # Append Axes object to list\n            axes.append(ax)\n\n        # Return list of Axes objects\n        return axes\n    else:\n        raise ValueError(\"DataFrame must contain numeric columns\")\n"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    \"\"\"\n    Scans specified IP address range checks if specified port open on each IP.\n    function returns dictionary with IP addresses as keys boolean indicating\n    port's status (True if open, False otherwise).\n\n    Parameters:\n        ip_range (str): IP address range scan, CIDR notation.\n        port (int): port number check on each IP range.\n\n    Returns:\n        dict: A dictionary mapping IP addresses their port status (True if open).\n\n    Examples:\n    >>> result = task_func('192.168.0.0/24', 80)\n    >>> isinstance(result, dict)\n    True\n    >>> all(isinstance(key, str) isinstance(value, bool) for key, value result.items())\n    True\n\n    Requirements:\n    - socket\n    - ipaddress.IPv4Network\n    - threading.Thread\n    \"\"\"\n    # Create a dictionary to store the results\n    result = {}\n\n    # Create a socket object\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    # Set a timeout of 1 second\n    sock.settimeout(1)\n\n    # Iterate over each IP in the range\n    for ip in IPv4Network(ip_range):\n        # Try to connect to the port on the current IP\n        try:\n            sock.connect((str(ip), port))\n            # If the connection is successful, add the IP to the result dictionary with a value of True\n            result[str(ip)] = True\n        except socket.error:\n            # If the connection fails, add the IP to the result dictionary with a value of False\n            result[str(ip)] = False\n\n    # Close the socket\n    sock.close()\n\n    # Return the result dictionary\n    return result\n"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(log_file):\n    \"\"\"\n    Extracts logging information such as message type, timestamp, message itself from log file and\n    stores data CSV format. This utility ideal for converting plain text logs into more s\n    tructured format can easily analyzed. log format 'TYPE: [TIMESTAMP (YYYY-MM-DD HH:MM:SS)] - MESSAGE'.\n\n    Parameters:\n    log_file (str): file path log file needs parsed.\n\n    Returns:\n    str: file path newly created CSV file contains structured log data.\n\n    Requirements:\n    - re\n    - pandas\n    - datetime\n\n    Raises:\n    ValueError: If timestamp log entry invalid or if no valid log entries found.\n\n    Example:\n    >>> output_path = task_func('server.log')\n    >>> print(output_path)\n    log_data.csv\n    \"\"\"\n    # Regular expression pattern to match log entry\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.*)'\n\n    # Read log file\n    with open(log_file, 'r') as f:\n        log_entries = f.readlines()\n\n    # Initialize list to store structured log data\n    log_data = []\n\n    # Iterate over each log entry\n    for entry in log_entries:\n        # Use regular expression to extract log entry components\n        match = re.match(pattern, entry)\n        if match:\n            # Extract log entry components\n            log_type = match.group(1)\n            timestamp = datetime.strptime(match.group(2), '%Y-%m-%d %H:%M:%S')\n            message = match.group(3).strip()\n\n            # Append structured log data to list\n            log_data.append({\n                'log_type': log_type,\n                'timestamp': timestamp,\n                'message': message\n            })\n        else:\n            # Raise ValueError if no valid log entry found\n            raise ValueError('No valid log entries found in log file.')\n\n    # Create DataFrame from structured log data\n    df = pd.DataFrame(log_data)\n\n    # Save DataFrame to CSV file\n    output_path = 'log_data.csv'\n    df.to_csv(output_path, index=False)\n\n    return output_path\n\n"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(text, rwidth=0.8):\n    \"\"\"\n    Analyzes visualizes distribution word lengths text. function generates histogram subplot,\n    facilitates understanding how word lengths vary within provided text.\n\n    Parameters:\n    text (str): text string from word lengths will calculated.\n    rwidth (float, optional): Specifies relative bar width histogram. Defaults 0.8.\n\n    Returns:\n    matplotlib.axes.Axes: An Axes object containing histogram word lengths.\n\n    Requirements:\n    - re\n    - matplotlib\n    - numpy\n\n    Note:\n    If no words input text, or all words filtered out, histogram will empty as no\n    bins will created.\n\n    Example:\n    >>> import matplotlib\n    >>> ax = task_func('Hello world, this test sentence.')\n    >>> isinstance(ax, matplotlib.axes.Axes)\n    True\n    \"\"\"\n    # Regular expression to find all words in text\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # If no words found, return empty Axes\n    if not words:\n        return plt.Axes()\n\n    # Calculate word lengths\n    lengths = [len(word) for word in words]\n\n    # Create histogram\n    ax = plt.hist(lengths, bins=np.arange(0, max(lengths) + 2) - 0.5, rwidth=rwidth)\n\n    return ax\n\n"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\n\n\ndef task_func(df):\n    \"\"\"\n    Extracts articles whose titles contain specific case-insensitive keywords (\"like\" or \"what\") from DataFrame analyzes\n    frequency each word content these articles, excluding punctuation.\n\n    Parameters:\n    df (DataFrame): DataFrame containing columns 'Title' 'Content' with article data.\n\n    Returns:\n    dict: A dictionary with keys as words values as their corresponding frequency, excluding punctuation marks.\n\n    Requirements:\n    - re\n    - nltk\n    - string\n\n    Raises:\n    ValueError: If DataFrame empty or does not contain necessary columns 'Title' 'Content'.\n\n    Example:\n    >>> import pandas as pd\n    >>> data = {'Title': ['What happening', 'Nothing special'], 'Content': ['Like what see?', 'Just normal text.']}\n    >>> df = pd.DataFrame(data)\n    >>> task_func(df)\n    {'Like': 1, 'what': 1, 'you': 1, 'see': 1}\n    \"\"\"\n    if not df.empty and 'Title' in df.columns and 'Content' in df.columns:\n        # Filter articles with specific keywords in titles\n        filtered_df = df[df['Title'].str.contains(r'\\b(like|what)\\b', case=False)]\n        \n        # Join content of filtered articles into a single string\n        content = ' '.join(filtered_df['Content'])\n        \n        # Tokenize content into words\n        words = nltk.word_tokenize(content)\n        \n        # Remove punctuation from words\n        words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n        \n        # Count frequency of each word\n        word_freq = nltk.FreqDist(words)\n        \n        return dict(word_freq)\n    else:\n        raise ValueError(\"DataFrame is empty or does not contain necessary columns 'Title' 'Content'.\")\n\n"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\n\ndef task_func(dataframe, text_column):\n    \"\"\"\n    Prepares transforms text data from specified column DataFrame by removing stopwords, numbers,\n    punctuation, subsequently applying vectorization process convert text into numeric format suitable\n    for analysis.\n\n    Parameters:\n    dataframe (DataFrame): A pandas DataFrame containing text data.\n    text_column (str): name column from text will processed.\n\n    Returns:\n    DataFrame: Returns DataFrame with each word (after preprocessing) as column their count as rows.\n\n    Requirements:\n    - pandas\n    - re\n    - sklearn\n\n    Example:\n    >>> df = pd.DataFrame({'text': ['This test.', 'Python cool!', 'nltk sklearn useful for text analysis.']})\n    >>> result = task_func(df, 'text')\n    >>> print(result.to_string(index=False))\n     analysis  cool  nltk  python  sklearn  test  text  useful\n            0     0     0       0        0     1     0       0\n            0     1     0       1        0     0     0       0\n            1     0     1       0        1     0     1       1\n    \"\"\"\n    # Remove stopwords, numbers, punctuation\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[0-9]+', '', x))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n    dataframe[text_column] = dataframe[text_column].apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n\n    # Vectorize text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column])\n\n    # Convert vectorized data into DataFrame\n    result = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return result\n\n"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    \"\"\"\n    Create GeoPandas DataFrame for list cities with randomly generated coordinates based on specified ranges.\n\n    Parameters:\n    dic (dict): Dictionary with 'Lon' 'Lat' keys, each tuple (min, max) for coordinate range. \n                Default: {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    cities (list): List city names. Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n\n    Returns:\n    GeoDataFrame: A GeoPandas DataFrame containing 'City' 'Coordinates' (Point objects).\n\n    Raises:\n    ValueError: If 'Lon' or 'Lat' keys missing dictionary, or if their values not tuples.\n\n    Requirements:\n    - numpy\n    - geopandas\n    - shapely.geometry\n\n    Example:\n    >>> dic = {'Lon': (-180, 180), 'Lat': (-90, 90)}\n    >>> gdf = task_func(dic)\n    \"\"\"\n    # Check if dictionary has required keys and values are tuples\n    if not all(key in dic for key in ['Lon', 'Lat']):\n        raise ValueError(\"Dictionary must have 'Lon' and 'Lat' keys.\")\n    if not all(isinstance(value, tuple) for value in dic.values()):\n        raise ValueError(\"Dictionary values must be tuples.\")\n\n    # Generate random coordinates for each city\n    coordinates = []\n    for city in cities:\n        lon = np.random.uniform(*dic['Lon'])\n        lat = np.random.uniform(*dic['Lat'])\n        coordinates.append(Point(lon, lat))\n\n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame(\n        data={'City': cities, 'Coordinates': coordinates},\n        geometry='Coordinates',\n        crs='EPSG:4326'\n    )\n\n    return gdf\n"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    \"\"\"\n    Generate weather report for specified cities at given UTC datetime.\n\n    Parameters:\n    - utc_datetime (datetime): UTC datetime for weather report generated, with tzinfo set UTC.\n    - cities (list str): Cities for weather report generated. Default: ['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']\n    - weather_conditions (list str): Possible weather conditions choose from for report. Default: ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\n    - timezones (dict): A mapping city names their respective timezones. Default provided for default cities.\n    - seed (int): seed value for random number generation ensure reproducibility. Default: 42\n\n    Returns:\n    - pandas.DataFrame: A DataFrame containing weather report. Columns include:\n      - 'City': name city.\n      - 'Local Time': local time weather report for city, formatted as 'YYYY-MM-DD HH:MM:SS ZZZ' (ZZZ timezone abbreviation).\n      - 'Weather Condition': weather condition city at given local time.\n\n    Raises:\n    - ValueError: If utc_datetime not datetime object or if other parameters not expected format.\n\n    Requirements:\n    - pandas\n    - pytz\n    - datetime\n    - random\n\n    Example:\n    >>> utc_time = datetime(2023, 1, 1, 12, 0, 0, tzinfo=pytz.UTC)\n    >>> report = task_func(utc_time)\n    >>> print(report)\n           City                Local Time Weather Condition\n    0  New York   2023-01-01 07:00:00 EST             Sunny\n    1    London   2023-01-01 12:00:00 GMT             Sunny\n    2   Beijing   2023-01-01 20:00:00 CST             Rainy\n    3     Tokyo   2023-01-01 21:00:00 JST            Cloudy\n    4    Sydney  2023-01-01 23:00:00 AEDT            Cloudy\n    \"\"\"\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n\n    set_seed(seed)\n    weather_report = []\n\n    for city in cities:\n        timezone = timezones[city]\n        local_time = utc_datetime.astimezone(pytz.timezone(timezone))\n        weather_condition = random.choice(weather_conditions)\n        weather_report.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    return pd.DataFrame(weather_report)\n"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate draw random sequence \"elements\" number steps. steps either \n    -1 or 1, sequence plotted as random walk. Returns descriptive statistics \n    random walk plot random walk. descriptive statistics include \n    count, mean, standard deviation, minimum, 5th percentile, 25th percentile, median, 75th \n    percentile, 95th percentile maximum.\n\n    Parameters:\n    elements (int): number steps random walk.\n    seed (int): seed for random number generator. Default 0.\n\n    Returns:\n    dict: A dictionary containing descriptive statistics random walk.\n    matplotlib.axes.Axes: Axes object with plotted random walk.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - pandas\n\n    Raises:\n    ValueError: If elements not positive integer.\n\n    Example:\n    >>> stats, ax = task_func(1000)\n    >>> print(stats)\n    {'count': 1000.0, 'mean': 18.18, 'std': 9.516415405086212, 'min': -5.0, '5%': 1.0, '25%': 11.0, '50%': 20.0, '75%': 26.0, '95%': 31.0, 'max': 36.0}\n    \"\"\"\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"Elements must be a positive integer.\")\n\n    np.random.seed(seed)\n    steps = np.random.choice([-1, 1], size=elements)\n    walk = np.cumsum(steps)\n    stats = pd.Series(walk).describe()\n    ax = plt.plot(walk)\n    plt.title(\"Random Walk\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Position\")\n    plt.show()\n    return stats.to_dict(), ax\n\n"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    \"\"\"\n    Download keep zip file from URL, extract its contents specified directory, return list extracted files.\n\n    Parameters:\n    url (str): URL zip file download.\n    destination_directory (str): directory where contents zip file will extracted.\n    headers (dict, optional): Custom headers included request. Defaults {'accept': 'application/octet-stream'}.\n\n    Returns:\n    list: A list filenames extracted files.\n\n    Requirements:\n    - requests\n    - os\n    - zipfile\n\n    Example:\n    >>> extracted_files = task_func(\"https://example.com/data.zip\", \"/path/to/destination\")\n    >>> print(extracted_files)\n    ['file1.txt', 'file2.csv']\n    \"\"\"\n    # Download zip file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n\n    # Extract zip file\n    with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Get list of extracted files\n    extracted_files = os.listdir(destination_directory)\n\n    return extracted_files\n"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generate random RGB image view it.\n\n    Parameters:\n    - seed (int, optional): Random seed for reproducibility. Default 42.\n    - image_size (tuple, optional): Size generated image (height, width, channels). Default (100, 100, 3).\n    - range_low (int, optional): Lower bound random range. Default 0.\n    - range_high (int, optional): Upper bound random range. Default 255.\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object plot.\n    - image (numpy.ndarray): numpy array generated image.\n\n    Raises:\n    - ValueError: If range_low not less than range_high.\n\n    Requirements:\n    - random\n    - numpy\n    - opencv\n    - matplotlib.pyplot\n\n    Example:\n    >>> ax, image = task_func()\n    \"\"\"\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set random seed for reproducibility\n    random.seed(seed)\n\n    # Generate random image\n    image = np.random.randint(range_low, range_high, image_size)\n\n    # Convert image to BGR format for OpenCV\n    image = image[:, :, ::-1]\n\n    # Display image\n    ax = plt.imshow(image)\n    plt.show()\n\n    return ax, image\n"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    \"\"\"\n    Creates MxN matrix from list L, normalizes based on sound pressure level\n    (SPL) specified audio file, generates spectrogram from matrix.\n\n    Parameters:\n        L (list): A list numbers form matrix.\n        M (int): number rows matrix.\n        N (int): number columns matrix.\n        audio_file (str): path audio file for SPL calculation.\n\n    Returns:\n        numpy.ndarray: normalized MxN matrix.\n        matplotlib.figure.Figure: figure object for generated spectrogram.\n\n    Raises:\n        FileNotFoundError: If specified audio file does not exist.\n\n    Notes:\n        spectrogram generated based on amplitude normalized matrix, with the\n        sound pressure level (SPL) calculated from audio file. SPL calculated using \n        formula:\n        \n        SPL = 20 * log10(sqrt(mean(data^2)))\n        \n        where 'data' audio data read from file.\n\n        spectrogram displayed with logarithmic scale for frequency linear scale for time, \n        with SPL used adjust amplitude displayed spectrogram.\n\n    Requirements:\n    - numpy\n    - os\n    - soundfile\n    - librosa\n    - matplotlib\n\n    Examples:\n    >>> matrix = task_func([i for i range(100)], 10, 10, 'audio.wav') # Requires 'audio.wav' exist\n    >>> matrix.shape\n    (10, 10)\n    >>> isinstance(matrix, np.ndarray)\n    True\n    \"\"\"\n    # Check if audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"Audio file '{audio_file}' not found.\")\n\n    # Read audio file\n    data, _ = librosa.load(audio_file)\n\n    # Calculate SPL\n    spl = 20 * np.log10(np.sqrt(np.mean(data**2)))\n\n    # Create MxN matrix from list L\n    matrix = np.array(L).reshape(M, N)\n\n    # Normalize matrix based on SPL\n    matrix = matrix / (10 ** (spl / 20))\n\n    # Generate spectrogram\n    fig, ax = plt.subplots()\n    ax.imshow(librosa.amplitude_to_db(np.abs(librosa.stft(matrix)), ref=np.max), cmap='inferno', origin='lower')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Spectrogram')\n    plt.show()\n\n    return matrix, fig\n"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\ndef task_func(original):\n    \"\"\"\n    Given list tuples, extract numeric values, compute basic statistics, \n    generate histogram with overlaid probability density function (PDF).\n\n    Parameters:\n    original (list tuples): Input list where each tuple's second element numeric value.\n\n    Returns:\n    np.array: A numpy array extracted numeric values.\n    dict: Basic statistics for array including mean, standard deviation, minimum, maximum.\n    Axes: A matplotlib Axes object showing histogram with overlaid PDF. histogram \n          plotted with density set True, alpha as 0.6, bins set 'auto' for automatic bin selection.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats\n\n    Example:\n    >>> original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    >>> arr, stats, ax = task_func(original)\n    >>> print(arr)\n    [1 2 3 4]\n    >>> print(stats)\n    {'mean': 2.5, 'std': 1.118033988749895, 'min': 1, 'max': 4}\n    \"\"\"\n    # Extract numeric values from tuples\n    arr = np.array([t[1] for t in original])\n\n    # Compute basic statistics\n    stats = {\n        'mean': np.mean(arr),\n        'std': np.std(arr),\n        'min': np.min(arr),\n        'max': np.max(arr)\n    }\n\n    # Generate histogram with overlaid PDF\n    n, bins, patches = plt.hist(arr, bins='auto', density=True, alpha=0.6)\n    # Fit a normal distribution to the data\n    params = stats.norm.fit(arr)\n    # Plot the PDF\n    x = np.linspace(min(arr), max(arr), 100)\n    y = stats.norm.pdf(x, *params)\n    plt.plot(x, y, 'r--')\n\n    return arr, stats, plt.gca()\n\n"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\n\ndef task_func(original):\n    \"\"\"\n    Create numeric array from \"original\" list, normalize array, draw original normalized arrays.\n    \n    function will plot original normalized arrays using matplotlib.\n\n    Parameters:\n    original (list): original list with tuples unzipped into numpy array.\n\n    Returns:\n    np.array: A numpy array for original data.\n    np.array: Normalized array.\n    matplotlib.axes.Axes: Axes object with plotted data.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.preprocessing\n\n    Example:\n    >>> original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]\n    >>> arr, norm_arr, ax = task_func(original)\n    >>> print(arr)\n    [1 2 3 4]\n    >>> print(norm_arr)\n    [0.18257419 0.36514837 0.54772256 0.73029674]\n    \"\"\"\n    # Unzip original list into numpy array\n    arr = np.array([x[1] for x in original])\n    \n    # Normalize array\n    norm_arr = preprocessing.normalize([arr])[0]\n    \n    # Create figure and axis\n    fig, ax = plt.subplots()\n    \n    # Plot original normalized arrays\n    ax.plot(norm_arr)\n    \n    return arr, norm_arr, ax\n\n"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\n\ndef task_func(data, sample_rate=8000):\n    \"\"\"\n    Given dictionary \"data\", this function performs following operations:\n    1. Adds new key \"a\" with value 1 dictionary.\n    2. Generates signal based on values \"data\".\n    3. Runs Fast Fourier Transform (FFT) on signal.\n    4. Plots returns FFT signal.\n    \n    Parameters:\n    data (dict): input data as dictionary.\n\n    Returns:\n    tuple: A tuple containing:\n        - ndarray: FFT signal.\n        - Axes: plot FFT.\n\n    Requirements:\n    - numpy\n    - scipy.fftpack\n    - matplotlib\n\n    Example:\n    >>> data = {'key1': 1, 'key2': 2, 'key3': 3}\n    >>> fft, ax = task_func(data)\n    \"\"\"\n    # Add new key \"a\" with value 1 to dictionary\n    data['a'] = 1\n\n    # Generate signal based on values in dictionary\n    signal = np.array([data[key] for key in data.keys()])\n\n    # Run Fast Fourier Transform (FFT) on signal\n    fft_signal = fftpack.fft(signal)\n\n    # Plot FFT signal\n    ax = plt.plot(np.abs(fft_signal))\n    plt.show()\n\n    return fft_signal, ax\n\n"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\n\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\ndef task_func():\n    \"\"\"\n    Creates HTTP POST request handler for processing incoming data. data expected\n    JSON format with key 'data'. handler responds with 200 success message\n    if data valid, or error message otherwise. \n    type response can retrieved as 'content-type' length response as 'content-length'.\n    two types error messages: 'Content-Type header not application/json' 'No data key request'.\n\n    Returns:\n        function: A class handles HTTP POST requests validates incoming data.\n\n    Requirements:\n    - cgi\n    - http.server\n    - json\n\n    Notes:\n        If 'content-type' header not 'application/json', indicating \n            client sent request with unsupported format. This condition sends a\n            400 Bad Request response client with message \"Content-Type header \n            not application/json\".\n        If JSON object does not contain 'data' key, leading 400 Bad\n            Request response with message \"No data key request\".\n        If request body does not contain valid JSON, resulting in\n            400 Bad Request response with message \"Invalid JSON\".\n     \n    Examples:\n    >>> handler = task_func()\n    >>> isinstance(handler, type)\n    True\n    >>> issubclass(handler, http.server.BaseHTTPRequestHandler)\n    True\n    \"\"\"\n    class RequestHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers.get('content-length', 0))\n            body = self.rfile.read(content_length)\n            try:\n                data = json.loads(body)\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                self.wfile.write(json.dumps(ERROR_RESPONSE).encode())\n                return\n            if 'data' not in data:\n                self.send_response(400)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                self.wfile.write(json.dumps(ERROR_RESPONSE).encode())\n                return\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode())\n    return RequestHandler\n"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    \"\"\"\n    Creates HTTP POST request handler processes incoming email data sends\n    email. email data must JSON object with 'subject', 'message', 'to' keys.\n    type response can retrieved as 'content-type' length response as 'content-length'.\n    \n    Parameters:\n        smtp_server (str): SMTP server address.\n        smtp_port (int): SMTP server port.\n        smtp_username (str): SMTP username.\n        smtp_password (str): SMTP password.\n\n    Returns:\n        function: A class handles HTTP POST requests sends emails based on\n                  provided data.\n\n    Requirements:\n    - cgi\n    - http.server\n    - smtplib\n    - email.mime.text.MIMEText\n    - json\n\n    Raises:\n        JSONDecodeError: If email data not valid JSON. This results 400 Bad Request response.\n        ValueError: If 'subject', 'message', or 'to' keys missing from email data, \n                    leading 400 Bad Request response.\n        smtplib.SMTPAuthenticationError: If authentication issue with SMTP server. \n                                         This communicated client with 535 Authentication Failed response.\n\n    Examples:\n    >>> handler = task_func('smtp.example.com', 587, 'user@example.com', 'password')\n    >>> isinstance(handler, type)\n    True\n    >>> issubclass(handler, http.server.BaseHTTPRequestHandler)\n    True\n    \"\"\"\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['content-length'])\n            body = self.rfile.read(content_length)\n            try:\n                data = json.loads(body.decode('utf-8'))\n            except json.JSONDecodeError:\n                self.send_response(400)\n                self.end_headers()\n                return\n            if not all(key in data for key in ('subject', 'message', 'to')):\n                self.send_response(400)\n                self.end_headers()\n                return\n            msg = MIMEText(data['message'])\n            msg['Subject'] = data['subject']\n            msg['To'] = data['to']\n            server = smtplib.SMTP(smtp_server, smtp_port)\n            server.starttls()\n            server.login(smtp_username, smtp_password)\n            server.sendmail(smtp_username, data['to'], msg.as_string())\n            server.quit()\n            self.send_response(200)\n            self.end_headers()\n\n    return EmailHandler\n"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    \"\"\"\n    Count number words .txt files within specified directory, \n    export counts JSON file, return total number words.\n\n    Parameters:\n    filename (str): name output JSON file.\n    directory (str): directory where .txt files located.\n\n    Returns:\n    int: total number words .txt files\n\n    Requirements:\n    - collections.Counter\n    - os\n    - json\n\n    Example:\n    >>> with open(\"./testdir/single_file.txt\",\"r\") as f: print f.read()\n    hello world hello\n    >>> count = task_func('single_file.txt', './testdir/')\n    >>> print(count)\n    3\n    \"\"\"\n    # Initialize counter\n    counter = Counter()\n\n    # Iterate over all files in directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if file is .txt\n            if file.endswith(\".txt\"):\n                # Open file, read content, split into words, update counter\n                with open(os.path.join(root, file), \"r\") as f:\n                    counter.update(f.read().split())\n\n    # Export counter to JSON file\n    with open(filename, \"w\") as f:\n        json.dump(dict(counter), f)\n\n    # Return total number words\n    return sum(counter.values())\n"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes pandas DataFrame by splitting lists 'Value' column into separate columns, \n    calculates Pearson correlation coefficient between these columns, optionally visualizes \n    correlation matrix using heatmap.\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame with two columns: 'Date' 'Value'. \n                    'Date' column contains dates, 'Value' column contains lists numbers.\n    plot (bool): Optional; if True, displays heatmap correlation matrix returns it.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing correlation coefficients among lists 'Value' column.\n    Axes (optional): A matplotlib Axes object containing heatmap plot, returned if 'plot' True.\n\n    Note:\n    - This function use \"Correlation Heatmap\" as title heatmap plot\n\n    Raises:\n    - If DataFrame input empty or have invalid 'Value', this function will raise ValueError.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame([['2021-01-01', [8, 10, 12]], ['2021-01-02', [7, 9, 11]]], columns=['Date', 'Value'])\n    >>> corr_df = task_func(df)\n    >>> print(corr_df[0][0])\n    1.0\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"Input DataFrame is empty\")\n\n    # Check if 'Value' column contains lists\n    if not all(isinstance(x, list) for x in df['Value']):\n        raise ValueError(\"Invalid 'Value' column. It should contain lists.\")\n\n    # Split lists in 'Value' column into separate columns\n    df = pd.concat([df, df['Value'].apply(pd.Series)], axis=1)\n\n    # Drop 'Value' column\n    df = df.drop('Value', axis=1)\n\n    # Calculate Pearson correlation coefficient\n    corr_df = df.corr()\n\n    # If plot is True, display heatmap correlation matrix\n    if plot:\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(corr_df, annot=True, cmap='coolwarm', square=True)\n        plt.title('Correlation Heatmap')\n        plt.show()\n        return plt.gca()\n\n    return corr_df\n"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    \"\"\"\n    Create report on students' grades different subjects calculate average grade for each student subject.\n    \n    Parameters:\n    additional_fields (list string, optional): additional list student subjects not duplicate with constants (default = [])\n\n    Returns:\n    DataFrame: A pandas DataFrame with columns being subjects, each student's grades, their average grades. \n               DataFrame also includes average grade per subject.\n\n    Note:\n    - This function does not take input parameters generates report based on predefined constants additional fields from input (if any).\n    - This function use 'Average' as row name for average grade for each subject.\n    - This function use 'Average Grade' as column name for average grade for each student\n    - Grade each subject between 0 100.\n\n    Requirements:\n    - pandas\n    - random\n    - statistics.mean\n\n    Example:\n    >>> random.seed(0)\n    >>> report = task_func(['Computer Science', 'Geography'])\n    >>> print(report.columns)\n    Index(['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History',\n           'Computer Science', 'Geography', 'Average Grade'],\n          dtype='object')\n    \"\"\"\n    # Generate random grades for each student and subject\n    grades = [[random.randint(0, 100) for _ in range(len(FIELDS))] for _ in range(len(STUDENTS))]\n    \n    # Create a DataFrame with the generated grades\n    df = pd.DataFrame(grades, index=STUDENTS, columns=FIELDS)\n    \n    # Add additional fields if provided\n    for field in additional_fields:\n        if field not in FIELDS:\n            df[field] = [random.randint(0, 100) for _ in range(len(STUDENTS))]\n            FIELDS.append(field)\n    \n    # Calculate average grade for each student and subject\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate average grade for each subject\n    avg_grades = df.mean(axis=0)\n    avg_grades.name = 'Average'\n    df = pd.concat([df, avg_grades.to_frame().T], axis=0)\n    \n    return df\n"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    \"\"\"\n    Generates CSV file containing simulated data for 100 people, including name, age, height, weight. \n    It also calculates appends average age, height, weight at end file.\n\n    Parameters:\n    filename (str): name CSV file created.\n\n    Returns:\n    str: path created CSV file.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - statistics.mean\n\n    Example:\n    >>> random.seed(0)\n    >>> filename = 'people_report.csv'\n    >>> path = task_func(filename)\n    >>> os.path.exists(path)\n    True\n    \"\"\"\n    # Create a list of names\n    names = ['John', 'Alice', 'Bob', 'Eve', 'Mike', 'Emma', 'Tom', 'Lily', 'Sam', 'Olivia']\n    # Create a list of ages\n    ages = [random.randint(18, 80) for _ in range(PEOPLE_COUNT)]\n    # Create a list of heights\n    heights = [random.randint(150, 200) for _ in range(PEOPLE_COUNT)]\n    # Create a list of weights\n    weights = [random.randint(50, 100) for _ in range(PEOPLE_COUNT)]\n\n    # Create a list of lists with the data\n    data = [[name, age, height, weight] for name, age, height, weight in zip(names * (PEOPLE_COUNT // len(names) + 1), ages, heights, weights)]\n\n    # Create a CSV file\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    # Calculate and append average age, height, weight\n    with open(filename, 'a', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Average Age', mean(ages), 'Average Height', mean(heights), 'Average Weight', mean(weights)])\n\n    return os.path.abspath(filename)\n"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\n\ndef task_func(directory):\n    \"\"\"\n    Organize files directory based on first text not enclosed square brackets.\n    Move files subdirectories named after this text. If no matching text found,\n    file not moved.\n\n    Parameters:\n    directory (str): directory path.\n\n    Returns:\n    tuple: \n        - str: directory path with organized files.\n        - dict: A dictionary where keys created subdirectories values lists files moved them.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n    - datetime\n\n    Example:\n    >>> import tempfile\n    >>> temp_dir = tempfile.mkdtemp()\n    >>> create_test_directory(temp_dir, {\"file1.txt\": \"subdir1[content]\", \"file2.txt\": \"subdir1[content]\", \"file3.txt\": \"subdir2[content]\"})\n    >>> dir, files = task_func(temp_dir)\n    >>> files['subdir2'][0].startswith('file3_')\n    True\n    \"\"\"\n    # Create a dictionary to store the files moved to each subdirectory\n    files = {}\n    # Iterate over each file in the directory\n    for filename in os.listdir(directory):\n        # Construct the full path to the file\n        filepath = os.path.join(directory, filename)\n        # Check if the file is a regular file (not a directory)\n        if os.path.isfile(filepath):\n            # Open the file and read its contents\n            with open(filepath, 'r') as file:\n                # Use regular expression to find the first text not enclosed in square brackets\n                match = re.search(r'\\[(.*?)\\]', file.read())\n                if match:\n                    # If a match is found, extract the text\n                    text = match.group(1)\n                    # Construct the subdirectory path\n                    subdir = os.path.join(directory, text)\n                    # Check if the subdirectory exists\n                    if not os.path.exists(subdir):\n                        # If not, create it\n                        os.makedirs(subdir)\n                    # Move the file to the subdirectory\n                    shutil.move(filepath, subdir)\n                    # Add the file to the dictionary\n                    if text not in files:\n                        files[text] = []\n                    files[text].append(filename)\n    # Return the directory path and the dictionary of files moved\n    return directory, files\n\n"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\n\n\ndef task_func(file_list):\n    \"\"\"\n    Run files from list files as subprocesses at same time.\n    \n    Parameters:\n    - file_list (list str): List files name run.\n\n    Returns:\n    list: exit codes subprocesses.\n\n    Requirements:\n    - subprocess\n    - time\n    - threading\n\n    Example:\n    >>> task_func([\"task_func_data/file1.bat\", \"task_func_data/file2.bat\"])\n    [0, 0]\n    \"\"\"\n    # Create list to store exit codes\n    exit_codes = []\n    # Create list to store processes\n    processes = []\n    # Create lock to avoid concurrent access\n    lock = threading.Lock()\n\n    # Iterate over files\n    for file in file_list:\n        # Create process\n        process = subprocess.Popen([file], shell=True)\n        # Add process to list\n        processes.append(process)\n        # Add lock to list\n        lock.acquire()\n        # Add exit code to list\n        exit_codes.append(0)\n        # Release lock\n        lock.release()\n\n    # Wait for processes to finish\n    for process in processes:\n        # Wait for process to finish\n        process.wait()\n        # Get exit code\n        exit_code = process.returncode\n        # Add exit code to list\n        lock.acquire()\n        exit_codes[processes.index(process)] = exit_code\n        # Release lock\n        lock.release()\n\n    # Return exit codes\n    return exit_codes\n\n"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    \"\"\"\n    Find run all .bat files given directory, returning their file names exit codes.\n\n    Parameters:\n    directory_path (str): path directory search for .bat files.\n\n    Returns:\n    list tuples: A list where each tuple contains file name its exit code. \n                    exit code None if file could not executed.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - glob\n\n    Example:\n    >>> task_func(\"path/to/directory\")\n    [(\"file1.bat\", 0), (\"file2.bat\", 1)]\n    \"\"\"\n    # Get all .bat files in the directory\n    bat_files = glob.glob(os.path.join(directory_path, \"*.bat\"))\n\n    # Initialize list to store results\n    results = []\n\n    # Iterate over each .bat file\n    for file in bat_files:\n        # Get file name\n        file_name = os.path.basename(file)\n\n        # Try to run the .bat file\n        try:\n            # Use subprocess to run the .bat file\n            # The stdout and stderr are redirected to /dev/null to suppress output\n            # The check_call function will raise a CalledProcessError if the command returns a non-zero exit code\n            subprocess.check_call([file], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n            # If the command runs successfully, append the file name and exit code to the results list\n            results.append((file_name, 0))\n        except subprocess.CalledProcessError as e:\n            # If the command returns a non-zero exit code, append the file name and exit code to the results list\n            results.append((file_name, e.returncode))\n        except Exception as e:\n            # If any other exception occurs, append the file name and None to the results list\n            results.append((file_name, None))\n\n    # Return the results list\n    return results\n"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    This function takes pandas DataFrame column name as input generates two subplots one matplotlib figure:\n    first subplot histogram (with kernel density estimate for numerical data), second box plot,\n    representing distribution values specified column.\n\n    Parameters:\n    df (DataFrame): Input DataFrame with numerical or categorical data.\n    col (str): name column plotted. This column should exist DataFrame contain numerical or categorical data.\n\n    Returns:\n    matplotlib.figure.Figure: A matplotlib figure object containing histogram box plot.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n\n    Raises:\n    - input df must DataFrame, not empty, must contain specified column, if not, function will raise ValueError.\n   \n\n    Example:\n    >>> df = pd.DataFrame({'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\n    >>> fig = task_func(df, 'value')\n    >>> type(fig)\n    <class 'matplotlib.figure.Figure'>\n    >>> plt.close()\n    >>> df = pd.DataFrame({'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']})\n    >>> fig = task_func(df, 'category')\n    >>> type(fig)\n    <class 'matplotlib.figure.Figure'>\n    >>> len(fig.axes)\n    2\n    >>> plt.close()\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input must be pandas DataFrame\")\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if col not in df.columns:\n        raise ValueError(\"DataFrame does not contain specified column\")\n    if df[col].dtype == 'object':\n        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n        sns.countplot(x=df[col], ax=ax[0])\n        ax[0].set_title('Count plot')\n        ax[0].set_xlabel('Category')\n        ax[0].set_ylabel('Count')\n        sns.boxplot(x=df[col], ax=ax[1])\n        ax[1].set_title('Box plot')\n        ax[1].set_xlabel('Category')\n        ax[1].set_ylabel('Value')\n    else:\n        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n        sns.histplot(df[col], kde=True, ax=ax[0])\n        ax[0].set_title('Histogram with KDE')\n        ax[0].set_xlabel('Value')\n        ax[0].set_ylabel('Frequency')\n        sns.boxplot(x=df[col], ax=ax[1])\n        ax[1].set_title('Box plot')\n        ax[1].set_xlabel('Value')\n        ax[1].set_ylabel('Value')\n    return fig\n"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\n\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run Python script as process with predefined arguments. By default, waits for process complete.\n    If wait False, function returns None.\n\n    Parameters:\n    script_path (str): path Python script run.\n    wait (bool): Whether wait for script complete. Default True.\n    *args: arguments passed script.\n\n    Returns:\n    int: return code subprocess. If 'wait' False, returns None.\n\n    Requirements:\n    - subprocess\n    - os\n    - sys\n    - time\n\n    Raise:\n    - ValueError: If script does not exist.\n    - subprocess.CalledProcessError: If script raises exception.\n    \n    Example:\n    >>> import tempfile\n    >>> script_path = tempfile.NamedTemporaryFile(suffix='.py').name\n    >>> with open(script_path, 'w') as f:\n    ...     _ = f.write('import sys;sys.exit(0);')\n    >>> task_func(script_path, True, 'arg1', 'arg2')\n    0\n    >>> task_func(script_path, False, 'arg1', 'arg2') # Should return None\n    \"\"\"\n    if not os.path.exists(script_path):\n        raise ValueError(f\"Script {script_path} does not exist.\")\n\n    try:\n        process = subprocess.Popen([sys.executable, script_path] + list(args))\n        if wait:\n            return process.wait()\n        else:\n            return None\n    except subprocess.CalledProcessError as e:\n        raise e\n    except Exception as e:\n        raise e\n\n"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    \"\"\"\n    Load data from Excel spreadsheet (.xlsx), calculate mean standard deviation each column, \n    draw bar chart. bar chart will returned as matplotlib figure object.\n\n    Parameters:\n    - file_location (str): path Excel file.\n    - sheet_name (str): name sheet load data from.\n\n    Returns:\n    - dict: A dictionary with mean standard deviation each column.\n    - matplotlib.figure.Figure: figure object containing bar chart. figure titled 'Mean Standard Deviation', X-axis labeled 'Columns', Y-axis labeled 'Values'.\n\n    Raises:\n    - FileNotFoundError: If Excel file does not exist at specified path.\n    - ValueError: If specified sheet does not exist workbook.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - os\n    - openpyxl\n\n    Example:\n    >>> file_path='test.xlsx'\n    >>> create_dummy_excel(file_path)\n    >>> result, fig = task_func(file_path, 'TestSheet')\n    >>> os.remove(file_path)\n    >>> fig.axes[0].get_title()\n    'Mean Standard Deviation'\n    \"\"\"\n    # Load data from Excel spreadsheet\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"File {file_location} not found.\")\n    except ValueError:\n        raise ValueError(f\"Sheet {sheet_name} not found in workbook.\")\n\n    # Calculate mean and standard deviation for each column\n    mean_std = df.describe().loc[['mean', 'std']]\n\n    # Draw bar chart\n    fig, ax = plt.subplots()\n    mean_std.plot(kind='bar', ax=ax)\n    ax.set_title('Mean Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n\n    return mean_std.to_dict(orient='index'), fig\n"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef task_func(activities):\n    \"\"\"\n    Return bar chart number activities performed on each day week based on provided list activities.\n    If activities not datetime objects, raise TypeError.\n\n    Parameters:\n    - activities (list datetime objects): A list datetime objects representing when each activity occurred.\n\n    Returns:\n    - matplotlib.axes.Axes: Axes object representing bar chart, with x-axis labeled 'Day Week', y-axis labeled 'Number Activities', title 'Weekly Activity'.\n\n    Requirements:\n    - datetime\n    - collections\n    - matplotlib.pyplot\n\n    Raises:\n    - TypeError: If activities not datetime objects.\n\n    Example:\n    >>> ax = task_func([datetime(2023, 10, 25), datetime(2023, 10, 26)])\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Check if all elements in activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All elements in activities must be datetime objects.\")\n\n    # Create a dictionary to store the count of activities for each day of the week\n    activities_count = defaultdict(int)\n\n    # Iterate over each activity in the list\n    for activity in activities:\n        # Get the day of the week for the activity\n        day_of_week = activity.weekday()\n        # Increment the count for that day of the week\n        activities_count[day_of_week] += 1\n\n    # Create a list of days of the week\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    # Create a bar chart\n    plt.bar(days_of_week, activities_count.values())\n    plt.xlabel('Day Week')\n    plt.ylabel('Number Activities')\n    plt.title('Weekly Activity')\n    return plt.gca()  # Return the axes object\n\n"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\n\n\ndef task_func(src_dir: str, dest_dir: str, seed:int = 100) -> str:\n    \"\"\"\n    Moves random file from source directory specified destination directory.\n    \n    Parameters:\n    - src_dir (str): path source directory from file will randomly selected moved.\n    - dest_dir (str): path destination directory where file will moved.\n    - seed (int, Optional): seed for random number generator. Defaults 100.\n    \n    Returns:\n    str: name file moved. Format: 'filename.extension' (e.g., 'file1.txt').\n    \n    Requirements:\n    - os\n    - shutil\n    - random\n\n    Examples:\n    >>> import tempfile\n    >>> src_dir = tempfile.mkdtemp()\n    >>> dest_dir = tempfile.mkdtemp()\n    >>> open(os.path.join(src_dir, 'file1.txt'), 'w').close()\n    >>> open(os.path.join(src_dir, 'file2.txt'), 'w').close()\n    >>> task_func(src_dir, dest_dir, seed=1)\n    'file2.txt'\n    \"\"\"\n    # Set random seed\n    random.seed(seed)\n    \n    # Get list of files in source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Select random file\n    file_to_move = random.choice(files)\n    \n    # Move file to destination directory\n    shutil.move(os.path.join(src_dir, file_to_move), dest_dir)\n    \n    # Return name of moved file\n    return file_to_move\n\n"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\n\ndef task_func(directory_path='./xlsx_files/'):\n    \"\"\"\n    Protects all double quotes all Excel (.xlsx) files specified directory by prefixing them with double backslash.\n    \n    Parameters:\n    - directory_path (str): path directory containing Excel files. Default './xlsx_files/'.\n    \n    Returns:\n    - int: number Excel files processed.\n    \n    Requirements:\n    - Libraries: re, openpyxl, glob\n    - Excel files specified directory.\n    \n    Example:\n    >>> import tempfile\n    >>> temp_dir = tempfile.mkdtemp()\n    >>> workbook = Workbook()\n    >>> sheet = workbook.active\n    >>> sheet.append(['This \"test\" string.'])\n    >>> workbook.save(temp_dir + '/test.xlsx')\n    >>> task_func(temp_dir)\n    1\n    \"\"\"\n    # Get all Excel files in the specified directory\n    excel_files = glob.glob(directory_path + '*.xlsx')\n    \n    # Initialize counter\n    processed_files = 0\n    \n    # Iterate over each Excel file\n    for file in excel_files:\n        # Load the workbook\n        wb = load_workbook(filename=file)\n        \n        # Get the active sheet\n        sheet = wb.active\n        \n        # Iterate over each cell in the sheet\n        for row in sheet.rows:\n            for cell in row:\n                # Check if the cell value is a string\n                if isinstance(cell.value, str):\n                    # Use regular expression to find all double quotes\n                    matches = re.findall(r'\"', cell.value)\n                    \n                    # If there are double quotes, prefix them with double backslash\n                    if matches:\n                        cell.value = cell.value.replace('\"', '\\\\\"')\n        \n        # Save the workbook\n        wb.save(file)\n        \n        # Increment the counter\n        processed_files += 1\n    \n    # Return the number of processed files\n    return processed_files\n\n"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Create diagram sine wave cosine wave with given frequency return plot.\n\n    Parameters:\n    frequency (float): frequency wave. Must non-negative float.\n    sample_size (int, Optional): A positive integer integer denoting number samples taken for \n    wave. Default 10000.\n\n    Returns:\n    matplotlib.figure.Figure: figure object containing plot.\n    matplotlib.axes.Axes: axes object plot.\n\n    Requirements:\n    - numpy for data generation\n    - matplotlib.pyplot for plotting\n    - math for mathematical constants\n\n    Raises:\n    - ValueError: If frequency negative.\n    - ValueError: If sample size non-positive.\n    \n    Example:\n    >>> fig, ax = task_func(1, 2500)\n    >>> type(fig)\n    <class 'matplotlib.figure.Figure'>\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    # Generate time array\n    t = np.linspace(0, 1, sample_size)\n\n    # Generate sine and cosine waves\n    sine_wave = np.sin(2 * math.pi * frequency * t)\n    cosine_wave = np.cos(2 * math.pi * frequency * t)\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine wave')\n    ax.plot(t, cosine_wave, label='Cosine wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title(f'Sine and Cosine Waves with Frequency {frequency}')\n    ax.legend()\n\n    return fig, ax\n\n"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    \"\"\"\n    Initializes Flask-Mail instance for sending emails using generated Flask application with specified app_name. \n    \n    Parameters:\n    app_name (string): Flask application name\n\n    Returns:\n    tuple: A tuple containing Flask-Mail instance app's mail configurations.\n\n    Note:\n    - details email server retrieved from environment variables. \n    - If variables do not exist, use defaults:\n      - 'MAIL_SERVER': 'localhost'\n      - 'MAIL_PORT': 25\n      - 'MAIL_USE_TLS': False (boolean)\n      - 'MAIL_USERNAME': None\n      - 'MAIL_PASSWORD': None\n    \n    Requirements:\n    - flask\n    - os\n    - flask_mail\n\n    Example:\n    >>> mail, configs = task_func(\"test\")\n    >>> print(mail.__getattribute__(\"app\").name)\n    test\n    \"\"\"\n    # Create Flask application\n    app = Flask(app_name)\n    \n    # Initialize Flask-Mail instance\n    mail = Mail(app)\n    \n    # Get email server configurations from environment variables\n    mail_server = os.environ.get('MAIL_SERVER', 'localhost')\n    mail_port = int(os.environ.get('MAIL_PORT', 25))\n    mail_use_tls = os.environ.get('MAIL_USE_TLS', 'False').lower() == 'true'\n    mail_username = os.environ.get('MAIL_USERNAME')\n    mail_password = os.environ.get('MAIL_PASSWORD')\n    \n    # Return Flask-Mail instance and its configurations\n    return mail, {\n        'MAIL_SERVER': mail_server,\n        'MAIL_PORT': mail_port,\n        'MAIL_USE_TLS': mail_use_tls,\n        'MAIL_USERNAME': mail_username,\n        'MAIL_PASSWORD': mail_password\n    }\n"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    \"\"\"\n    Calculate mean, median, standard deviation data from specific column Excel file.\n\n    Parameters:\n    - excel_file_path (str): path directory containing Excel file.\n    - file_name (str): name Excel file.\n    - column_name (str): name column analyze.\n\n    Returns:\n    - dict: A dictionary with mean, median, standard deviation.\n\n    Raises:\n    - FileNotFoundError: If Excel file does not exist at specified path.\n    - ValueError: If specified column not found Excel file.\n\n    Requirements:\n    - pandas\n    - numpy\n    - os \n\n    Example:\n    >>> data_dir, file_name = './data/', 'excel_file1.xls'\n    >>> create_dummy_file(data_dir, file_name)\n    >>> stats = task_func(data_dir, file_name, 'Sales')\n    >>> os.remove(data_dir + file_name)\n    >>> os.rmdir(data_dir)\n    >>> print(stats)\n    {'mean': 200.0, 'median': 200.0, 'std_dev': 70.71067811865476}\n    \"\"\"\n    # Check if Excel file exists\n    if not os.path.exists(excel_file_path + file_name):\n        raise FileNotFoundError(f\"File {file_name} not found at {excel_file_path}\")\n\n    # Read Excel file\n    try:\n        df = pd.read_excel(excel_file_path + file_name)\n    except Exception as e:\n        raise ValueError(f\"Error reading Excel file: {e}\")\n\n    # Check if specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in Excel file\")\n\n    # Calculate mean, median, standard deviation\n    mean = df[column_name].mean()\n    median = df[column_name].median()\n    std_dev = df[column_name].std()\n\n    return {'mean': mean, 'median': median, 'std_dev': std_dev}\n"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    Trains simple neural network on given input data target labels. function:\n    - Splits data into training set (75%) test set (25%), assuming input dimension always 2.\n    - Constructs Sequential model with one dense hidden layer sigmoid activation function.\n    - Compiles model using binary cross-entropy loss SGD optimizer with specified learning rate.\n    - Fits model training data (without verbose output), also evaluating on test set as validation data.\n    - Plots model's training validation loss over epochs returns plot's Axes object for further customization.\n\n    Parameters:\n    X (np.ndarray): Input features for model, where each feature set has input dimension 2.\n    Y (np.ndarray): Target labels for model.\n\n    Returns:\n    - Sequential: trained Keras Sequential model.\n    - matplotlib.axes.Axes: Axes object plot. plot visualizes model's training validation loss over epochs, with x-axis representing epochs y-axis representing loss. legend distinguishes between 'Train' 'Test' losses.\n\n    Notes:\n    - input dimension X must always 2.\n    - Axes title 'Model loss'\n    - x-axis label 'Epoch'\n    - y-axis label 'Loss'\n\n    Requirements:\n    - keras.layers.Dense\n    - keras.optimizers.SGD\n    - keras.models.Sequential\n    - sklearn.model_selection.train_test_split\n    - matplotlib.pyplot\n\n    Examples:\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> Y = np.array([[0], [1], [1], [0]])\n    >>> model, ax = task_func(X, Y)\n    >>> isinstance(model, Sequential)\n    True\n    >>> isinstance(ax, plt.Axes)\n    True\n    \"\"\"\n    # Split data into training set (75%) test set (25%)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n\n    # Construct Sequential model with one dense hidden layer sigmoid activation function\n    model = Sequential()\n    model.add(Dense(2, input_dim=2, activation='sigmoid'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Compile model using binary cross-entropy loss SGD optimizer with specified learning rate\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n\n    # Fit model training data (without verbose output), also evaluating on test set as validation data\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=10, validation_data=(X_test, Y_test), verbose=0)\n\n    # Plot model's training validation loss over epochs\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    ax = plt.gca()\n\n    return model, ax\n"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    \"\"\"\n    Divide input data into training test sets (70% training, 30% test), \n    create Keras Sequential model with one hidden layer using sigmoid activation function, \n    compile model with binary cross-entropy loss SGD optimizer specifying learning rate,\n    fit model training data non-verbose mode, plot ROC curve for \n    model on test set, including AUC score plot legend.\n\n    Parameters:\n    X (np.ndarray): input data. input dimension always 2.\n    Y (np.ndarray): target data.\n\n    Returns:\n    - keras.models.Sequential: trained Keras model.\n    - matplotlib.axes._axes.Axes: matplotlib Axes object for Precision-Recall curve plot.\n\n    Notes:\n    - title axes should 'ROC curve'\n    - x label 'False positive rate'\n    - y label 'True positive rate'\n\n    Requirements:\n    - tensorflow.keras\n    - sklearn.metrics.roc_curve\n    - sklearn.metrics.auc\n    - sklearn.model_selection.train_test_split\n    - matplotlib\n\n    Example:\n    >>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> Y = np.array([[0], [1], [1], [1]])\n    >>> model, ax = task_func(X, Y)\n    >>> isinstance(model, keras.models.Sequential)\n    True\n    \"\"\"\n    # Split data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create Keras Sequential model\n    model = keras.models.Sequential([\n        keras.layers.Dense(2, activation='sigmoid', input_shape=(2,)),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=0.01), metrics=['accuracy'])\n\n    # Fit model\n    model.fit(X_train, Y_train, epochs=100, verbose=0)\n\n    # Predict probabilities\n    Y_pred = model.predict(X_test)\n\n    # Plot ROC curve\n    fpr, tpr, _ = roc_curve(Y_test[:, 0], Y_pred[:, 0])\n    auc_score = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    return model, plt.gca()  # return trained model and matplotlib Axes object\n"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    \"\"\"\n    Reads RGB image, applies K-means clustering segment image into 'n_clusters' regions, \n    saves each region as separate image. function returns numpy arrays original \n    segmented images.\n\n    Parameters:\n    - image_path (str): path RGB image file. Default 'image.jpg'. image expected \n      RGB format as 3D array (height x width x channels), with channels order RGB.\n    - n_clusters (int): number clusters for K-means clustering. Default 3. A minimum 1 \n      cluster allowed, although clustering with single cluster will simply return original \n      image as segmented image.\n    - random_seed (int): seed for random number generator K-means clustering. Default 42.\n\n    Returns:\n    - tuple: A tuple containing two numpy arrays. first array represents original RGB image, \n             second array represents segmented image, with each pixel's color replaced by \n             centroid cluster belongs to.\n\n    Raises:\n    - FileNotFoundError: If image file does not exist at specified path.\n    - ValueError: If 'n_clusters' not positive integer.\n\n    Requirements:\n    - opencv: For reading image file converting BGR RGB.\n    - numpy: For array manipulations.\n    - os: For checking existence image file.\n    - sklearn.cluster: For applying K-means clustering.\n\n    Example:\n    >>> create_dummy_image('image.jpg')\n    >>> original_img_array, segmented_img_array = task_func('image.jpg', 3)\n    >>> os.remove('image.jpg')\n    >>> print(original_img_array.shape) # Example output\n    (10, 10, 3)\n    >>> print(segmented_img_array.shape) # Example output for n_clusters > 1\n    (10, 10, 3)\n\n    Note:\n    - This function assumes input image RGB format.\n    - segmented image array will have same shape as original image but with pixel colors \n      replaced by their corresponding cluster centroid colors, effectively segmenting image into \n      regions based on color similarity.\n    - Clustering with single cluster allowed will return original image as both \n      original segmented images, since all pixels will assigned same cluster.\n    \"\"\"\n    # Check if image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file does not exist at path: {image_path}\")\n\n    # Check if n_clusters is positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be positive integer\")\n\n    # Read image\n    img = cv2.imread(image_path)\n\n    # Convert BGR to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Reshape image to 2D array of pixels\n    pixels = img.reshape((-1, 3))\n\n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n    labels = kmeans.predict(pixels)\n\n    # Create array of cluster centroids\n    centroids = kmeans.cluster_centers_\n\n    # Replace each pixel with its corresponding cluster centroid\n    segmented_img_array = centroids[labels].reshape(img.shape)\n\n    # Return original and segmented image arrays\n    return img, segmented_img_array\n"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate product matrix 'P' 3D tensor 'T', flatten result,\n    apply KMeans clustering flattened data, visualize it.\n\n    Parameters:\n    P (numpy.ndarray): input matrix.\n    T (numpy.ndarray): input tensor with shape (3, 3, 3).\n    n_clusters (int): number clusters for KMeans clustering. Default 3.\n    random_state (int): random state for KMeans clustering. Default 0.\n    n_init (int): Number time k-means algorithm will run with different centroid seeds. Default 10.\n\n    Returns:\n    cluster_result (numpy.ndarray): result KMeans clustering.\n    ax (matplotlib.axes.Axes): visualization KMeans clustering, with title 'KMeans Clustering Visualization'.\n\n    Requirements:\n    - numpy\n    - sklearn\n    - matplotlib\n\n    Example:\n    >>> P = np.array([[6, 2, 7], [1, 1, 8], [8, 7, 1], [9, 6, 4], [2, 1, 1]])\n    >>> T = np.random.rand(3, 3, 3)\n    >>> cluster_result, ax = task_func(P, T, n_clusters=3, random_state=0, n_init=10)\n    >>> type(cluster_result)\n    <class 'numpy.ndarray'>\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Flatten tensor T\n    flattened_T = T.flatten()\n\n    # Apply KMeans clustering\n    kmeans = KMeans(\n        n_clusters=n_clusters,\n        random_state=random_state,\n        n_init=n_init,\n    )\n    cluster_result = kmeans.fit_predict(flattened_T.reshape(-1, 1))\n\n    # Visualize KMeans clustering\n    ax = plt.figure(figsize=(8, 6)).add_subplot(111, projection=\"3d\")\n    ax.scatter(\n        flattened_T[:, 0],\n        flattened_T[:, 1],\n        flattened_T[:, 2],\n        c=cluster_result,\n        cmap=\"viridis\",\n    )\n    ax.set_title(\"KMeans Clustering Visualization\")\n    return cluster_result, ax\n\n"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate Voronoi diagram for number points 2D plot it.\n    Note: this function will raise errors when input invalid, for example wrong type or shape.\n    Jittering applied prior plotting.\n\n    Parameters:\n    - points (np.ndarray): A numpy ndarray shape (n_points, 2) with coordinates points.\n    - seed (int): Random seed for reproducibility. Defaults 0.\n\n    Returns:\n    tuple (vor, ax): A tuple containing:\n        - vor (Voronoi): A Voronoi object representing Voronoi diagram points.\n        - ax (Axes): axes plotted Voronoi diagram.\n\n    Requirements:\n    - numpy\n    - scipy\n    - matplotlib.pyplot\n\n    Example:\n    >>> points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    >>> vor, ax = task_func(points)\n    >>> type(vor)\n    <class 'scipy.spatial.qhull.Voronoi'>\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Check if input is numpy array\n    if not isinstance(points, np.ndarray):\n        raise TypeError(\"Input must be a numpy array.\")\n    # Check if input array has correct shape\n    if points.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (n_points, 2).\")\n\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Create Voronoi object\n    vor = Voronoi(points)\n\n    # Jitter points\n    jitter = np.random.uniform(-0.01, 0.01, size=points.shape)\n    jittered_points = points + jitter\n\n    # Plot Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax, show_points=False, show_vertices=False, line_alpha=0.6)\n    ax.scatter(jittered_points[:, 0], jittered_points[:, 1], color='k', s=10)\n\n    return vor, ax\n\n"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\n\n\ndef task_func(src_dir, dest_dir, ext):\n    \"\"\"\n    Moves files with specified extension from source directory destination directory. \n    This function searches for files source directory match given extension.\n    If file with same name already exists destination directory, not moved.\n\n    Parameters:\n    - src_dir (str): source directory path.\n    - dest_dir (str): destination directory path.\n    - ext (str): file extension search for (without leading dot).\n\n    Returns:\n    - list: A list full paths files were successfully moved. If file was not moved\n            because already exists destination directory, will not included this list.\n\n    Raises:\n    FileNotFoundError: if either source or destination directory does not exist\n            \n    Requirements:\n    - os\n    - shutil\n    - glob\n\n    Examples:\n    >>> test_src_dir = './test_src'\n    >>> test_dest_dir = './test_dest'\n    >>> test_ext = 'txt'\n    >>> os.makedirs(test_src_dir, exist_ok=True)\n    >>> os.makedirs(test_dest_dir, exist_ok=True)\n    >>> moved_files = task_func(test_src_dir, test_dest_dir, test_ext)\n    >>> len(moved_files) > 0  # Check if files were moved\n    True\n    >>> 'test_file.txt' [os.path.basename(path) for path moved_files]  # Assuming test_file.txt exists test_src_dir\n    True\n    >>> os.listdir(test_dest_dir)  # Verify files were moved, no duplicates exist destination\n    ['test_file.txt']\n    \"\"\"\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist.\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory '{dest_dir}' does not exist.\")\n\n    # Get list of files with specified extension in source directory\n    files = glob.glob(os.path.join(src_dir, f\"*.{ext}\"))\n\n    # Initialize list to store paths of moved files\n    moved_files = []\n\n    # Iterate over files\n    for file in files:\n        # Get file name without path\n        file_name = os.path.basename(file)\n\n        # Construct destination file path\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if file already exists in destination directory\n        if not os.path.exists(dest_file_path):\n            # Move file to destination directory\n            shutil.move(file, dest_file_path)\n            # Add file path to list of moved files\n            moved_files.append(dest_file_path)\n\n    # Return list of moved files\n    return moved_files\n\n"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\n\n\ndef task_func(json_str):\n    \"\"\"\n    Load JSON string into dictionary, normalize dictionary by doubling numerical values,\n    create Pandas DataFrame from dictionary.\n\n    This function processes JSON string by converting into dictionary, normalizes data\n    by doubling numerical values, constructs Pandas DataFrame from this dictionary.\n    Note: function designed handle simple flat dictionaries, with values either\n    single numerical values, lists numerical values, or strings can interpreted as\n    numbers. It doubles values numerical data types within dictionary, including those\n    within lists those strings (which extracted using regex), but function does not\n    process nested dictionaries. Finally, returns DataFrame with numerical values stored as\n    floats other types left as-is, or empty DataFrame if input JSON string empty or\n    does not contain valid data structures for DataFrame conversion.\n\n    Parameters:\n    json_str (str): JSON string.\n\n    Returns:\n    DataFrame: A pandas DataFrame created from dictionary.\n\n    Requirements:\n    - pandas\n    - json\n    - re\n\n    Example:\n    >>> json_str = '{\"a\": [1, 2, 3], \"b\": 4.9, \"c\": \"5\"}'\n    >>> df = task_func(json_str)\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    >>> print(df)\n          b   c\n    0  2  9.8  10\n    1  4  9.8  10\n    2  6  9.8  10\n    \"\"\"\n    # Load JSON string into dictionary\n    data_dict = json.loads(json_str)\n\n    # Normalize dictionary by doubling numerical values\n    for key, value in data_dict.items():\n        if isinstance(value, list):\n            data_dict[key] = [double_value if isinstance(double_value, (int, float)) else double_value for double_value in value]\n        elif isinstance(value, str):\n            data_dict[key] = double_value if re.match(r'^-?\\d+(?:\\.\\d+)?$', value) else value\n        elif isinstance(value, (int, float)):\n            data_dict[key] = value * 2\n\n    # Create Pandas DataFrame from dictionary\n    try:\n        df = pd.DataFrame(data_dict)\n    except ValueError:\n        df = pd.DataFrame()\n\n    return df\n\n"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\n\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    \"\"\"\n    Executes given bash script returns CPU memory usage script's process.\n\n    This function checks whether script path exists, executes subprocess\n    uses psutil monitor script's process for CPU memory usage.\n    Note:\n        - CPU usage cumulative measure script process's CPU demand over execution\n          period, not average across cores.\n        - Memory usage reported as sum RSS memory increments.\n    function aggregates these metrics until script completes or specified timeout is\n    reached. It handles cases where process becomes zombie or not found, ensures the\n    subprocess terminated if runs beyond timeout.\n\n    Parameters:\n    script_path (str): path bash script executed. Path must exist.\n    timeout (int, optional): Maximum time (in seconds) function should wait for script complete.\n                             Defaults 10 seconds.\n\n    Returns:\n    dict: A dictionary containing:\n        - 'CPU Usage': accumulated CPU usage percentage.\n        - 'Memory Usage': accumulated memory usage bytes.\n\n    Requirements:\n    - subprocess\n    - psutil\n    - time\n    - os\n    \n    Examples:\n    >>> resources = task_func('/path/to/script.sh')\n    >>> resources\n    {'CPU Usage': 5.2, 'Memory Usage': 2048}\n    \"\"\"\n    # Check if script exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"Script {script_path} not found.\")\n\n    # Execute script\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Initialize variables\n    cpu_usage = 0\n    mem_usage = 0\n    start_time = time.time()\n\n    # Monitor process\n    while process.poll() is None and time.time() - start_time < timeout:\n        # Get process details\n        try:\n            process_details = process.status()\n        except psutil.NoSuchProcess:\n            # If process is not found, break loop\n            break\n\n        # Calculate CPU usage\n        cpu_usage += process_details.cpu_percent(interval=0.1)\n\n        # Calculate memory usage\n        mem_usage += process_details.memory_info().rss\n\n        # Check if process is zombie\n        if process_details.status() == psutil.STATUS_ZOMBIE:\n            # If process is zombie, terminate it\n            process.terminate()\n            break\n\n    # If process is still running, terminate it\n    if process.poll() is None:\n        process.terminate()\n\n    # Return resources\n    return {'CPU Usage': cpu_usage, 'Memory Usage': mem_usage}  # type: ignore\n\n"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    Create DataFrame with given number rows (N) 3 columns: \"x\" \"y\" with random values,\n    \"category\" with random categories from given CATEGORIES list. Each category guaranteed to\n    appear at least once if N greater than or equal number categories, otherwise is\n    randomly sampled without replacement from CATEGORIES. Finally, draw scatter plot \"x\" vs \"y,\"\n    colored by \"category\".\n\n    Parameters:\n    - N (int, optional): Number rows for DataFrame. Defaults 100.\n    - CATEGORIES (list, optional): List categories. Defaults ['A', 'B', 'C', 'D', 'E'].\n    - seed (int, optional): Random seed for reproducibility. Defaults 42.\n\n    Returns:\n    tuple: A tuple containing:\n        - DataFrame: generated DataFrame.\n        - Axes: Axes object scatter plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df, ax = task_func()\n    >>> df.head()\n              x         y category\n    0  0.239562  0.385098        C\n    1  0.144895  0.851137        D\n    2  0.489453  0.316922        C\n    3  0.985650  0.169493        E\n    4  0.242055  0.556801        A\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random values for \"x\" and \"y\" columns\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n\n    # Generate random categories\n    if N >= len(CATEGORIES):\n        # If N is greater or equal to number of categories, each category will appear at least once\n        categories = np.random.choice(CATEGORIES, size=N, replace=True)\n    else:\n        # Otherwise, randomly sample without replacement from CATEGORIES\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n\n    # Create DataFrame\n    df = pd.DataFrame({\"x\": x, \"y\": y, \"category\": categories})\n\n    # Draw scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(df[\"x\"], df[\"y\"], c=df[\"category\"])\n\n    return df, ax\n\n"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime\nimport pandas as pd\nimport numpy as np\n\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generate time series from given epoch start time end time with specified step trend.\n    time series plotted with timestamps on x-axis ('Time') values on y-axis ('Value').\n    values generated from normal distribution, linear trend added based on the\n    provided trend value.\n\n    Parameters:\n    - start_time (int): start epoch time milliseconds.\n    - end_time (int): end epoch time milliseconds. Must greater than start_time.\n    - step (int): step milliseconds between each data point. Must agreater than 0.\n    - trend (float): trend value added time series. It acts as multiplier\n                     for index, adding linear trend randomly generated values.\n    - seed (int, optional): Seed for reproducibility. Default 42.\n\n    Returns:\n    - ax (matplotlib.pyplot.Axes): Axes object generated plot, with x-axis labeled 'Time' y-axis labeled 'Value'.\n\n    Requirements:\n    - datetime.datetime\n    - pandas\n    - numpy\n\n    Example:\n    >>> ax = task_func(0, 10000, 100, 0.001)\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    >>> ax.get_xticklabels()\n    [Text(-20.0, 0, '1970-01-01 10:00:08.000000'), Text(0.0, 0, '1970-01-01 10:00:00.000000'), Text(20.0, 0, '1970-01-01 10:00:02.000000'), Text(40.0, 0, '1970-01-01 10:00:04.000000'), Text(60.0, 0, '1970-01-01 10:00:06.000000'), Text(80.0, 0, '1970-01-01 10:00:08.000000'), Text(100.0, 0, ''), Text(120.0, 0, '')]\n    \"\"\"\n    np.random.seed(seed)\n    # Generate time series\n    time_series = np.arange(start_time, end_time, step)\n    # Generate random values from normal distribution\n    random_values = np.random.normal(0, 1, len(time_series))\n    # Add linear trend\n    linear_trend = trend * np.arange(len(time_series))\n    # Add trend to random values\n    values = random_values + linear_trend\n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_series, 'Value': values})\n    # Plot time series\n    import matplotlib.pyplot as plt\n    ax = df.plot(x='Time', y='Value', kind='line')\n    # Set labels\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    return ax\n\n"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    \"\"\"\n    Generate sales data for five products from given epoch time up current time.\n\n    This function checks input validity, for each day between date given epoch\n    time date current time, generates random sales data for each 5 products.\n\n    Parameters:\n    - epoch_milliseconds (int): Start epoch time milliseconds. Must before current system time.\n    - random_seed (int):        Seed for reproducibility random sales data. Defaults 0.\n    - products (list str):   Product list choose from. Must contain 5 unique strings.\n                                Defaults ['Product1', 'Product2', 'Product3', 'Product4', 'Product5'].\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing sales data with columns 'Product' (string), 'Date' (datetime),\n                    'Sales' (integer). Sales quantity randomly sampled from range [10, 50].\n\n    Requirements:\n    - pandas\n    - datetime.datetime\n    - random\n\n    Example:\n    >>> sales_data = task_func(1236472051807, random_seed=42)\n    >>> type(sales_data)\n    <class 'pandas.core.frame.DataFrame'>\n    >>> sales_data.head()\n        Product                    Date  Sales\n    0  Product4 2009-03-08 11:27:31.807     50\n    1  Product5 2009-03-08 11:27:31.807     17\n    2  Product1 2009-03-08 11:27:31.807     11\n    3  Product3 2009-03-08 11:27:31.807     27\n    4  Product2 2009-03-08 11:27:31.807     25\n    \"\"\"\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int):\n        raise TypeError(\"epoch_milliseconds must be an integer.\")\n    if epoch_milliseconds >= int(datetime.now().timestamp() * 1000):\n        raise ValueError(\"epoch_milliseconds must be before current system time.\")\n    if not isinstance(random_seed, int):\n        raise TypeError(\"random_seed must be an integer.\")\n    if len(products) != 5 or len(set(products)) != 5:\n        raise ValueError(\"products must be a list of 5 unique strings.\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch milliseconds to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Generate sales data for each day between start date and current date\n    sales_data = []\n    current_date = start_date\n    while current_date <= datetime.now():\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({\"Product\": product, \"Date\": current_date, \"Sales\": sales})\n        current_date += datetime.timedelta(days=1)\n\n    # Convert sales data to DataFrame\n    sales_data = pd.DataFrame(sales_data)\n\n    return sales_data\n\n"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    \"\"\"\n    Convert JSON strings Excel file, including handling empty JSON arrays.\n\n    This function takes JSON string converts into Excel file with specified filename. If JSON string represents empty array, function creates Excel file with no data rows.\n\n    Parameters:\n    - json_str (str, bytes, bytearray): JSON content as string, bytes, or bytearray.\n    - filename (str): name Excel file created.\n    - sheet_name (str, optional): name sheet Excel file. Default \"sheet1\".\n\n    Returns:\n    - str: absolute path created Excel file.\n\n    Raises:\n    - ValueError: If `json_str` not valid JSON.\n    - TypeError: If `json_str` not string, bytes, or bytearray.\n    - Exception: For other general errors related file writing.\n\n    Requirements:\n    - xlwt: For writing Excel files.\n    - xlrd\n    - os: For file path operations.\n    - pandas: For data manipulation.\n\n\n    Example:\n    >>> json_str = '[{\"Name\": \"John\", \"Age\": 30}, {\"Name\": \"Jane\", \"Age\": 28}]'\n    >>> True if task_func(json_str, 'data.xls').endswith('data.xls') else False # True\n    True\n    >>> os.remove('data.xls')\n    \"\"\"\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be string, bytes, or bytearray\")\n\n    try:\n        json_obj = json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Invalid JSON string\") from e\n\n    if not isinstance(json_obj, list):\n        raise ValueError(\"JSON string must represent a list\")\n\n    if not json_obj:\n        # Create Excel file with no data rows\n        excel_file = xlwt.Workbook()\n        sheet = excel_file.add_sheet(sheet_name)\n        excel_file.save(filename)\n        return os.path.abspath(filename)\n\n    # Convert JSON list to pandas DataFrame\n    df = pd.DataFrame(json_obj)\n\n    # Write DataFrame to Excel file\n    df.to_excel(filename, sheet_name=sheet_name, index=False)\n    return os.path.abspath(filename)\n"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\n\ndef task_func(days_in_past=7, random_seed=0):\n    \"\"\"\n    Generates graph daily activity durations for specified number days past\n    using randomly generated data for activities.\n\n    This function randomly generates acitivity durations from 0 120 for each activity\n    from [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"].\n\n    Parameters:\n    days_in_past (int, optional): number days past for generate graph.\n                                  Defaults 7 days. Must past.\n    random_seed (int, optional):  Seed for random number generation ensure reproducibility.\n                                  Defaults 0.\n\n    Returns:\n    Tuple containing\n    - ax (matplotlib.pyplot.Axes): DataFrame used for plotting.\n    - df (pd.DataFrame): Seaborn lineplot with date on x-axis, duration on y-axis, activity as hue.\n\n    Requirements:\n    - datetime.datetime\n    - datetime.timedelta\n    - pandas\n    - random\n    - seaborn\n\n    Example:\n    >>> ax, df = task_func(7, random_seed=42)\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n\n    A sample row from returned DataFrame might look like:\n    Date        Activity  Duration\n    YYYY-MM-DD  Running   45\n    \"\"\"\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Generate list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n\n    # Generate list of dates\n    start_date = datetime.now() - timedelta(days=days_in_past)\n    dates = [start_date + timedelta(days=x) for x in range(days_in_past)]\n\n    # Generate random durations for each activity\n    durations = {activity: [random.randint(0, 120) for _ in range(days_in_past)] for activity in activities}\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        \"Date\": dates,\n        \"Activity\": [random.choice(activities) for _ in range(days_in_past)],\n        \"Duration\": [random.choice(durations[activity]) for activity in activities for _ in range(days_in_past)]\n    })\n\n    # Convert Date to datetime\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\n    # Plot\n    ax = sns.lineplot(x=\"Date\", y=\"Duration\", hue=\"Activity\", data=df)\n    return ax, df\n\n"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Create DataFrame stock prices for specified number days past using random data.\n\n    Parameters:\n    - days_in_past (int, optional): number days past for we want stock data.\n                                    Must positive. Defaults 7.\n    - stock_names (list str, optional): list stock names for we want data.\n                                           Must not empty. Defaults [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"].\n    - random_seed (int, optional): seed for random number generation ensure reproducibility. Defaults 0.\n\n    Returns:\n    DataFrame: A pandas DataFrame containing random stock prices for specified number days.\n               Prices floats [0.0,1.0).\n\n    Requirements:\n    - datetime.datetime\n    - pandas\n    - numpy\n\n    Example:\n    >>> df = task_func(5, random_seed=42)\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    >>> print(df.head(1))\n                     AAPL      GOOGL       MSFT       AMZN         FB\n    2024-03-30  37.454012  95.071431  73.199394  59.865848  15.601864\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Create a list of dates for the specified number of days in the past\n    today = datetime.now()\n    dates = [(today - datetime.timedelta(days=x)).strftime(\"%Y-%m-%d\") for x in range(days_in_past)]\n\n    # Create a DataFrame with the specified stock names and dates\n    df = pd.DataFrame(index=dates, columns=stock_names)\n\n    # Generate random stock prices for each stock and date\n    for stock in stock_names:\n        df[stock] = np.random.rand(days_in_past)\n\n    return df\n\n"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    \"\"\"\n    Compare two CSV files create difference report.\n\n    This function compares two CSV files line by line provides detailed report differences. It represents each difference with line number, status indicator, content line.\n\n    Parameters:\n    file_path1 (str): file path first CSV file.\n    file_path2 (str): file path second CSV file.\n    delimiter (str, optional): Delimiter character used CSV files. Default ','.\n    quotechar (str, optional): Quote character used CSV files. Default '\"'.\n\n    Returns:\n    DataFrame: A pandas DataFrame with differences. DataFrame contains following columns:\n        - 'Line Number': line number file where difference occurs.\n        - 'Status': A character indicating type difference:\n            - ' ': No change (line same both files).\n            - '-': Line present first file but not second.\n            - '+': Line present second file but not first.\n        - 'Content': actual text content line from either file.\n\n    Raises:\n    FileNotFoundError: If either files cannot found.\n    ValueError: If either files empty.\n    Exception: For other IO related errors.\n\n    Requirements:\n    - pandas: For data manipulation analysis.\n    - csv: For reading CSV files.\n    - difflib: For performing difference operation.\n    - os \n\n    Example:\n    >>> create_dummy_test_files()\n    >>> df = task_func('file1.csv', 'file2.csv')\n    >>> os.remove('file1.csv')\n    >>> os.remove('file2.csv')\n    >>> df.head()\n       Line Number Status          Content\n    0            1         ('name', 'age')\n    1            2      -  ('Alice', '30')\n    2            3      +  ('Alice', '31')\n    3            4           ('Bob', '25')\n    \"\"\"\n    try:\n        # Read CSV files\n        with open(file_path1, 'r', newline='', encoding='utf-8') as f1, open(file_path2, 'r', newline='', encoding='utf-8') as f2:\n            reader1 = csv.reader(f1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(f2, delimiter=delimiter, quotechar=quotechar)\n            # Get all lines from both files\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n            # Perform difference operation\n            diff = ndiff(lines1, lines2)\n            # Create a list to store differences\n            differences = []\n            for line in diff:\n                if line[0] == '-':\n                    differences.append((1, '-', line[2:]))\n                elif line[0] == '+':\n                    differences.append((2, '+', line[2:]))\n                else:\n                    differences.append((1, ' ', line[2:]))\n            # Convert differences to DataFrame\n            df = pd.DataFrame(differences, columns=['Line Number', 'Status', 'Content'])\n            return df\n    except FileNotFoundError as e:\n        print(f\"Error: File not found: {e}\")\n        raise\n    except ValueError as e:\n        print(f\"Error: File is empty: {e}\")\n        raise\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise\n\n"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze list employee data calculate statistics for given column. If data list empty,\n    sum will 0 mean, min, max values will NaN. function also visualizes data with\n    pie chart, using Age column as labels.\n\n    Parameters:\n    column (str): column analyze. Valid values 'Age', 'Salary', 'Experience'.\n                  If invalid, function will raise KeyError.\n    data (list lists): employee data, where each list represents [Age, Salary, Experience].\n\n    Returns:\n    tuple: A tuple containing:\n        - dict: A dictionary with 'sum', 'mean', 'min', 'max' column.\n        - Axes object: pie chart visualizing column data.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = [[25, 50000, 2], [30, 75000, 5], [35, 100000, 7], [40, 125000, 10], [45, 150000, 12]]\n    >>> stats, ax = task_func('Salary', data)\n    >>> stats\n    {'sum': 500000, 'mean': 100000.0, 'min': 50000, 'max': 150000}\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    # Check if column is valid\n    valid_columns = ['Age', 'Salary', 'Experience']\n    if column not in valid_columns:\n        raise KeyError(f\"Invalid column. Valid columns are: {valid_columns}\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data, columns=['Age', 'Salary', 'Experience'])\n\n    # Select column to analyze\n    column_data = df[column]\n\n    # Calculate statistics\n    stats = {\n        'sum': column_data.sum(),\n        'mean': column_data.mean(),\n        'min': column_data.min(),\n        'max': column_data.max()\n    }\n\n    # Visualize data with pie chart\n    ax = column_data.value_counts().plot(kind='pie', autopct='%1.1f%%')\n    ax.set_title(f'Distribution of {column}')\n\n    return stats, ax\n\n"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef task_func(column, data):\n    \"\"\"\n    Analyze list fitness data, calculate sum, mean, minimum,\n    maximum certain column draw line chart. Additionally, validate\n    numeric values for steps, calories burned, distance walked are\n    non-negative.\n\n    Parameters:\n    column (str): column analyze from data. allowed columns are:\n                 'Date', 'Steps', 'Calories Burned', 'Distance Walked'.\n    data (list list): A list where each inner list contains datetime object\n                         representing date, followed by numeric values for steps,\n                         calories burned, distance walked order. Each\n                         numeric value must non-negative. Must not empty.\n\n    Returns:\n    tuple: A tuple containing:\n        - dict: A dictionary with sum, mean, min, max column.\n        - matplotlib.axes.Axes: Axes object plotted line chart. line\n                                chart will have Date on its x-axis, column value\n                                on its y-axis, title Line Chart (column).\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Raises:\n    - KeyError: If specified column not valid.\n    - ValueError: If data list empty or if numeric values for\n                    steps, calories burned, distance walked negative.\n    Example:\n    >>> data = [[datetime(2022, 1, 1), 5000, 200, 3.5],\n    ...         [datetime(2022, 1, 2), 5500, 220, 4.0],\n    ...         [datetime(2022, 1, 3), 6000, 240, 4.5]]\n    >>> stats, ax = task_func('Steps', data)\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    >>> print(stats)\n    {'sum': 16500, 'mean': 5500.0, 'min': 5000, 'max': 6000}\n    \"\"\"\n    # Check if data list is not empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n\n    # Convert data to DataFrame\n    df = pd.DataFrame(data, columns=['Date', 'Steps', 'Calories Burned', 'Distance Walked'])\n\n    # Validate numeric values for steps, calories burned, distance walked\n    for index, row in df.iterrows():\n        if row['Steps'] < 0 or row['Calories Burned'] < 0 or row['Distance Walked'] < 0:\n            raise ValueError(\"Numeric values for steps, calories burned, distance walked must be non-negative\")\n\n    # Check if specified column is valid\n    if column not in ['Date', 'Steps', 'Calories Burned', 'Distance Walked']:\n        raise KeyError(\"Invalid column\")\n\n    # Calculate sum, mean, min, max column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n\n    # Draw line chart\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['Date'], df[column], marker='o')\n    plt.title(f'Line Chart ({column})')\n    plt.xlabel('Date')\n    plt.ylabel(column)\n    ax = plt.gca()\n\n    return stats, ax\n\n"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef task_func(input_file=\"data.json\"):\n    \"\"\"\n    Read list dictionaries from JSON file, calculate mean median for each key\n    (ignoring non-numeric or missing values), convert results into Pandas DataFrame.\n\n    Parameters:\n    - input_file (str, optional): input JSON file name. Defaults 'data.json'.\n                                  file should contain list dictionaries. If key is\n                                  missing dictionary, treated as NaN for record.\n                                  Non-numeric values ignored for calculation mean\n                                  median. If all values for key non-numeric or missing,\n                                  statistics for key will NaN.\n\n    Returns:\n    - df (pd.DataFrame): A DataFrame indexed sorted by variable names (keys) from the\n                         input data, containing columns 'mean' 'median'.\n\n    Requirements:\n    - numpy\n    - collections\n    - json\n    - pandas\n\n    Example:\n    >>> df = task_func('data_1.json')\n           mean  median\n    b        mean  median\n    c        mean  median\n    \"\"\"\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Create dictionary to store results\n    result = defaultdict(list)\n\n    # Iterate over each dictionary in the list\n    for d in data:\n        # Iterate over each key in the dictionary\n        for k, v in d.items():\n            # If value is numeric, add it to the list for this key\n            if isinstance(v, (int, float)):\n                result[k].append(v)\n\n    # Calculate mean and median for each key\n    for k, v in result.items():\n        mean = np.mean(v)\n        median = np.median(v)\n        result[k] = [mean, median]\n\n    # Convert result to DataFrame\n    df = pd.DataFrame(result).T\n    df.columns = ['mean', 'median']\n\n    # Sort DataFrame by index\n    df = df.sort_index()\n\n    return df\n\n"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef task_func(file_path):\n    \"\"\"\n    Identifies duplicate rows from CSV file using csv library, convert duplicated rows\n    into pandas DataFrame, plot using matplotlib.\n\n    Parameters:\n    - file_path (str): path CSV file.\n\n    Returns:\n    - dict: A dictionary with duplicate rows as keys their counts as values.\n    - Axes: A matplotlib Axes object with bar chart duplicate rows.\n\n    Requirements:\n    - csv\n    - collections.Counter\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> duplicates, ax = task_func(\"sample_data.csv\")\n    >>> duplicates\n    {('Alice', '25', 'New York'): 3, ('Bob', '30', 'London'): 2}\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n\n    Note: Ensure CSV file proper format has .csv extension. Other file formats will raise ValueError.\n    \"\"\"\n    # Read CSV file into DataFrame\n    df = pd.read_csv(file_path)\n\n    # Identify duplicate rows\n    duplicates = df.duplicated().sum()\n\n    # Count duplicate rows\n    duplicate_rows = df[df.duplicated()].value_counts().to_dict()\n\n    # Plot bar chart\n    plt.bar(duplicate_rows.keys(), duplicate_rows.values())\n    plt.xlabel('Duplicate Rows')\n    plt.ylabel('Count')\n    plt.title('Duplicate Rows Count')\n    ax = plt.gca()\n\n    return duplicate_rows, ax\n\n"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    \"\"\"\n    Identify duplicate entries DataFrame record age distribution for duplicate names.\n\n    This function takes DataFrame with 'name' 'age' columns. If age provided as floats,\n    they will rounded down nearest integer. Age must not negative, otherwise function\n    raises ValueError. Then, function identifies duplicate names records age distribution.\n    It returns Counter object with age distribution histogram plot showing distribution\n    ages for duplicate names, with age on x-axis count on y-axis. Bins calculated\n    based on minimum maximum ages found among duplicates, adjusted by .5 ensure that\n    integer ages fall squarely within bins.\n\n    Parameters:\n    df: pd.DataFrame - A DataFrame with columns 'name' 'age'.\n                       Must not empty. If empty, function raises ValueError.\n\n    Returns:\n    Counter: Age distribution among duplicate names.\n    plt.Axes or None: Histogram plot displaying age distribution, or None if no duplicates.\n\n    Requirements:\n    - pandas\n    - numpy\n    - collections.Counter\n    - seaborn\n    - matplotlib.pyplot\n\n    Raises:\n    - ValueError: If DataFrame empty or if age negative.\n    \n    Example:\n    >>> df = pd.DataFrame({'name': ['Alice', 'Bob', 'Alice'], 'age': [25, 26, 25]})\n    >>> duplicates_counter, ax = task_func(df)\n    >>> duplicates_counter\n    Counter({25: 2})\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    if df['age'].min() < 0:\n        raise ValueError(\"Age cannot be negative\")\n    df['age'] = df['age'].apply(lambda x: int(np.floor(x)))\n    duplicates = df[df.duplicated(subset='name', keep=False)]\n    if duplicates.empty:\n        return None, None\n    bins = np.arange(duplicates['age'].min() - 0.5, duplicates['age'].max() + 1.5)\n    plt.hist(duplicates['age'], bins=bins, alpha=0.7, color='blue', edgecolor='black')\n    plt.title('Age Distribution for Duplicate Names')\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.xticks(bins)\n    return Counter(duplicates['age']), plt.gca()\n\n"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n\ndef task_func(df, bins=4):\n    \"\"\"\n    Identify count duplicate values DataFrame's 'value' column.\n    This function also plots histogram for all values 'value' column\n    overlays normal distribution curve on histogram.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing numeric 'value' column. If empty,\n                       function will return empty Counter empty plot.\n    bins (int, optional): Number bins for histogram. Defaults 4.\n\n    Returns:\n    tuple: A tuple containing:\n        - Counter: A Counter object with count each duplicate value.\n        - Axes: A matplotlib.axes.Axes object represents plot\n                histogram with 'value' column data. If applicable,\n                normal distribution curve fitted data overlaid. The\n                histogram's bars green with 60% opacity, normal\n                distribution curve black with linewidth 2. plot is\n                titled \"Distribution\", with \"Value\" as x-axis label and\n                \"Frequency\" as y-axis label.\n\n    Requirements:\n    - collections.Counter\n    - numpy\n    - scipy.stats.norm\n    - matplotlib.pyplot\n\n    Example:\n    >>> df = pd.DataFrame({'value': [1, 2, 2, 3, 3, 4, 3, 2, 1, 4, 4, 4, 2, 2, 3, 1, 1, 1, 3, 2]})\n    >>> counter, ax = task_func(df)\n    >>> ax\n    <Axes: title={'center': 'Distribution'}, xlabel='Value', ylabel='Frequency'>\n    >>> counter\n    Counter({2: 6, 1: 5, 3: 5, 4: 4})\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        return Counter(), None\n\n    # Count duplicate values\n    counter = Counter(df['value'])\n\n    # Plot histogram\n    ax = df['value'].hist(bins=bins, color='green', alpha=0.6)\n    # Fit normal distribution curve\n    mean = np.mean(df['value'])\n    std = np.std(df['value'])\n    x = np.linspace(mean - 3 * std, mean + 3 * std, 100)\n    y = norm.pdf(x, mean, std)\n    ax.plot(x, y, 'k-', linewidth=2)\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return counter, ax\n\n"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\n\ndef task_func(a, b):\n    \"\"\"\n    Generate pandas DataFrame with random values based on lists 'a' 'b', plot as bar chart.\n    List 'a' sets DataFrame's row indices, while length list 'b' determines number columns\n    using predefined names from 'COLUMNS = ['A', 'B', 'C', 'D', 'E']' list.\n\n    Parameters:\n    - (list): A list used define number rows DataFrame.\n    - b (list): Another list used define number columns DataFrame. actual column names predefined.\n\n    Returns:\n    - matplotlib.axes.Axes: Axes object plotted bar chart.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib\n\n    Data Structure:\n    - Uses pandas DataFrame structure data.\n\n    Example:\n    >>> ax = task_func([1, 2, 3], ['A', 'B', 'C', 'D', 'E'])\n    \"\"\"\n    # Generate random values\n    data = np.random.randint(0, 100, size=(len(a), len(b)))\n\n    # Create DataFrame\n    df = pd.DataFrame(data, index=a, columns=b)\n\n    # Plot bar chart\n    ax = df.plot(kind='bar')\n    return ax\n\n"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef task_func(data):\n    \"\"\"\n    This function plots bar chart monthly data values for single year, with 'month' on x-axis 'value'\n    on y-axis.\n\n    Parameters:\n    data (str): data string format 'yyyy-mm-value'.\n\n    Returns:\n    Axes object: A matplotlib.axes.Axes object representing plot, with title formatted as 'Monthly Data for yyyy', x label as 'Month', y label as 'Value'.\n\n    Requirements:\n    - pandas\n    - datetime\n    - matplotlib.pyplot\n\n    Example:\n    >>> data = '2022-01-100,2022-02-200,2022-03-150,2022-04-300,2022-05-250,2022-06-350,2022-07-400,2022-08-450,2022-09-500,2022-10-550,2022-11-600,2022-12-650'\n    >>> ax = task_func(data)\n    \"\"\"\n    # Split data string into list of strings\n    data_list = data.split(',')\n\n    # Create list to hold data\n    data_values = []\n\n    # Iterate over each data string\n    for item in data_list:\n        # Split data string into year, month, value\n        year, month, value = item.split('-')\n        # Convert year and month to datetime object\n        date = datetime.strptime(f'{year}-{month}', '%Y-%m')\n        # Append value to data_values list\n        data_values.append((date, int(value)))\n\n    # Sort data_values list by date\n    data_values.sort(key=lambda x: x[0])\n\n    # Create pandas DataFrame from data_values list\n    df = pd.DataFrame(data_values, columns=['date', 'value'])\n\n    # Plot bar chart\n    ax = df['date'].dt.to_period('M').value_counts().plot(kind='bar')\n    ax.set_title(f'Monthly Data for {df[\"date\"].dt.year[0]}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    return ax\n\n"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    \"\"\"\n    This function draws histogram visualize frequency distribution numeric values provided string format,\n     with 'Value' on x-axis, 'Frequency' on y-axis 'Histogram Values' as title.\n\n\n    Parameters:\n    data (str): data string format 'value-value-value-...'.\n\n    Returns:\n    ax (matplotlib.axes._axes.Axes): Axes object created histogram.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n\n    Notes:\n    - histogram uses bins calculated as `np.arange(data.min(), data.max()+2) - 0.5`.\n\n    Example:\n    >>> data = '1-2-3-4-5-6-7-8-9-10'\n    >>> ax = task_func(data)\n    \"\"\"\n    # Split data string into list of numeric values\n    data = [float(val) for val in data.split('-')]\n\n    # Create pandas Series\n    series = pd.Series(data)\n\n    # Calculate bins for histogram\n    bins = np.arange(series.min(), series.max()+2) - 0.5\n\n    # Create histogram\n    ax = series.hist(bins=bins, alpha=0.7, color='skyblue', edgecolor='black')\n\n    # Set title and labels\n    ax.set_title('Histogram Values')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n\n    return ax\n"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef task_func(array_length=100, noise_level=0.2):\n    \"\"\"\n    Create noisy sine wave specified length adjusts curve using curve_fit from scipy.optimize data.\n    \n    Parameters:\n    - array_length (int): Length sine wave array. Defaults 100.\n    - noise_level (float): Level noise added sine wave. Defaults 0.2.\n\n    Returns:\n    - Axes object: A plot showing noisy sine wave its adjusted curve.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> ax = task_func(100, 0.2)\n    \"\"\"\n    # Generate sine wave\n    x = np.linspace(0, 10, array_length)\n    y = np.sin(x)\n\n    # Add noise\n    y += np.random.normal(0, noise_level, array_length)\n\n    # Define sine wave function\n    def sine_func(x, a, b, c):\n        return a * np.sin(b * x + c)\n\n    # Fit sine wave\n    popt, pcov = curve_fit(sine_func, x, y)\n\n    # Plot data\n    plt.plot(x, y, label='Noisy sine wave')\n    plt.plot(x, sine_func(x, *popt), label='Adjusted curve', color='r')\n    plt.legend()\n    plt.show()\n\n    return plt.gca()\n\n"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(csv_file):\n    \"\"\"\n    Reads CSV file, normalizes text ASCII, counts words, returns 10 most common words \n    along with their frequencies as matplotlib bar plot list tuples.\n\n    Parameters:\n    csv_file (str): path CSV file.\n\n    Returns:\n    tuple: A tuple containing matplotlib.axes.Axes object for bar plot list 10 most common words \n           with their frequencies.\n\n    Raises:\n    FileNotFoundError: If CSV file cannot found at specified path.\n    IOError: If error reading file.\n\n    Requirements:\n    - unicodedata\n    - csv\n    - collections\n    - matplotlib.pyplot\n\n\n    Example:\n    >>> create_dummy_csv_file('dummy.csv')\n    >>> ax, most_common_words = task_func('dummy.csv')\n    >>> os.remove('dummy.csv')\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    >>> type(most_common_words)\n    <class 'list'>\n\n    Note:\n    function assumes CSV file contains text data file properly formatted.\n    \"\"\"\n    try:\n        with open(csv_file, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text = ' '.join(row for row in reader)\n    except FileNotFoundError:\n        print(f\"File {csv_file} not found.\")\n        raise\n    except IOError as e:\n        print(f\"Error reading file: {e}\")\n        raise\n\n    # Normalize text to ASCII\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n\n    # Split text into words\n    words = text.split()\n\n    # Count words\n    word_counts = Counter(words)\n\n    # Get 10 most common words\n    most_common_words = word_counts.most_common(10)\n\n    # Create bar plot\n    ax = plt.bar([word for word, _ in most_common_words], [count for _, count in most_common_words])\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('10 Most Common Words')\n    plt.xticks(rotation=90)\n\n    return ax, most_common_words\n"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n\ndef task_func(size=1000):\n    \"\"\"\n    Create list normally distributed random numbers plot their histogram probability density function (PDF).\n    \n    Parameters:\n    - size (int): number random numbers generate. Default 1000.\n    \n    Requirements:\n    - numpy\n    - scipy.stats\n    - matplotlib.pyplot\n    \n    Returns:\n    - matplotlib.figure.Figure: A figure object containing histogram PDF plot.\n    \n    Example:\n    >>> import matplotlib\n    >>> fig = task_func(size=500)\n    >>> isinstance(fig, matplotlib.figure.Figure)  # Check if output matplotlib figure object\n    True\n    >>> len(fig.axes[0].lines) == 1  # Ensure one line plot on axes for PDF\n    True\n    >>> len(fig.axes[0].patches) > 10  # Check if histogram bars (patches) present\n    True\n    \"\"\"\n    # Generate list of normally distributed random numbers\n    random_numbers = np.random.normal(0, 1, size)\n    \n    # Create histogram with 30 bins\n    n, bins, patches = plt.hist(random_numbers, bins=30, density=True)\n    \n    # Plot probability density function (PDF) of normal distribution\n    x = np.linspace(-4, 4, 100)\n    plt.plot(x, stats.norm.pdf(x), 'r--')\n    \n    # Set title and labels\n    plt.title('Histogram of Normally Distributed Random Numbers')\n    plt.xlabel('Value')\n    plt.ylabel('Probability Density')\n    \n    # Return figure object\n    return plt.gcf()\n\n"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    \"\"\"\n    Generates RSA public private key pair saves private key file after encrypting it\n    with password using AES encryption. Returns public key filename encrypted\n    private key, along with encryption details for testing. name output file format of\n    \"private_key_<number>.txt\", where <number> hex representation 8 randomly generated bytes.\n\n    Returns:\n    rsa.PublicKey: RSA public key.\n    str: filename where encrypted private key stored.\n    bytes: encryption password, for testing decryption.\n    bytes: encryption nonce, for testing decryption.\n\n    Requirements:\n    - rsa\n    - os\n    - Crypto.Cipher.AES\n    - Crypto.Random.get_random_bytes\n    - base64.b64encode\n    - base54.b64decode\n\n    Examples:\n    >>> pub_key, filename, password, nonce = task_func()\n    >>> isinstance(pub_key, rsa.PublicKey)\n    True\n    >>> isinstance(filename, str)\n    True\n    >>> isinstance(password, bytes)\n    True\n    >>> isinstance(nonce, bytes)\n    True\n    \"\"\"\n    # Generate 8 random bytes\n    random_bytes = get_random_bytes(8)\n    # Generate RSA key pair\n    (n, e) = rsa.newkeys(512)\n    # Save private key to file\n    with open(f\"private_key_{random_bytes.hex()}.txt\", \"w\") as f:\n        # Encrypt private key with AES\n        password = os.urandom(32)\n        nonce = os.urandom(16)\n        cipher = AES.new(password, AES.MODE_GCM, nonce)\n        encrypted_private_key = cipher.encrypt(n.export_key())\n        # Save encrypted private key to file\n        f.write(b64encode(password + nonce + encrypted_private_key).decode())\n    # Return public key, filename, password, and nonce\n    return n, f\"private_key_{random_bytes.hex()}.txt\", password, nonce\n"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    \"\"\"\n    Generates RSA public private keys, encrypts file using AES encryption, encrypts\n    AES key with public RSA key. encrypted file encrypted AES key saved\n    separate new files. This method demonstrates hybrid encryption approach.\n\n    Parameters:\n    file_path (str): path file encrypted.\n\n    Returns:\n    rsa.PublicKey: RSA public key.\n    str: filename encrypted file.\n    str: filename file containing encrypted AES key.\n\n    Requirements:\n    - rsa\n    - os\n    - cryptography.hazmat.backends.default_backend\n    - cryptography.hazmat.primitives.ciphers.Cipher\n    - cryptography.hazmat.primitives.ciphers.algorithms\n    - cryptography.hazmat.primitives.ciphers.modes\n    - cryptography.hazmat.primitives\n    - base64.b64encode\n\n    Examples:\n    >>> pub_key, encrypted_file, encrypted_key_file = task_func('my_file.txt')\n    >>> isinstance(pub_key, rsa.PublicKey)\n    True\n    >>> encrypted_file.endswith('.encrypted')\n    True\n    >>> encrypted_key_file.endswith('.encrypted')\n    True\n    \"\"\"\n    # Generate RSA public and private keys\n    (n, e) = rsa.newkeys(512).publickey().public_numbers()\n    pub_key = rsa.PublicKey(n, e)\n\n    # Generate AES key\n    aes_key = os.urandom(32)\n\n    # Generate AES initialization vector\n    iv = os.urandom(16)\n\n    # Create AES cipher object\n    cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv), backend=default_backend())\n\n    # Encrypt file using AES\n    encryptor = cipher.encryptor()\n    with open(file_path, 'rb') as file:\n        data = file.read()\n    padder = padding.PKCS7(128).padder()\n    padded_data = padder.update(data) + padder.finalize()\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    # Encrypt AES key with RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, pub_key)\n\n    # Save encrypted file\n    with open(file_path + '.encrypted', 'wb') as encrypted_file:\n        encrypted_file.write(iv + encrypted_data)\n\n    # Save encrypted AES key\n    with open(file_path + '.encrypted.key', 'wb') as encrypted_key_file:\n        encrypted_key_file.write(encrypted_aes_key)\n\n    return pub_key, file_path + '.encrypted', file_path + '.encrypted.key'\n"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    \"\"\"\n    Extracts text href attributes all anchor tags from given URL's HTML content, \n    returns this data pandas DataFrame along with time data extraction.\n\n    Parameters:\n    url (str): URL from fetch HTML content.\n\n    Returns:\n    pandas.DataFrame: A DataFrame with columns 'text', 'href', 'fetch_time'. Each row \n                      corresponds anchor tag HTML, with 'text' 'href' containing \n                      text hyperlink reference anchor tag, respectively. \n                      'fetch_time' contains timestamp when data was fetched format\n                        'YYYY-MM-DD HH:MM:SS'.\n\n    Raises:\n    ValueError: If provided URL invalid or empty.\n    URLError: If issue with network connectivity or server.\n\n    Requirements:\n    - urllib.request\n    - pyquery\n    - datime\n    - pandas\n    - urllib.error\n\n    Example:\n    >>> df = task_func('https://en.wikibooks.org/wiki/Main_Page')\n\n    Note:\n    function requires internet connectivity fetch HTML content.\n    \"\"\"\n    if not url:\n        raise ValueError('URL cannot be empty')\n\n    try:\n        # Fetch HTML content from given URL\n        response = urllib.request.urlopen(url)\n        # Parse HTML content using pyquery\n        doc = pq(response.read())\n        # Extract text and href attributes from all anchor tags\n        data = [{'text': a.text, 'href': a.attr['href']} for a in doc('a').items()]\n        # Create pandas DataFrame\n        df = pd.DataFrame(data)\n        # Add fetch time column\n        df['fetch_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        return df\n    except (ValueError, URLError) as e:\n        raise e\n"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Create sensor data for specified number hours save CSV file\n    with coloumns 'Time', 'Temperature', 'Humidity' 'Pressure'.\n\n    Parameters:\n    - hours (int): number hours for sensor data generated.\n    - output_dir (str, optional): output file path\n\n    Returns:\n    - hours (int): Number hours generate data for.\n\n\n    Requirements:\n    - datetime\n    - os\n    - random\n    - csv\n\n    Example:\n    >>> file_path = task_func(1)  # Generate data for 1 hour\n    >>> os.path.exists(file_path)  # Check if file was actually created\n    True\n    >>> isinstance(file_path, str)  # Validate return type string\n    True\n    >>> 'sensor_data.csv' file_path  # Ensure filename correct\n    True\n    \"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate sensor data for specified number of hours\n    data = []\n    for hour in range(hours):\n        for sensor in SENSORS:\n            data.append({\n                'Time': datetime.now() + timedelta(hours=hour),\n                'Sensor': sensor,\n                'Value': randint(0, 100)\n            })\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'sensor_data.csv')\n    with open(file_path, 'w', newline='') as csvfile:\n        fieldnames = ['Time', 'Sensor', 'Value']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for row in data:\n            writer.writerow(row)\n\n    return file_path\n"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generates traffic data for different vehicle types over specified number hours,\n    saves data CSV file with coloumns 'Time', 'Car', 'Bus', 'Truck', 'Bike',\n    plots data line chart with 'Time' on x-axis 'Vehicle Count' on y-axis.\n\n    Parameters:\n    - hours (int): Number hours generate data for.\n    - output_dir (str, optional): output file path\n\n    Returns:\n    - tuple: Path CSV file matplotlib axes object line plot.\n\n    Requirements:\n    - pandas\n    - os\n    - csv\n    - matplotlib.pyplot\n    - random\n    - datetime\n\n    Example:\n    >>> import matplotlib\n    >>> file_path, ax = task_func(2)  # Generate data for 2 hours\n    >>> isinstance(file_path, str)\n    True\n    >>> 'traffic_data.csv' file_path\n    True\n    >>> isinstance(ax, matplotlib.axes.Axes)\n    True\n    \"\"\"\n    # Generate data\n    data = []\n    for hour in range(hours):\n        for vehicle_type in VEHICLE_TYPES:\n            count = randint(0, 100)\n            data.append({\n                'Time': f'{hour}h',\n                'Vehicle Type': vehicle_type,\n                'Count': count\n            })\n\n    # Save data to CSV file\n    file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(file_path, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=['Time', 'Vehicle Type', 'Count'])\n        writer.writeheader()\n        writer.writerows(data)\n\n    # Plot data\n    df = pd.read_csv(file_path)\n    ax = df.groupby('Time')['Count'].sum().plot(kind='line', figsize=(10, 6))\n    ax.set_title('Traffic Count Over Time')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n\n    return file_path, ax\n\n"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Generate weather data for specified number hours, save CSV file with colomns 'Time' 'Condition'\n     back up file backup directory.\n    \n    Parameters:\n    - hours (int): number hours for weather data generated.\n    - output_dir (str, optional): output file path\n\n    Returns:\n    - str: path generated CSV file.\n    \n    Requirements:\n    - datetime\n    - os\n    - random\n    - csv\n    - shutil\n    \n    Example:\n    >>> 'weather_data.csv' task_func(24)\n    True\n    >>> 'weather_data.csv' task_func(10)\n    True\n    \"\"\"\n    # Create output directory if not exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate weather data\n    data = []\n    for i in range(hours):\n        time = datetime.now() + datetime.timedelta(hours=i)\n        condition = WEATHER_CONDITIONS[randint(0, len(WEATHER_CONDITIONS) - 1)]\n        data.append([time.strftime('%Y-%m-%d %H:%M:%S'), condition])\n\n    # Save data to CSV file\n    filename = 'weather_data.csv'\n    filepath = os.path.join(output_dir, filename)\n    with open(filepath, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['Time', 'Condition'])  # header\n        writer.writerows(data)\n\n    # Backup file\n    backup_dir = os.path.join(output_dir, 'backup')\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n    backup_filename = f'{filename}.bak'\n    backup_filepath = os.path.join(backup_dir, backup_filename)\n    shutil.copy(filepath, backup_filepath)\n\n    return filepath\n\n"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generate visualize Pandas DataFrame results football matches for multiple teams 'Team' with\n    random goals 'Goals' penalties 'Penalty Cost'. Penalties converted into fines according penalty costs.\n\n    Parameters:\n    goals (int): maximum number goals team can score match.\n    penalties (int): maximum number penalties team can receive match.\n\n    Returns:\n    pd.DataFrame: A dataframe containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals penalty costs.\n\n    Requirements:\n    - pandas\n    - seaborn\n    - matplotlib.pyplot\n    - random\n\n    Example:\n    >>> df, plots = task_func(5, 3)\n    \"\"\"\n    # Generate random match results\n    results = []\n    for team in TEAMS:\n        goals_scored = randint(0, goals)\n        penalties_received = randint(0, penalties)\n        fine = penalties_received * PENALTY_COST\n        results.append({\n            'Team': team,\n            'Goals': goals_scored,\n            'Penalties': penalties_received,\n            'Fine': fine\n        })\n\n    # Create a Pandas DataFrame from the results\n    df = pd.DataFrame(results)\n\n    # Create a Seaborn plot for goals\n    goals_plot = sns.barplot(x='Team', y='Goals', data=df)\n    plt.title('Goals Scored by Team')\n    plt.show()\n\n    # Create a Seaborn plot for penalty costs\n    penalty_costs_plot = sns.barplot(x='Team', y='Fine', data=df)\n    plt.title('Penalty Costs by Team')\n    plt.show()\n\n    return df, [goals_plot, penalty_costs_plot]\n"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\n\ndef task_func(rows):\n    \"\"\"\n    Create Pandas DataFrame with random integer values between 0 9 for given number rows.\n    Count non-zero values each column visualize this information using bar plot.\n    \n    Parameters:\n    rows (int): number rows DataFrame.\n\n    Returns:\n    tuple: A tuple containing following elements:\n        - DataFrame: generated DataFrame with random integer values.\n        - Axes: matplotlib Axes object containing bar plot.\n\n    Requirements:\n    - numpy\n    - pandas\n    - matplotlib.pyplot\n\n    Example:\n    >>> df, ax = task_func(10)\n    >>> print(ax.title.get_text())  # Should return 'Non-Zero Value Counts'\n    Non-Zero Value Counts\n    \"\"\"\n    # Generate random integer values between 0 and 9 for given number of rows\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    \n    # Create DataFrame with generated data\n    df = pd.DataFrame(data, columns=COLUMNS)\n    \n    # Count non-zero values each column\n    non_zero_counts = df.apply(lambda x: (x != 0).sum())\n    \n    # Create bar plot\n    ax = non_zero_counts.plot(kind='bar', figsize=(10, 6))\n    \n    # Set title and labels\n    ax.set_title('Non-Zero Value Counts')\n    ax.set_xlabel('Column')\n    ax.set_ylabel('Count')\n    \n    return df, ax\n\n"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\n\ndef task_func(num_students):\n    \"\"\"\n    Generate Pandas DataFrame displays grades randomly selected group students multiple courses.\n    Calculate average grade each course, number students with passing grade (>= 60), \n    visualize this information using bar plot with title 'Course-wise Average Passing Grade Counts'.\n\n    Parameters:\n    num_students (int): number students sample.\n\n    Returns:\n    Tuple[pd.DataFrame, plt.Axes]: A tuple containing generated DataFrame bar plot's Axes object.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - random\n    - typing\n\n    Example:\n    >>> df, ax = task_func(50)\n    >>> ax.get_title()\n    'Course-wise Average Passing Grade Counts'\n    \"\"\"\n    # Generate random grades for each student in each course\n    np.random.seed(0)\n    grades = np.random.randint(0, 100, size=(num_students, 5))  # 5 courses\n\n    # Create a DataFrame with student IDs and grades\n    df = pd.DataFrame({\n        'Student ID': range(1, num_students + 1),\n        'Course 1': grades[:, 0],\n        'Course 2': grades[:, 1],\n        'Course 3': grades[:, 2],\n        'Course 4': grades[:, 3],\n        'Course 5': grades[:, 4]\n    })\n\n    # Calculate average grade for each course\n    avg_grades = df[['Course 1', 'Course 2', 'Course 3', 'Course 4', 'Course 5']].mean()\n\n    # Calculate number of students with passing grade (>= 60) for each course\n    passing_counts = df[['Course 1', 'Course 2', 'Course 3', 'Course 4', 'Course 5']].ge(60).sum()\n\n    # Create a bar plot to visualize the information\n    fig, ax = plt.subplots()\n    ax.bar(avg_grades.index, avg_grades.values, label='Average Grade')\n    ax.bar(avg_grades.index, passing_counts.values, label='Passing Count')\n    ax.set_title('Course-wise Average Passing Grade Counts')\n    ax.legend()\n\n    return df, ax\n"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fit exponential decay function indices array where first column matches target value.\n\n    Parameters:\n    - array (np.ndarray): A numpy array where first column will searched for target value.\n    - target_value (float or int): value first column filter data for fitting.\n\n    Returns:\n    - tuple: Containing optimized parameters fitting function (popt) matplotlib Axes object.\n\n    Requirements:\n    - numpy\n    - scipy.optimize\n    - matplotlib.pyplot\n\n    Example:\n    >>> import numpy as np\n    >>> array = np.array([[1, 2], [1, 3], [1, 4], [2, 5], [2, 6]])\n    >>> target = 1\n    >>> params, ax = task_func(array, target)\n    >>> len(params)\n    3\n    \"\"\"\n    # Filter array for target value\n    filtered_array = array[array[:, 0] == target_value]\n\n    # Define exponential decay function\n    def exponential_decay(x, a, b, c):\n        return a * np.exp(-b * x) + c\n\n    # Fit function to filtered array\n    popt, pcov = optimize.curve_fit(exponential_decay, filtered_array[:, 1], filtered_array[:, 2])\n\n    # Create matplotlib plot\n    fig, ax = plt.subplots()\n    ax.plot(filtered_array[:, 1], filtered_array[:, 2], 'o', label='Original data')\n    ax.plot(filtered_array[:, 1], exponential_decay(filtered_array[:, 1], *popt), 'r-', label='Fitted curve')\n    ax.legend()\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Exponential decay fit')\n\n    return popt, ax\n\n"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    Performs topic extraction from collection text documents using Non-Negative Matrix Factorization (NMF).\n    This function first preprocesses input texts by removing non-alphanumeric characters (excluding spaces),\n    converting all characters lowercase, removing stopwords. It vectorizes processed texts\n    using TF-IDF applies NMF extract specified number topics. Each topic represented as list\n    its most significant words based on NMF component weights.\n\n    Parameters:\n    - texts (list str): input text documents from extract topics.\n    - num_topics (int): number topics extract.\n\n    Returns:\n    - list list str: A list where each element list words representing topic.\n\n    Requirements:\n    - re\n    - nltk\n    - sklearn.decomposition\n    - sklearn.feature_extraction.text\n\n    Example:\n    >>> texts = [\n    ...     \"Data science involves study data.\",\n    ...     \"Machine learning provides systems ability learn from data.\",\n    ...     \"Python programming language used data science.\"\n    ... ]\n    >>> topics = task_func(texts, 2)\n    >>> print(topics)\n    [['data', 'science'], ['systems', 'provides']]\n\n    Note: exact output may vary depending on TF-IDF vectorization NMF initialization.\n    \"\"\"\n    # Preprocess input texts\n    preprocessed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        text = ALPHANUMERIC.sub(' ', text)\n        # Convert all characters to lowercase\n        text = text.lower()\n        # Remove stopwords\n        tokens = nltk.word_tokenize(text)\n        tokens = [token for token in tokens if token not in STOPWORDS]\n        # Join tokens back into string\n        preprocessed_text = ' '.join(tokens)\n        preprocessed_texts.append(preprocessed_text)\n\n    # Vectorize preprocessed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)\n\n    # Apply NMF to extract specified number of topics\n    nmf = NMF(n_components=num_topics)\n    nmf_matrix = nmf.fit_transform(tfidf_matrix)\n\n    # Get most significant words for each topic\n    topics = []\n    for i in range(num_topics):\n        topic_words = []\n        for j in range(len(vectorizer.get_feature_names_out())):\n            if nmf_matrix[i, j] > 0:\n                topic_words.append(vectorizer.get_feature_names_out()[j])\n        topics.append(topic_words)\n\n    return topics\n\n"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from list texts using gensim Word2Vec model nltk.corpus.stopwords.\n    texts first cleaned by removing all non-alphanumeric characters except space,\n    lowercased, stop words removed.\n\n    Parameters:\n    texts (list): A list strings.\n    stopwords (list, optional): A list stopwords removed. If not provided, nltk's stopwords will used.\n\n    Returns:\n    Word2Vec: A trained Word2Vec model.\n\n    Requirements:\n    - re\n    - nltk\n    - gensim\n\n    Example:\n    >>> texts = [\"Hello, World!\", \"Machine Learning great\", \"Python my favorite programming language\"]\n    >>> model = task_func(texts)\n    >>> vector = model.wv['python']\n    \"\"\"\n    # Clean texts\n    cleaned_texts = []\n    for text in texts:\n        cleaned_text = ALPHANUMERIC.sub(' ', text).lower()\n        cleaned_texts.append(cleaned_text)\n\n    # Tokenize texts\n    tokenized_texts = []\n    for text in cleaned_texts:\n        tokens = nltk.word_tokenize(text)\n        tokenized_texts.append(tokens)\n\n    # Remove stopwords\n    if stopwords is None:\n        stopwords = nltk.corpus.stopwords.words('english')\n    filtered_texts = []\n    for tokens in tokenized_texts:\n        filtered_tokens = [token for token in tokens if token not in stopwords]\n        filtered_texts.append(filtered_tokens)\n\n    # Train Word2Vec model\n    model = Word2Vec(filtered_texts, min_count=1, vector_size=100, window=5, workers=4)\n    return model\n\n"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    \"\"\"\n    Processes JSON files directory. function reads each JSON file alphabetically into DataFrame inserts \"Source\" column specifies filename. processed files moved \"processed\" subdirectory. path may not exist initially.\n    \n    Parameters:\n    - path (str): path directory containing JSON files.\n    \n    Returns:\n    - df (pandas.DataFrame): A DataFrame containing data from all processed files.\n\n    Requirements:\n    - pandas\n    - json\n    - os\n    - shutil\n    \n    Example:\n    >>> os.mkdir('data')\n    >>> with open('data/a.json', 'w') as f:\n    ...     f.write('[{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}]')\n    ...\n    36\n    >>> with open('data/b.json', 'w') as f:\n    ...     f.write('[{\"a\": 5, \"b\": 6}, {\"a\": 7, \"b\": 8}]')\n    ...\n    36\n    >>> df = task_func('data')\n    >>> print(df)\n        b  source\n    0  5  6  b.json\n    1  7  8  b.json\n    0  1  2  a.json\n    1  3  4  a.json\n    >>> shutil.rmtree('data')\n    \"\"\"\n    # Create \"processed\" subdirectory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n\n    # Initialize empty list to store DataFrames\n    dfs = []\n\n    # Iterate over files in the directory\n    for filename in sorted(os.listdir(path)):\n        # Check if file is a JSON file\n        if filename.endswith(\".json\"):\n            # Read JSON file into DataFrame\n            with open(os.path.join(path, filename), 'r') as f:\n                df = pd.read_json(f)\n            # Insert \"Source\" column with filename\n            df['Source'] = filename\n            # Append DataFrame to list\n            dfs.append(df)\n\n    # Concatenate DataFrames into one\n    df = pd.concat(dfs, ignore_index=True)\n\n    # Move processed files to \"processed\" subdirectory\n    for filename in os.listdir(path):\n        if filename.endswith(\".json\"):\n            shutil.move(os.path.join(path, filename), os.path.join(processed_path, filename))\n\n    return df\n"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    \"\"\"\n    Create delete CSV file \"task_func_data/Output.txt\" with sensor data for temperature humidity.\n    data generated randomly, written append mode, file deleted after use.\n\n    Returns:\n    - Returns path CSV file \"task_func_data/Output.txt\" before deletion.\n\n    Requirements:\n    - os\n    - csv\n    - random\n    - datatime\n\n    Example:\n    >>> task_func()\n    \"\"\"\n    # Create directory if not exist\n    if not os.path.exists('task_func_data'):\n        os.makedirs('task_func_data')\n\n    # Generate random data\n    data = [\n        [random.random() * 100, random.random() * 100, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")]\n        for _ in range(10)\n    ]\n\n    # Write data to CSV file\n    with open('task_func_data/Output.txt', 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(data)\n\n    # Return path to CSV file\n    return 'task_func_data/Output.txt'\n"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    \"\"\"\n    Scrape data from given URL save scraped data CSV file.\n\n    Parameters:\n    - url (str): URL scrape data from.\n\n    Returns:\n    - CSV_FILE_PATH (str): path CSV file where scraped data saved.\n\n    Requirements:\n    - urllib\n    - bs4\n    - csv\n    - os\n\n    Example:\n    >>> task_func('http://www.example.com/')\n    'scraped_data.csv'\n    \"\"\"\n    # Send HTTP request to the given URL\n    response = urllib.request.urlopen(url)\n\n    # Parse HTML content\n    soup = BeautifulSoup(response, 'html.parser')\n\n    # Find all elements with class 'data'\n    data_elements = soup.find_all(class_='data')\n\n    # Create a list to store scraped data\n    scraped_data = []\n\n    # Iterate over each data element\n    for element in data_elements:\n        # Extract text from the element\n        text = element.get_text()\n\n        # Append extracted text to the scraped data list\n        scraped_data.append(text)\n\n    # Create a CSV writer\n    with open(CSV_FILE_PATH, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        # Write scraped data to the CSV file\n        writer.writerow(scraped_data)\n\n    # Return the path to the CSV file\n    return CSV_FILE_PATH\n"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state = 0) -> float:\n    \"\"\"\n    Train linear regression model return model score test set.\n\n    provided DataFrame used as training data, where target_column used\n    as target training model. Before training provided data split \n    into training test set using test_size random_state parameters. \n\n    Parameters:\n    data (DataFrame): input data for training.\n    target_column (str): column predict.\n    random_state (int): seed for train-test split. Defaults 0\n    test_size (float): fractional size test set. Defaults 0.2\n\n\n    Returns:\n    float: model's score.\n\n    Raises:\n    ValueError: If data not DataFrame.\n    ValueError: If data empty.\n    ValueError: If target_column ist not column data.\n    ValueError: If data contains values not numeric.\n    ValueError: If random_state not integer.\n    ValueError: If test_size not between 0 1.\n\n    Requirements:\n    - pandas\n    - sklearn.model_selection.train_test_split\n    - sklearn.linear_model.LinearRegression\n    - numpy\n\n    Example:\n    >>> rng = np.random.default_rng(seed=42)\n    >>> data = pd.DataFrame({\n    ...     'x1': rng.random(100),\n    ...     'x2': rng.random(100),\n    ...     'y': rng.random(100)\n    ... })\n    >>> result = task_func(data, 'y', random_state=2, test_size=0.3)\n    >>> result\n    -0.25486317198996633\n\n    >>> data = pd.DataFrame({\n    ...     'x1': rng.random(500),\n    ... })\n    >>> data['y'] = data['x1'] * 2 + 1\n    >>> result = task_func(data, 'y', random_state=9, test_size=0.1)\n    >>> result\n    1.0\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be DataFrame\")\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n    if target_column not in data.columns:\n        raise ValueError(\"Target column not in data\")\n    if not data.select_dtypes(include=[np.number]).empty:\n        raise ValueError(\"Data contains values not numeric\")\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be integer\")\n    if not 0 <= test_size <= 1:\n        raise ValueError(\"Test size must be between 0 and 1\")\n\n    X = data.drop(target_column, axis=1)\n    y = data[target_column]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    return model.score(X_test, y_test)  # type: ignore[operator]  # mypy bug #1\n"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    \"\"\"\n    Creates random DataFrame with 100 records. Each record consists ID (ranging from 1 100), \n    Name (randomly selected from provided lists Latin other names), \n    Date Birth (randomly generated dates between specified years), \n    Email (constructed using name, year birth, provided email domain).\n    \n    Improperly encoded Latin characters names corrected during process.\n    \n    Parameters:\n    - start_year (int): starting year for range birth years. Defaults 1980.\n    - end_year (int): ending year for range birth years. Defaults 2000.\n    - email_domain (str): domain used for email addresses. Defaults 'example.com'.\n    - latin_names (list str): A list Latin names used generation.\n        Defaults to: latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz']\n    - other_names (list str): A list other names used generation.\n        Defaults to: other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones']\n    - rng_seed (int): seed for rng.\n\n    Returns:\n    - DataFrame: A pandas DataFrame containing generated user data. DataFrame has columns: \n               'ID', 'Name', 'Date Birth', 'Email'.\n\n    Requirements:\n    - pandas\n    - numpy\n    - codecs\n    - re\n    - datetime\n\n    Examples:\n    >>> df = task_func(rng_seed=1)\n    >>> print(df)   \n         ID     Name Date Birth                    Email\n    0     1    Brown    1992-09-10    brown1992@example.com\n    1     2    Smith    1996-02-13    smith1996@example.com\n    2     3    Jones    1986-10-19    jones1986@example.com\n    3     4    G\u00f3mez    2000-12-11    g\u00f3mez2000@example.com\n    4     5    G\u00f3mez    1984-08-24    g\u00f3mez1984@example.com\n    ..  ...      ...           ...                      ...\n    95   96  Johnson    1990-09-17  johnson1990@example.com\n    96   97    Brown    1992-10-14    brown1992@example.com\n    97   98    Mu\u00f1oz    1998-05-04    mu\u00f1oz1998@example.com\n    98   99    Mu\u00f1oz    1982-01-01    mu\u00f1oz1982@example.com\n    99  100    Jones    1990-03-28    jones1990@example.com\n    <BLANKLINE>\n    [100 rows x 4 columns]\n\n    >>> df = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)\n    >>> print(df)\n         ID      Name        Date Birth                Email\n    0     1   Sopet\u00f3n  0952-09-01 00:00:00   sopet\u00f3n952@test.at\n    1     2     Brown  0875-10-10 00:00:00     brown875@test.at\n    2     3   Sopet\u00f3n  0605-08-15 00:00:00   sopet\u00f3n605@test.at\n    3     4     G\u00f3mez  0337-11-23 00:00:00     g\u00f3mez337@test.at\n    4     5     G\u00f3mez  0641-04-27 00:00:00     g\u00f3mez641@test.at\n    ..  ...       ...                  ...                  ...\n    95   96     Brown  0044-05-17 00:00:00      brown44@test.at\n    96   97  Williams  0530-01-21 00:00:00  williams530@test.at\n    97   98   Johnson  1005-12-15 00:00:00  johnson1005@test.at\n    98   99    M\u00e9ndez  1134-07-19 00:00:00   m\u00e9ndez1134@test.at\n    99  100   Johnson  0696-08-22 00:00:00   johnson696@test.at\n    <BLANKLINE>\n    [100 rows x 4 columns]\n    \"\"\"\n    # Set seed for rng\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n\n    # Generate random birth years\n    birth_years = np.random.randint(start_year, end_year + 1, 100)\n\n    # Generate random names\n    names = []\n    for i in range(100):\n        if np.random.rand() < 0.5:\n            names.append(np.random.choice(latin_names))\n        else:\n            names.append(np.random.choice(other_names))\n\n    # Correctly encode Latin characters\n    names = [re.sub(r'([aeiou])\\1+', r'\\1\\1', name) for name in names]\n\n    # Generate random dates of birth\n    dates_birth = [datetime.date(birth_year, np.random.randint(1, 13), np.random.randint(1, 29)) for birth_year in birth_years]\n\n    # Generate email addresses\n    emails = [f'{name}{birth_year}{email_domain}' for name, birth_year in zip(names, birth_years)]\n\n    # Create DataFrame\n    df = pd.DataFrame({\n        'ID': range(1, 101),\n        'Name': names,\n        'Date Birth': dates_birth,\n        'Email': emails\n    })\n\n    return df\n"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    \"\"\"\n    Read list dictionaries from JSON file, calculate mean median for each key, write results CSV file.\n    \n    Parameters:\n    - input_file (str): input JSON file name.\n    - output_file (str): output CSV file name.\n\n    Returns:\n    - dict: A dictionary where each key field from input JSON each value another dictionary with mean median field.\n\n    Requirements:\n    - numpy\n    - collections\n    - json\n    - csv\n\n    Example:\n    >>> task_func('data.json', 'stats.csv')\n    \"\"\"\n    # Read JSON file\n    with open(input_file, 'r') as f:\n        data = json.load(f)\n\n    # Create a dictionary to store the results\n    results = defaultdict(lambda: {'mean': 0, 'median': 0})\n\n    # Iterate over each dictionary in the list\n    for item in data:\n        # Iterate over each key in the dictionary\n        for key, value in item.items():\n            # If the key is not in the results dictionary, add it\n            if key not in results:\n                results[key] = {'mean': 0, 'median': 0}\n            # Add the value to the list of values for this key\n            results[key]['values'].append(value)\n\n    # Calculate the mean and median for each key\n    for key, values in results.items():\n        # Calculate the mean\n        values['mean'] = np.mean(values['values'])\n        # Calculate the median\n        values['median'] = np.median(values['values'])\n\n    # Write the results to a CSV file\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['key', 'mean', 'median'])\n        for key, values in results.items():\n            writer.writerow([key, values['mean'], values['median']])\n\n    return results\n"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    \"\"\"\n    Process files from dictionary by checking if file exists, if has content, copies target directory.\n    \n    Parameters:\n    - kwargs (dict): A dictionary where keys full file paths values file content.\n    - target_dir (str, optional): directory where files will copied to. Defaults 'non_none_files'.\n\n    Returns:\n    - copied_files (list): A list full file paths were copied.\n\n    Requirements:\n    - os\n    - pathlib.Path\n    - shutil\n\n    Example:\n    >>> files = {'/path/to/file1.txt': 'Hello', '/path/to/file2.txt': None, '/path/to/file3.txt': 'World'}\n    >>> task_func(files)\n    >>> files = {'/path/to/file4.txt': 'Another', '/path/to/file5.txt': 'Example'}\n    >>> task_func(files, target_dir=\"another_directory\")\n    \"\"\"\n    copied_files = []\n    for file_path, file_content in kwargs.items():\n        if file_content is not None:\n            file_path = Path(file_path)\n            target_path = Path(target_dir) / file_path.name\n            if not target_path.exists():\n                shutil.copy(file_path, target_path)\n                copied_files.append(target_path)\n    return copied_files\n"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n\n    \"\"\"\n    Processes CSV files directory based on specified pattern creates new files with altered names while preserving content, you've laid out solid foundation with your initial tests.\n\n    Parameters:\n    - directory (str): path directory containing CSV files processed. \n    - pattern (str, optional): A regular expression pattern filenames interest should match.\n\n    Returns:\n    - new_files (list): A list strings, where each string filename new CSV file created by function.\n\n    Requirements:\n    - re\n    - os\n    - pathlib\n    - csv\n\n    Example:\n    >>> task_func(\"/mnt/data/test_data\")\n    ['Shan.csv', 'Shannon.csv']\n    \"\"\"\n    # Initialize an empty list to store new file names\n    new_files = []\n\n    # Iterate over each file in the specified directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the specified pattern\n        if re.match(pattern, filename):\n            # Extract the base filename without the number\n            base_filename = re.match(pattern, filename).group(1)\n            # Create a new filename by removing the number and appending '.csv'\n            new_filename = f\"{base_filename}.csv\"\n            # Append the new filename to the list\n            new_files.append(new_filename)\n\n            # Open the original file and read its content\n            with open(os.path.join(directory, filename), 'r') as file:\n                reader = csv.reader(file)\n                # Open a new file with the altered name and write the content\n                with open(os.path.join(directory, new_filename), 'w', newline='') as new_file:\n                    writer = csv.writer(new_file)\n                    for row in reader:\n                        writer.writerow(row)\n\n    # Return the list of new file names\n    return new_files\n"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    \"\"\"\n    Unzip all zip files directory whose name matches certain pattern by splitting filename last time \"-\" occurs using prefix part filename as directory extract.\n    \n    Parameters:\n    - directory (str): directory where zip files located.\n    - pattern (str): Regex pattern match zip files.\n\n    Returns:\n    - list: A list directories where files were extracted.\n\n    Requirements:\n    - os\n    - re\n    - zipfile\n\n    Example:\n    >>> task_func('/tmp/my_data')\n    ('/tmp/backup/backup_20230827010101', [])\n    \"\"\"\n    # Get all zip files in the directory\n    zip_files = [os.path.join(directory, f) for f in os.listdir(directory) if re.match(pattern, f)]\n    \n    # Initialize a list to store the directories where files were extracted\n    extracted_dirs = []\n    \n    # Iterate over each zip file\n    for zip_file in zip_files:\n        # Get the prefix part of the filename\n        prefix = re.match(pattern, os.path.basename(zip_file)).group(1)\n        \n        # Create a directory to extract the zip file\n        extract_dir = os.path.join(directory, prefix)\n        \n        # Check if the directory already exists\n        if not os.path.exists(extract_dir):\n            # Create the directory\n            os.makedirs(extract_dir)\n        \n        # Extract the zip file\n        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n            zip_ref.extractall(extract_dir)\n        \n        # Add the directory to the list\n        extracted_dirs.append(extract_dir)\n    \n    # Return the list of directories where files were extracted\n    return extracted_dirs, []\n"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\n\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    \"\"\"\n    Archive all files match particular pattern delete original files.\n    \n    Parameters:\n    - pattern (str): pattern match files.\n    \n    Returns:\n    - archive_file (str): archive file path.\n    \n    Requirements:\n    - subprocess\n    - os\n    - glob\n    \n    Example:\n    >>> task_func('*.txt')\n    \n    Note: This function will return archive file path.\n    \"\"\"\n    # Get all files match pattern\n    files = glob.glob(pattern)\n    \n    # Create archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, 'archive.tar.gz')\n    \n    # Archive files\n    with tarfile.open(archive_file, 'w:gz') as tar:\n        for file in files:\n            tar.add(file)\n    \n    # Delete original files\n    for file in files:\n        os.remove(file)\n    \n    return archive_file\n"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\n\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\n\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\n\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    \"\"\"\n    Count total number goals penalties from CSV file update with given goals penalties.\n\n    Parameters:\n    - goals (dict): A dictionary where keys team names values numbers goals scored.\n    - penalties (dict): A dictionary where keys team names values numbers penalties incurred.\n\n    Returns:\n    - count (Counter.collections): A Counter object with total counts goals penalties.\n\n    Requirements:\n    - csv\n    - os\n    - collections.Counter\n\n    Example:\n    >>> goals = {'Team A': 3, 'Team B': 2, 'Team C': 1, 'Team D': 0, 'Team E': 2}\n    >>> penalties = {'Team A': 1, 'Team B': 0, 'Team C': 2, 'Team D': 3, 'Team E': 1}\n    >>> counts = task_func(goals, penalties)\n    >>> print(counts)\n    Counter({'goals': 8, 'penalties': 7})\n    \"\"\"\n    # Read the CSV file\n    with open(csv_file_path, 'r') as file:\n        reader = csv.reader(file)\n        # Skip the header row\n        next(reader)\n        # Extract team names, goals, and penalties\n        teams = [row[0] for row in reader]\n        goals = [int(row[1]) for row in reader]\n        penalties = [int(row[2]) for row in reader]\n\n    # Update the goals and penalties dictionaries with the new data\n    for team, goal, penalty in zip(teams, goals, penalties):\n        goals[team] += goal\n        penalties[team] += penalty\n\n    # Count the total number of goals and penalties\n    count = Counter()\n    for team in goals:\n        count['goals'] += goals[team]\n        count['penalties'] += penalties[team]\n\n    return count\n"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Move files from source directory target directory based on specified pattern.\n\n    This function iterates through all files source directory, if file's name matches\n    specified pattern, moved target directory.\n\n    Parameters:\n    - source_dir (str): path source directory.\n    - target_dir (str): path target directory.\n    - file_pattern (str, optional): regular expression pattern filenames must match order\n                                   moved. Default r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b',\n                                   matches filenames consist alphanumeric characters\n                                   have extensions txt, doc, or docx.\n\n    Returns:\n    - moved_files_count (int): number files were successfully moved from source directory target directory.\n\n    Requirements:\n    - re\n    - os\n    - shutil\n\n    Example:\n    >>> task_func('/path/to/source', '/path/to/target')\n    3\n    This example would move 3 files from '/path/to/source' '/path/to/target' if their filenames match default pattern.\n    \"\"\"\n    moved_files_count = 0\n    for filename in os.listdir(source_dir):\n        if re.match(file_pattern, filename):\n            file_path = os.path.join(source_dir, filename)\n            shutil.move(file_path, target_dir)\n            moved_files_count += 1\n    return moved_files_count\n"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate similarity values between two texts based on cosine similarity Levenshtein ratio.\n    texts first cleaned by removing all non-alphanumeric characters except spaces converted lowercase.\n    Cosine similarity computed based on term frequency each text.\n    Levenshtein ratio computed using 'ratio' function from 'python-Levenshtein' library, measures similarity two strings as number between 0 1.\n\n    Parameters:\n    - text1 (str): first string compare.\n    - text2 (str): second string compare.\n\n    Returns:\n    - tuple: A tuple containing cosine similarity Levenshtein ratio as floats. \n        - cosine similarity (float): cosine similarity ranges from 0 1,\n           where 1 means identical term frequency, 0 indicates no common terms. \n        - levenshtein_ratio (float): Levenshtein ratio also ranges from 0 1,\n           where 1 means strings identical, 0 means they completely different.\n\n    Requirements:\n    - re\n    - numpy\n    - collections\n    - Levenshtein\n\n    Example:\n    >>> task_func(\"Hello, World!\", \"Hello World\")\n    (0.9999999999999998, 0.9565217391304348)\n    \"\"\"\n    # Clean the texts by removing non-alphanumeric characters except spaces and convert to lowercase\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n\n    # Split the texts into words\n    words1 = text1.split()\n    words2 = text2.split()\n\n    # Count the frequency of each word in each text\n    freq1 = Counter(words1)\n    freq2 = Counter(words2)\n\n    # Calculate the cosine similarity\n    # Convert the frequency dictionaries to numpy arrays\n    arr1 = np.array(list(freq1.values()))\n    arr2 = np.array(list(freq2.values()))\n\n    # Calculate the dot product and magnitudes\n    dot_product = np.dot(arr1, arr2)\n    magnitude1 = np.linalg.norm(arr1)\n    magnitude2 = np.linalg.norm(arr2)\n\n    # Calculate the cosine similarity\n    cosine_similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 * magnitude2 != 0 else 0\n\n    # Calculate the Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n\n    return cosine_similarity, levenshtein_ratio\n"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    \"\"\"\n    Generate all permutations given list numbers calculate sum \n    factorials each number each permutation.\n    If empty list given, function returns empty lists.\n\n    Parameters:\n    numbers (list int): A list integers permute calculate \n                           factorial sums.\n\n    Returns:\n    list int: A list containing sums factorials each number \n                 each permutation.\n    list list int: A list containing all permutations numbers.\n\n    Raises:\n    TypeError: If numbers not list integers.\n    ValueError: If input numbers negative.\n\n    Requirements:\n    - functools.reduce\n    - itertools.permutations\n    - math.factorial\n\n    Example:\n    >>> fac, perm = task_func([1, 2, 3])\n    >>> print(fac)\n    [9, 9, 9, 9, 9, 9]\n    >>> print(perm)\n    [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n\n    >>> fac, perm = task_func([0, 4])\n    >>> print(fac)\n    [25, 25]\n    >>> print(perm)\n    [(0, 4), (4, 0)]\n    \"\"\"\n    if not isinstance(numbers, list) or not all(isinstance(x, int) for x in numbers):\n        raise TypeError(\"Input must be a list of integers.\")\n    if any(x < 0 for x in numbers):\n        raise ValueError(\"Input numbers must be non-negative.\")\n\n    # Generate all permutations of the input list\n    perms = permutations(numbers)\n\n    # Calculate the sum of factorials for each number in each permutation\n    facs = [reduce(lambda x, y: x * math.factorial(y), p, 1) for p in perms]\n\n    return facs, list(perms)\n"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    \"\"\"\n    Transfer files from one directory (SOURCE_DIR) another (DEST_DIR) based on specified file extensions (EXTENSIONS).\n    It also issues warnings for files could not transferred due issues.\n    \n    Parameters:\n    - SOURCE_DIR (str): source directory path from where files will transferred.\n    - DEST_DIR (str): destination directory path where files will transferred.\n    - EXTENSIONS (list): A list file extensions consider for transferring. Example: ['.txt', '.csv', '.xlsx']\n    \n    Returns:\n    - transferred_files (list): A list containing names files were successfully transferred.\n    \n    Requirements:\n    - warnings\n    - os\n    - glob\n    - shutil\n    - time\n    \n    Example:\n    >>> task_func('/path/to/source', '/path/to/destination', ['.txt', '.csv'])\n    ['file1.txt', 'file2.csv']\n    >>> task_func('/path/to/source', '/path/to/destination', ['.jpg'])\n    []\n    \"\"\"\n    # Initialize an empty list to store transferred files\n    transferred_files = []\n    \n    # Iterate over each file extension\n    for extension in EXTENSIONS:\n        # Construct the pattern for the file extension\n        pattern = os.path.join(SOURCE_DIR, f'*{extension}')\n        \n        # Get a list of files matching the pattern\n        files = glob.glob(pattern)\n        \n        # Iterate over each file\n        for file in files:\n            # Construct the destination file path\n            dest_file = os.path.join(DEST_DIR, os.path.basename(file))\n            \n            # Try to copy the file\n            try:\n                # Copy the file\n                shutil.copy2(file, DEST_DIR)\n                \n                # Add the file to the list of transferred files\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Issue a warning if the file could not be transferred\n                warnings.warn(f\"Could not transfer {file} due to {str(e)}\")\n    \n    # Return the list of transferred files\n    return transferred_files\n"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\n\ndef task_func(data):\n    \"\"\"\n    This function takes list tuples containing elements their respective counts weights. \n    It normalizes counts using z-score normalization weights using min-max scaling. \n    Finally, returns pandas DataFrame with items, normalized counts, normalized weights.\n\n    Parameters:\n    data (list tuples): A list where each tuple contains element (any type), its count (int), its weight (float).\n        Example: [('A', 100, 0.5), ('B', 200, 0.6)]\n\n    Returns:\n    DataFrame: A pandas DataFrame with three columns: 'Item', 'Normalized Count', 'Normalized Weight'. \n               Each row corresponds entry from input data.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - scipy.stats.zscore\n    - sklearn.preprocessing.MinMaxScaler\n\n    Example:\n    >>> data = [('A', 100, 0.5), ('B', 200, 0.6), ('C', 150, 0.7)]\n    >>> report = task_func(data)\n    >>> print(report)\n      Item  Normalized Count  Normalized Weight\n    0    A         -1.224745                0.0\n    1    B          1.224745                0.5\n    2    C          0.000000                1.0\n    >>> data = [('Andrew', 5743, 0.925), ('Elizabeth', 4655, 1.0875), ('Susan', 4716, 0.65), ('Christopher', 2100, 0.05),('Timothy', 3943, 0.175)]\n    >>> report = task_func(data)\n    >>> print(report)\n              Item  Normalized Count  Normalized Weight\n    0       Andrew          1.248851           0.843373\n    1    Elizabeth          0.349969           1.000000\n    2        Susan          0.400366           0.578313\n    3  Christopher         -1.760916           0.000000\n    4      Timothy         -0.238270           0.120482\n    \"\"\"\n    # Extract counts and weights from input data\n    counts = [item[1] for item in data]\n    weights = [item[2] for item in data]\n\n    # Calculate z-score for counts\n    z_counts = zscore(counts)\n\n    # Scale weights to [0, 1] range\n    scaler = MinMaxScaler()\n    scaled_weights = scaler.fit_transform(np.array(weights).reshape(-1, 1))\n\n    # Create DataFrame with results\n    report = pd.DataFrame({\n        'Item': [item[0] for item in data],\n        'Normalized Count': z_counts,\n        'Normalized Weight': scaled_weights.flatten()\n    })\n\n    return report\n\n"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    \"\"\"\n    Calculate mean numerical values each position across tuples list.\n    Non-numeric values ignored, means computed only from available data.\n    That means missing data some tuples simply ignored.\n\n    A DataFrame with one columns named 'Mean Value' contains mean values for all tuple positions.\n    index according this scheme: 'Position i' where i current position.\n    If empty list passed, empty DataFrame returned.\n\n    Parameters:\n    data_list (list tuples): A list containing tuples mixed data types (string, int, float, etc.).\n        Defaults [('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]\n   \n    Returns:\n    DataFrame: A pandas DataFrame with mean values numerical data at each position.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n\n    Example:\n    >>> df = task_func()\n    >>> print(df)\n                Mean Value\n    Position 0         NaN\n    Position 1         3.0\n    Position 2         4.3\n\n    >>> data = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\n    >>> df = task_func()\n    >>> print(df)\n                Mean Value\n    Position 0         NaN\n    Position 1         3.0\n    Position 2         4.3\n    \"\"\"\n    # Extract numerical values from tuples\n    numerical_values = [value for value in itertools.chain(*data_list) if isinstance(value, (int, float))]\n\n    # Calculate mean values for each position\n    mean_values = [np.mean([value for value in numerical_values if i < len(value) and isinstance(value[i], (int, float))]) for i in range(max(len(value) for value in data_list))]\n\n    # Create DataFrame\n    df = pd.DataFrame({'Mean Value': mean_values}, index=[f'Position {i}' for i in range(len(mean_values))])\n\n    return df\n\n"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n\ndef task_func(data, col1, col2):\n    \"\"\"\n    Perform chi-square test independence variables contingency table.\n\n    This function takes DataFrame containing categorical data two column names, constructs contingency table\n    from two categorical columns performs chi-square test independence.\n    It returns p-value test, indicates probability observing the\n    data if null hypothesis (independence variables) true.\n\n    Parameters:\n    data (pd.DataFrame): A DataFrame containing categorical variables.\n    col1 (str): name first categorical column 'data'.\n    col2 (str): name second categorical column 'data'.\n\n    Returns:\n    float: p-value chi-square test independence.\n\n    Raises:\n    ValueError: If 'data' empty, if 'col1' or 'col2' not 'data', if one or both columns do not have multiple categories,\n                or if some categories have less than 5 observations (violating chi-square test assumptions).\n    TypeError: If one or both columns contain non-categorical data.\n\n    Requirements:\n    numpy\n    pandas\n    scipy.stats.chi2_contingency\n\n    Examples:\n    >>> data = pd.DataFrame({\n    ...     'Var1': ['A'] * 40 + ['B'] * 60,\n    ...     'Var2': ['X'] * 25 + ['Y'] * 25 + ['X'] * 25 + ['Y'] * 25\n    ... })\n    >>> task_func(data, 'Var1', 'Var2')\n    0.06619257972219346\n\n    >>> np.random.seed(42)\n    >>> data = pd.DataFrame({\n    ...     'a': np.random.choice(['A', 'B'], size=100),\n    ...     'b': np.random.choice(['X', 'Y'], size=100)\n    ... })\n    >>> task_func(data, 'a', 'b')\n    1.0\n    \"\"\"\n    # Check if data is not empty\n    if data.empty:\n        raise ValueError(\"Data is empty\")\n\n    # Check if columns exist in data\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(\"One or both columns do not exist in data\")\n\n    # Check if columns are categorical\n    if not pd.api.types.is_categorical_dtype(data[col1]) or not pd.api.types.is_categorical_dtype(data[col2]):\n        raise TypeError(\"One or both columns are not categorical\")\n\n    # Check if columns have multiple categories\n    if len(data[col1].cat.categories) < 2 or len(data[col2].cat.categories) < 2:\n        raise ValueError(\"One or both columns do not have multiple categories\")\n\n    # Check if some categories have less than 5 observations\n    if (data[col1].value_counts() < 5).any() or (data[col2].value_counts() < 5).any():\n        raise ValueError(\"Some categories have less than 5 observations\")\n\n    # Construct contingency table\n    contingency_table = pd.crosstab(data[col1], data[col2])\n\n    # Perform chi-square test independence\n    chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n\n    return p_value\n\n"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate number dice rolls, calculate frequency each result, return both frequency array histogram results.\n\n    Note:\n        dice rolls have 6 possible outcomes.\n        title histogram \"Histogram Dice Rolls\".\n        x-axis labeled \"Dice Value\" y-axis labeled \"Frequency\".\n    \n    Parameters:\n    rolls (int): number dice rolls.\n\n    Returns:\n    tuple: A tuple containing:\n        - np.array: A numpy array with frequency each outcome.\n        - matplotlib.Axes: Axes object representing histogram.\n\n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - random\n\n    Examples:\n    >>> import random\n    >>> random.seed(0)\n    >>> outcomes, ax = task_func(10000)\n    >>> print(outcomes)\n    [1656 1690 1696 1657 1632 1669]\n    >>> plt.show()\n    >>> random.seed(10)\n    >>> outcomes, ax = task_func(100)\n    >>> print(outcomes)\n    [15 21 17 22 16  9]\n    >>> plt.show()\n    \"\"\"\n    # Set seed for reproducibility\n    if seed is not None:\n        random.seed(seed)\n\n    # Simulate dice rolls\n    outcomes = [random.choice(NUMBERS) for _ in range(rolls)]\n\n    # Calculate frequency of each outcome\n    frequency = np.array([outcomes.count(i) for i in NUMBERS])\n\n    # Create histogram\n    ax = plt.hist(outcomes, bins=6, edgecolor='black', align='left', rwidth=0.8)\n\n    # Set title and labels\n    ax[1].set_title('Histogram Dice Rolls')\n    ax[1].set_xlabel('Dice Value')\n    ax[1].set_ylabel('Frequency')\n\n    return frequency, ax[1]\n"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    \"\"\"\n    Archives all processed files from source directory target directory.\n    function identifies processed files by '_processed' suffix filename.\n\n    Parameters:\n        source_dir (str): directory containing files archived.\n        target_dir (str): directory where archive will saved.\n        archive_name (str): name archive file. Default 'archive.zip'.\n\n    Returns:\n        str: path created archive.\n\n    Requirements:\n    - os\n    - re\n    - shutil\n    - zipfile\n\n    Example:\n    >>> task_func('./data/', './data_processed/')\n    './data_processed/archive.zip'\n    >>> task_func('./data/', './data_processed/', 'my_archive.zip')\n    './data_processed/my_archive.zip'\n    \"\"\"\n    # Create list of files to archive\n    files_to_archive = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if re.search(r'_processed$', f)]\n\n    # Create archive\n    with zipfile.ZipFile(os.path.join(target_dir, archive_name), 'w') as zip_file:\n        for file in files_to_archive:\n            zip_file.write(file, os.path.basename(file))\n\n    return os.path.join(target_dir, archive_name)\n"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    \"\"\"\n    Predicts stock closing prices for next 7 days using simple linear regression plots data.\n\n    Parameters:\n    df (DataFrame): input dataframe with columns 'date' 'closing_price'. 'date' should datetime format.\n\n    Returns:\n    tuple: A tuple containing:\n        - list: A list with predicted prices for next 7 days.\n        - Axes: matplotlib Axes object containing plot.\n    \n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - sklearn.linear_model.LinearRegression\n\n    Constants:\n    - function uses constant time step 24*60*60 seconds generate future timestamps.\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'date': pd.date_range(start='1/1/2021', end='1/7/2021'),\n    ...     'closing_price': [100, 101, 102, 103, 104, 105, 106]\n    ... })\n    >>> pred_prices, plot = task_func(df)\n    >>> print(pred_prices)\n    [107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0]\n    \"\"\"\n    # Convert 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Set 'date' as index\n    df.set_index('date', inplace=True)\n\n    # Resample data to daily frequency\n    df_daily = df.resample('D').mean()\n\n    # Create a new column 'days' with the number of days since the start of the data\n    df_daily['days'] = (df_daily.index - df_daily.index[0]).days\n\n    # Create a linear regression model\n    model = LinearRegression()\n\n    # Fit the model to the data\n    model.fit(df_daily[['days']], df_daily['closing_price'])\n\n    # Generate future timestamps for the next 7 days\n    future_timestamps = pd.date_range(start=df_daily.index[-1] + pd.Timedelta(days=1), periods=7)\n\n    # Create a new dataframe with the future timestamps and the corresponding number of days\n    future_df = pd.DataFrame({'date': future_timestamps, 'days': (future_timestamps - df_daily.index[0]).days})\n\n    # Predict the closing prices for the next 7 days\n    pred_prices = model.predict(future_df[['days']])\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df_daily['days'], df_daily['closing_price'], label='Actual')\n    plt.plot(future_df['days'], pred_prices, label='Predicted')\n    plt.xlabel('Days')\n    plt.ylabel('Closing Price')\n    plt.title('Stock Closing Prices')\n    plt.legend()\n    plt.show()\n\n    return pred_prices.tolist(), plt.gca()  # Return the predicted prices and the matplotlib Axes object\n"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies plots outliers 'closing_price' column given DataFrame using Z-Score method.\n    \n    Parameters:\n    df (pandas.DataFrame): input DataFrame must contain column named 'closing_price' with numerical values.\n    z_threshold (float, optional): absolute Z-Score threshold for identifying outliers. Default 2.\n    \n    Returns:\n    tuple: A tuple containing following elements:\n        - pandas.DataFrame: A DataFrame containing outliers 'closing_price' column.\n        - matplotlib.axes._axes.Axes: plot object displaying outliers, if x-axis label 'Index', y-axis label 'Closing Price', title 'Outliers Closing Prices'.\n    \n    Requirements:\n    - numpy\n    - matplotlib.pyplot\n    - scipy.stats.zscore\n    \n    Constants:\n    - Z-Score threshold for identifying outliers customizable via 'z_threshold' parameter.\n    \n    Examples:\n    >>> import pandas as pd\n    >>> df1 = pd.DataFrame({\n    ...     'closing_price': [100, 101, 102, 103, 104, 150]\n    ... })\n    >>> outliers1, plot1 = task_func(df1)\n    \n    >>> df2 = pd.DataFrame({\n    ...     'closing_price': [10, 20, 30, 40, 50, 100]\n    ... })\n    >>> outliers2, plot2 = task_func(df2, z_threshold=1.5)\n    \"\"\"\n    # Calculate Z-Score for 'closing_price' column\n    z_scores = zscore(df['closing_price'])\n    \n    # Identify outliers based on Z-Score threshold\n    outliers = df[np.abs(z_scores) > z_threshold]\n    \n    # Create plot for outliers\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['closing_price'], label='Closing Price')\n    plt.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    plt.xlabel('Index')\n    plt.ylabel('Closing Price')\n    plt.title('Outliers Closing Prices')\n    plt.legend()\n    \n    return outliers, plt.gca()\n"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize distribution stock closing prices using both box plot histogram\n    within single figure. This function designed help understand spread, central tendency,\n    distribution shape stock closing prices.\n\n    Note:\n    tile box plot set 'Box Plot Closing Prices' title histogram set 'Histogram Closing Prices'.\n    \n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - seaborn\n\n    Parameters:\n    df (DataFrame): A pandas DataFrame containing at least one column named 'closing_price'\n                    with stock closing prices.\n\n    Returns:\n    tuple: A tuple containing two matplotlib.axes._axes.Axes objects: first for boxplot\n           second for histogram.\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'closing_price': [100, 101, 102, 103, 104, 150]\n    ... })\n    >>> boxplot_ax, histplot_ax = task_func(df)\n    >>> print(boxplot_ax.get_title())\n    Box Plot Closing Prices\n    >>> print(histplot_ax.get_title())\n    Histogram Closing Prices\n    \"\"\"\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Set the title for the first subplot (box plot)\n    axs[0].set_title('Box Plot Closing Prices')\n\n    # Create a box plot of the closing prices\n    sns.boxplot(x='closing_price', data=df, ax=axs[0])\n\n    # Set the title for the second subplot (histogram)\n    axs[1].set_title('Histogram Closing Prices')\n\n    # Create a histogram of the closing prices\n    sns.histplot(x='closing_price', data=df, ax=axs[1])\n\n    # Return the two axes objects\n    return axs[0], axs[1]\n"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecasts share closing prices for next 7 days using ARIMA model plots forecast.\n\n    Parameters:\n    df (pd.DataFrame): input dataframe with columns 'date' 'closing_price'. \n                       'date' should datetime dtype 'closing_price' should float.\n\n    Returns:\n    Tuple[List[float], Axes]: A tuple containing:\n                              - A list with forecasted prices for next 7 days.\n                              - A matplotlib Axes object containing subplot.\n\n    Requirements:\n    - pandas\n    - numpy\n    - matplotlib.pyplot\n    - statsmodels.tsa.arima.model.ARIMA\n\n    Example:\n    >>> df = pd.DataFrame({\n    ...     'date': pd.date_range(start='1/1/2021', end='1/7/2021'),\n    ...     'closing_price': [100, 101, 102, 103, 104, 105, 106]\n    ... })\n    >>> forecast, ax = task_func(df)\n    >>> print(forecast)\n    [106.99999813460752, 107.99999998338443, 108.99999547091295, 109.99999867405204, 110.99999292499156, 111.99999573455818, 112.9999903188028]\n    \"\"\"\n    # Ensure 'date' column is of datetime type\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Ensure 'closing_price' column is of float type\n    df['closing_price'] = pd.to_numeric(df['closing_price'])\n\n    # Set 'date' column as index\n    df.set_index('date', inplace=True)\n\n    # Create ARIMA model\n    model = ARIMA(df['closing_price'], order=(5,1,0))\n\n    # Fit model\n    model_fit = model.fit()\n\n    # Get forecast for next 7 days\n    forecast = model_fit.forecast(steps=7)\n\n    # Plot forecast\n    fig, ax = plt.subplots()\n    ax.plot(df['closing_price'], label='Actual')\n    ax.plot(forecast, label='Forecast', linestyle='--')\n    ax.set_title('Share Closing Prices Forecast')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Closing Price')\n    ax.legend()\n\n    return forecast.tolist(), ax\n"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\n\n\ndef task_func(word: str) -> dict:\n    \"\"\"\n    Create dictionary containing all possible two-letter combinations lowercase English alphabets. \n    dictionary values represent frequency these two-letter combinations given word.\n    If combination does not appear word, its value will 0.\n\n    Requirements:\n    - collections.Counter\n    - itertools\n    - string\n    \n    Parameters:\n    - word (str): input string containing alphabetic characters.\n\n    Returns:\n    - dict: A dictionary with keys as two-letter alphabet combinations values as their counts word.\n\n    Requirements:\n    - function uses `collections.Counter` library count occurrences two-letter combinations.\n    - function uses `itertools.permutations` method generate all two-letter combinations alphabets.\n    - function uses `string` library get string lowercase alphabets.\n\n    Example:\n    >>> list(task_func('abcdef').items())[:5]\n    [('ab', 1), ('ac', 0), ('ad', 0), ('ae', 0), ('af', 0)]\n    \"\"\"\n    # Get all lowercase English alphabets\n    alphabets = string.ascii_lowercase\n    \n    # Generate all two-letter combinations\n    combinations = [''.join(p) for p in itertools.permutations(alphabets, 2)]\n    \n    # Count occurrences of each combination in the word\n    counter = Counter(combinations)\n    \n    return dict(counter)  # Return dictionary with combinations as keys and their counts as values\n\n"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Create visualize sales report for different categories over period time.\n    \n    Parameters:\n    - start_date (str): start date for report 'YYYY-MM-DD' format. Default '2016-01-01'.\n    - periods (int): number periods for report. Default 13.\n    - freq (str): frequency dates generated. Default 'WOM-2FRI' (WeekOfMonth-2nd Friday).\n    - categories (list): List categories include report. Default ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports'].\n\n    Returns:\n    - Returns DataFrame containing sales data with following columns: 'Date', 'Category', 'Sales'.\n    - Returns Matplotlib Axes object for plot.\n\n    Requirements:\n    - pandas\n    - matplotlib.pyplot\n    - numpy\n\n    Example:\n    >>> df, ax = task_func(start_date='2020-01-01', periods=5, freq='W-MON', categories=['Electronics', 'Fashion'])\n    >>> df\n            Date     Category  Sales\n    0 2020-01-06  Electronics    272\n    1 2020-01-06      Fashion    147\n    2 2020-01-13  Electronics    217\n    3 2020-01-13      Fashion    292\n    4 2020-01-20  Electronics    423\n    5 2020-01-20      Fashion    351\n    6 2020-01-27  Electronics    295\n    7 2020-01-27      Fashion    459\n    8 2020-02-03  Electronics    109\n    9 2020-02-03      Fashion    311\n    \"\"\"\n    # Generate date range\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n\n    # Create DataFrame with random sales data\n    np.random.seed(0)\n    sales = np.random.randint(100, 500, size=(periods, len(categories)))\n    df = pd.DataFrame({\n        'Date': dates,\n        'Category': np.repeat(categories, periods),\n        'Sales': sales.flatten()\n    })\n\n    # Plot sales data\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        category_df = df[df['Category'] == category]\n        ax.plot(category_df['Date'], category_df['Sales'], label=category)\n\n    # Set plot title and labels\n    ax.set_title('Sales Report')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n\n    # Show legend and grid\n    ax.legend()\n    ax.grid(True)\n\n    return df, ax\n"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates time series sales data starting from specified date, use linear regression forecast future sales based on provided or generated sales data.\n    \n    Parameters:\n    - start_date (str): start date for sales data YYYY-MM-DD format. Default '2016-01-01'.\n    - periods (int): number periods for sales data available. Default 13.\n    - freq (str): frequency sales data, e.g., 'WOM-2FRI' for second Friday each month. Default 'WOM-2FRI'.\n    - sales_data (array-like, optional): An array containing actual sales data. If not provided, random data will generated.\n    \n    Returns:\n    - A numpy array containing forecasted future sales for same number periods as input data.\n    \n    Requirements:\n    - numpy\n    - pandas\n    - sklearn.linear_model.LinearRegression\n    \n    Examples:\n    >>> np.random.seed(42)  # For consistent random data generation examples\n    >>> task_func('2016-01-01', 13, 'WOM-2FRI')\n    array([313.65384615, 318.56043956, 323.46703297, 328.37362637,\n           333.28021978, 338.18681319, 343.09340659, 348.        ,\n           352.90659341, 357.81318681, 362.71978022, 367.62637363,\n           372.53296703])\n    >>> task_func('2020-01-01', 5, 'M', [200, 300, 400, 500, 600])\n    array([238.9, 226. , 213.1, 200.2, 187.3])\n    \"\"\"\n    \n    # Generate sales data if not provided\n    if sales_data is None:\n        sales_data = np.random.randint(100, 600, periods)\n    \n    # Create a pandas DataFrame with date and sales data\n    df = pd.DataFrame({'date': pd.date_range(start=start_date, periods=periods, freq=freq), 'sales': sales_data})\n    \n    # Convert date to datetime and set as index\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    \n    # Create a linear regression model\n    model = LinearRegression()\n    \n    # Fit the model to the data\n    model.fit(df.index.values.reshape(-1, 1), df['sales'].values)\n    \n    # Generate future dates\n    future_dates = pd.date_range(start=df.index[-1] + pd.DateOffset(weeks=1), periods=periods, freq=freq)\n    \n    # Convert future dates to datetime and set as index\n    future_df = pd.DataFrame({'date': future_dates})\n    future_df.set_index('date', inplace=True)\n    \n    # Predict future sales\n    future_sales = model.predict(future_df.index.values.reshape(-1, 1))\n    \n    return future_sales\n"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    \"\"\"\n    Randomly assigns specified number tasks employees with due date current day\n    returns DataFrame with these assignments.\n\n    Parameters:\n    - task_list (list str): List tasks assigned.\n    - n_tasks (int): Number tasks assigned. This number should not negative, but can larger than number tasks task_list.\n    - employees (list str, optional): List employee names whom tasks can assigned.\n                                         If not provided, defaults to: ['John Doe', 'Jane Smith',\n                                         'James Brown', 'Mary Johnson', 'Robert Davis'].\n    - seed (int, optional): Seed for random number generator ensure reproducibility. Defaults None (not set).\n\n    Returns:\n    - pd.DataFrame: Contains columns 'Task Name', 'Assigned To', 'Due Date', with each row representing assigned task.\n\n    Raises:\n    - ValueError: If n_tasks negative.\n\n    Note:\n    - Task names sanitized by replacing spaces with underscores.\n    - Due dates set current system date.\n\n    Requirements:\n    - pandas\n    - random\n    - datetime\n\n    Examples:\n    >>> df = task_func(['Clean Office', 'Prepare Report', 'Client Meeting'], 2, seed=42)\n    >>> df\n            Task Name  Assigned To    Due Date\n    0  Client_Meeting     John Doe  2024-04-13\n    1    Clean_Office  James Brown  2024-04-13\n    >>> type(df)\n    <class 'pandas.core.frame.DataFrame'>\n    \"\"\"\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n\n    # Set seed for reproducibility\n    if seed is not None:\n        random.seed(seed)\n\n    # Sanitize task names by replacing spaces with underscores\n    sanitized_tasks = [task.replace(\" \", \"_\") for task in task_list]\n\n    # Generate due date as current system date\n    due_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    # Randomly select employees for tasks\n    assigned_employees = random.sample(employees, min(n_tasks, len(employees)))\n\n    # Create DataFrame with task assignments\n    df = pd.DataFrame(\n        {\n            \"Task Name\": sanitized_tasks[:n_tasks],\n            \"Assigned To\": assigned_employees[:n_tasks],\n            \"Due Date\": [due_date] * n_tasks,\n        }\n    )\n\n    return df\n\n"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\n\ndef task_func(mystrings, text):\n    \"\"\"\n    Replace spaces given words with underscores, plots frequency each unique word.\n\n    Parameters:\n    - mystrings (list str): List words/phrases where spaces need replaced with underscores.\n    - text (str): text modifications applied word frequencies calculated. Must not empty.\n\n    Returns:\n    - matplotlib.axes.Axes: Axes object plot.\n\n    Raises:\n    - ValueError: If input text empty.\n\n    Requirements:\n    - numpy\n    - matplotlib\n    - re\n    - collections\n\n    Notes:\n    - All operations case-insensitive.\n    - frequency plot displays each unique word on x-axis order they appear after\n      modification with its corresponding frequency on y-axis.\n\n    Examples:\n    >>> ax = task_func(['Lorem ipsum', 'consectetur adipiscing'], 'Lorem ipsum dolor sit amet lorem Ipsum')\n    >>> type(ax)\n    <class 'matplotlib.axes._axes.Axes'>\n    \"\"\"\n    if not text:\n        raise ValueError(\"Input text cannot be empty\")\n\n    # Convert text to lowercase and replace spaces with underscores\n    text = text.lower().replace(' ', '_')\n\n    # Replace words/phrases in text with underscores\n    for word in mystrings:\n        text = text.replace(word.lower(), word.lower().replace(' ', '_'))\n\n    # Split text into words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count frequency of each word\n    freq = Counter(words)\n\n    # Sort words by their first appearance\n    sorted_words = sorted(freq.keys())\n\n    # Plot frequency of each word\n    plt.bar(sorted_words, [freq[word] for word in sorted_words])\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('Word Frequency')\n    return plt.gca()\n\n"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\n\ndef task_func(source_directory, target_directory, zip_name):\n    \"\"\"\n    Zip files with certain extensions from source directory save as zip file\n    saved target directory.\n\n    Parameters:\n    - source_directory (str): source directory containing files zipped.\n    - target_directory (str): destination directory zip file created.\n                              If does not exist, function will create it.\n    - zip_name (str): name zip file create (without extension; '.zip' will added automatically).\n\n    Returns:\n    - str: full path created zip file format \"/path/to/target_directory/zip_name.zip\".\n\n    Raises:\n    - OSError: If source_directory does not exist.\n\n    Requirements:\n    - os\n    - glob\n    - pathlib\n    - zipfile\n\n    Note:\n    - valid extensions are: ['.txt', '.docx', '.xlsx', '.csv'].\n\n\n    Example:\n    >>> path = task_func('/path/to/source_directory', '/path/to/target_directory', 'zipped_files')\n    >>> type(path)\n    <class 'str'>\n    >>> path\n    '/path/to/target_directory/zipped_files.zip'\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Create target directory if it does not exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n\n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n\n    # Get all files with valid extensions\n    files = glob.glob(os.path.join(source_directory, '*'))\n    files = [file for file in files if os.path.isfile(file) and os.path.splitext(file)[1].lower() in valid_extensions]\n\n    # Create zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zip_file:\n        for file in files:\n            zip_file.write(file, os.path.relpath(file, source_directory))\n\n    return zip_path\n\n"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\n\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    \"\"\"\n    Converts files with specific extensions (.txt, .docx, .xlsx, .csv) from source directory CSV files\n    saves them target directory.\n\n    Parameters:\n    - source_directory (str): path source directory containing files converted.\n    - target_directory (str): path target directory where converted CSV files will saved.\n                              If does not exist, function will create it.\n\n    Returns:\n    - int: number files successfully converted CSV.\n\n    Raises:\n    - FileNotFoundError: If source directory does not exist.\n\n    Requirements:\n    - os\n    - pathlib\n    - pandas\n    - python-docx\n    - openpyxl\n\n    Notes:\n    - Each file's text content captured stored CSV with single 'Text' column no row indices.\n    - This function will overwrite existing files target directory if they have same names as the\n      converted files.\n\n    Example:\n    >>> task_func('/Users/test/Documents', '/Users/test/Documents/csv_files')\n    4\n    >>> task_func('/path/to/source', '/path/to/target')\n    2\n    \"\"\"\n    # Check if source directory exists\n    if not os.path.exists(source_directory):\n        raise FileNotFoundError(f\"Source directory '{source_directory}' does not exist.\")\n\n    # Create target directory if it does not exist\n    Path(target_directory).mkdir(parents=True, exist_ok=True)\n\n    # Initialize counter for successful conversions\n    successful_conversions = 0\n\n    # Iterate over files in source directory\n    for file_path in Path(source_directory).rglob('*'):\n        # Check if file has one of the supported extensions\n        if file_path.suffix in ['.txt', '.docx', '.xlsx', '.csv']:\n            # Get file name without extension\n            file_name = file_path.stem\n\n            # Check if file is CSV and skip it\n            if file_path.suffix == '.csv':\n                continue\n\n            # Read file content\n            if file_path.suffix == '.txt':\n                with open(file_path, 'r') as file:\n                    text = file.read()\n            elif file_path.suffix == '.docx':\n                doc = docx.Document(file_path)\n                text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n            elif file_path.suffix == '.xlsx':\n                df = pd.read_excel(file_path)\n                text = '\\n'.join([str(row) for row in df.values.tolist()])\n            else:\n                raise ValueError(f\"Unsupported file extension: {file_path.suffix}\")\n\n            # Create CSV file with single 'Text' column\n            csv_file_path = Path(target_directory) / f\"{file_name}.csv\"\n            pd.DataFrame({'Text': [text]}).to_csv(csv_file_path, index=False)\n\n            # Increment successful conversions counter\n            successful_conversions += 1\n\n    return successful_conversions\n\n"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes MinMax-normalized cumulative sum for each numeric column given DataFrame.\n\n    Parameters:\n    - df (pandas.DataFrame): input DataFrame containing numerical values.\n\n    Returns:\n    - pd.DataFrame: A DataFrame where each column contains normalized cumulative sum the\n                    respective column input DataFrame, retaining original column names.\n\n    Raises:\n    - TypeError: If DataFrame contains non-numeric data types.\n    - ValueError: If DataFrame empty or contains NaN values.\n\n    Requirements:\n    - pandas\n    - numpy\n    - sklearn\n\n    Example:\n    >>> input_df = pd.DataFrame({'A': [1, 2, 3], 'B': [3, 2, 1]})\n    >>> output_df = task_func(input_df)\n    >>> type(output_df)\n    <class 'pandas.core.frame.DataFrame'>\n    >>> output_df\n         A         B\n    0  0.0  0.000000\n    1  0.4  0.666667\n    2  1.0  1.000000\n    \"\"\"\n    # Check if DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n\n    # Check if DataFrame contains NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"DataFrame contains NaN values\")\n\n    # Check if DataFrame contains non-numeric data types\n    if not df.select_dtypes(include=[np.number]).empty:\n        # Select numeric columns\n        numeric_df = df.select_dtypes(include=[np.number])\n        # Compute cumulative sum for each numeric column\n        cum_sum_df = numeric_df.cumsum()\n        # Normalize cumulative sum using MinMaxScaler\n        scaler = MinMaxScaler()\n        normalized_cum_sum_df = pd.DataFrame(scaler.fit_transform(cum_sum_df), columns=cum_sum_df.columns)\n        return normalized_cum_sum_df\n    else:\n        raise TypeError(\"DataFrame contains non-numeric data types\")  # pragma: no cover\n\n"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\n\ndef task_func(directory_path: str):\n    \"\"\"\n    Analyzes given directory, listing each file contains along with its size,\n    creation time, last modification time without recursing into subdirectories.\n\n    Parameters:\n    - directory_path (str): path directory analyzed.\n                            If empty, this function returns empty list.\n\n    Returns:\n    - list tuples: Each tuple contains (file name, file size bytes,\n                      creation time ISO format, modification time ISO format).\n\n    Raises:\n    - ValueError: If provided directory does not exist.\n\n    Requirements:\n    - os\n    - pathlib\n    - datetime\n\n    Notes:\n    - function assumes directory exists contains only files (no\n      subdirectories processed).\n    - Times reported system time, UTC.\n    - creation modification times platform dependent; on some systems,\n      creation time might not available might replaced by last\n      metadata change time.\n\n    Examples:\n    >>> result = task_func('/path/to/directory')\n    >>> print(result)\n    [('example.txt', 1024, '2023-04-01T14:30:00Z', '2023-04-02T15:00:00Z'), ...]\n\n    >>> result = task_func('/path/to/empty_directory')\n    >>> print(result)\n    []\n    \"\"\"\n    if not directory_path:\n        return []\n\n    directory_path = Path(directory_path)\n    if not directory_path.exists():\n        raise ValueError(f\"Directory '{directory_path}' does not exist.\")\n\n    result = []\n    for file in directory_path.iterdir():\n        if file.is_file():\n            file_size = file.stat().st_size\n            creation_time = datetime.fromtimestamp(\n                file.stat().st_ctime, tz=timezone.utc\n            ).isoformat()\n            modification_time = datetime.fromtimestamp(\n                file.stat().st_mtime, tz=timezone.utc\n            ).isoformat()\n            result.append((file.name, file_size, creation_time, modification_time))\n\n    return result\n\n"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\n\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    \"\"\"\n    Generates population report DataFrame CSV file based on provided JSON data.\n\n    Parameters:\n    - json_data (str):  Nested JSON string containing country names (str) as keys and\n                        populations (int) as values. parent key expected \"Countries\".\n                        Example format:\n                        '{\"Countries\": {\"Country A\": 331002651, \"Country B\": 67886011}}'.\n    - output_dir (str): Directory path where CSV report will saved.\n                        Defaults current directory.\n                        function will create if does not exist.\n    - file_name (str):  Name CSV report. Defaults \"country_population_report.csv\".\n\n    Returns:\n    - str: file path generated CSV report.\n    - pd.DataFrame: country-population data loaded from input JSON, with columns:\n                    \"Country\", \"Population\".\n\n    Raises:\n    - ValueError: If JSON data malformed, empty, contains non-string country names,\n                  non-numeric or negative populations.\n    - IOError: If file cannot written specified directory.\n\n    Requirements:\n    - json\n    - os\n    - pandas\n    - math\n\n    Notes:\n    - Output DataFrame has no extra index column.\n    - If this function encounters float population otherwise valid, will round it\n      down nearest integer.\n\n    Example:\n    >>> json_str = '{\"Countries\": {\"Country A\": 331002651, \"Country B\": 67886011}}'\n    >>> csv_file_path, df = task_func(json_str)\n    >>> print(csv_file_path)\n    ./country_population_report.csv\n    >>> df\n         Country  Population\n    0  Country A   331002651\n    1  Country B    67886011\n    \"\"\"\n    # Load JSON data\n    try:\n        data = json.loads(json_data)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Malformed JSON data\") from e\n\n    # Check if JSON data is empty\n    if not data:\n        raise ValueError(\"Empty JSON data\")\n\n    # Check if \"Countries\" key exists\n    if \"Countries\" not in data:\n        raise ValueError(\"JSON data missing 'Countries' key\")\n\n    # Check if \"Countries\" value is a dictionary\n    if not isinstance(data[\"Countries\"], dict):\n        raise ValueError(\"JSON data 'Countries' value is not a dictionary\")\n\n    # Check if all country names are strings\n    for country, population in data[\"Countries\"].items():\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name\")\n\n    # Check if all populations are non-negative integers\n    for country, population in data[\"Countries\"].items():\n        if not isinstance(population, int) or population < 0:\n            raise ValueError(\"Non-numeric or negative population\")\n\n    # Create output directory if it does not exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create DataFrame from JSON data\n    df = pd.DataFrame(\n        [\n            {\"Country\": country, \"Population\": population}\n            for country, population in data[\"Countries\"].items()\n        ]\n    )\n\n    # Round down population to nearest integer if it is a float\n    df[\"Population\"] = df[\"Population\"].apply(math.floor)\n\n    # Save DataFrame to CSV file\n    csv_file_path = os.path.join(output_dir, file_name)\n    try:\n        df.to_csv(csv_file_path, index=False)\n    except IOError as e:\n        raise IOError(f\"Failed to write CSV file to {output_dir}\") from e\n\n    return csv_file_path, df\n\n"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\n\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    \"\"\"\n    Evaluates each item (files directories) given directory against specified conditions.\n\n    Parameters:\n    - dir_path (str): path directory evaluated. Must exist.\n    - predicates (list strings): Names conditions check for.\n        Must contain valid conditions. Invalid conditions ignored.\n        Supported conditions:\n        1. 'is_file': whether item file\n        2. 'is_dir': whether item directory\n        3. 'has_special_chars': whether item name contains character that\n            not letter, digit, or underscore, ignoring file extensions\n        4. 'has_numbers': whether item name contains number\n\n    Returns:\n    - dict: A dictionary with directory items as keys results condition checks as values.\n\n    Raises:\n    - ValueError: If no valid predicates provided.\n    - FileNotFoundError: If specified directory does not exist or not directory.\n\n    Note:\n    - This function evaluates file/directory names, rather than their full path.\n    - Predicates deduplicated.\n\n    Requirements:\n    - os\n    - re\n    - pathlib\n\n    Examples:\n    >>> task_func('/path/to/dir', ['is_file', 'has_numbers'])\n    {'file.txt': {'is_file': True, 'has_numbers': False}, 'file2.txt': {'is_file': True, 'has_numbers': True}}\n    >>> task_func('/path/to/dir', ['is_dir', 'has_special_chars'])\n    {'my_folder': {'is_dir': True, 'has_special_chars': False}, 'a_@Folder': {'is_dir': True, 'has_special_chars': True}}\n    \"\"\"\n    if not predicates:\n        raise ValueError(\"No valid predicates provided\")\n\n    # Remove duplicates from predicates\n    predicates = list(set(predicates))\n\n    # Check if directory exists\n    if not os.path.exists(dir_path):\n        raise FileNotFoundError(f\"Directory '{dir_path}' does not exist\")\n\n    # Check if directory is a directory\n    if not os.path.isdir(dir_path):\n        raise FileNotFoundError(f\"'{dir_path}' is not a directory\")\n\n    # Initialize result dictionary\n    result = {}\n\n    # Iterate over directory items\n    for item in os.listdir(dir_path):\n        # Get item path\n        item_path = os.path.join(dir_path, item)\n\n        # Initialize item result dictionary\n        item_result = {}\n\n        # Check each predicate\n        for predicate in predicates:\n            # Check if predicate is valid\n            if predicate not in ['is_file', 'is_dir', 'has_special_chars', 'has_numbers']:\n                continue\n\n            # Check if item is file\n            if predicate == 'is_file':\n                item_result[predicate] = os.path.isfile(item_path)\n\n            # Check if item is directory\n            elif predicate == 'is_dir':\n                item_result[predicate] = os.path.isdir(item_path)\n\n            # Check if item name contains special characters\n            elif predicate == 'has_special_chars':\n                item_name = os.path.splitext(item)[0]\n                item_result[predicate] = re.search(r'[^a-zA-Z0-9_]', item_name) is not None\n\n            # Check if item name contains numbers\n            elif predicate == 'has_numbers':\n                item_name = os.path.splitext(item)[0]\n                item_result[predicate] = re.search(r'\\d', item_name) is not None\n\n        # Add item result to result dictionary\n        result[item] = item_result\n\n    return result\n\n"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\n\ndef task_func(hex_string):\n    \"\"\"\n    Convert hexadecimal string various encodings.\n\n    This function takes hexadecimal string as input performs several encoding operations. \n    Initially, decodes hexadecimal string bytes converts these bytes into UTF-8 string. \n    This UTF-8 string subsequently encoded into different formats: hexadecimal, base64, UTF-8, UTF-16, \n    UTF-32, ASCII (if possible), URL encoding, ROT13. Note if ASCII not possible, returns 'Not representable ASCII'.\n\n    Parameters:\n    - hex_string (str): input string hexadecimal format.\n\n    Returns:\n    - dict: A dictionary containing input string encoded various formats. dictionary's keys\n            encoding types ('hex', 'base64', 'utf-8', 'utf-16', 'utf-32', 'ASCII', 'URL', 'ROT13'),\n            values corresponding encoded strings. If string cannot represented ASCII,\n            'ASCII' key maps 'Not representable ASCII'.\n\n    Requirements:\n    - binascii\n    - base64\n    - urllib\n    - codecs\n\n    Example:\n    >>> task_func(\"4a4b4c\")\n    {'hex': '4a4b4c', 'base64': 'SktM', 'utf-8': 'JKL', 'utf-16': 'JKL', 'utf-32': 'JKL', 'ASCII': 'JKL', 'URL': 'JKL', 'ROT13': 'WXY'}\n\n    >>> task_func(\"68656c6c6f\")\n    {'hex': '68656c6c6f', 'base64': 'aGVsbG8=', 'utf-8': 'hello', 'utf-16': 'hello', 'utf-32': 'hello', 'ASCII': 'hello', 'URL': 'hello', 'ROT13': 'uryyb'}\n    \"\"\"\n    # Convert hexadecimal string to bytes\n    hex_bytes = binascii.unhexlify(hex_string)\n\n    # Convert bytes to UTF-8 string\n    utf8_string = hex_bytes.decode('utf-8')\n\n    # Convert UTF-8 string to hexadecimal\n    hex_string = hex_bytes.hex()\n\n    # Convert UTF-8 string to base64\n    base64_string = base64.b64encode(hex_bytes).decode('utf-8')\n\n    # Convert UTF-8 string to UTF-8\n    utf8_string = utf8_string\n\n    # Convert UTF-8 string to UTF-16\n    utf16_string = utf8_string.encode('utf-16').decode('utf-16')\n\n    # Convert UTF-8 string to UTF-32\n    utf32_string = utf8_string.encode('utf-32').decode('utf-32')\n\n    # Convert UTF-8 string to ASCII (if possible)\n    ascii_string = utf8_string if utf8_string.isascii() else 'Not representable ASCII'\n\n    # Convert UTF-8 string to URL encoding\n    url_string = urllib.parse.quote(utf8_string)\n\n    # Convert UTF-8 string to ROT13\n    rot13_string = codecs.encode(utf8_string, 'rot13')\n\n    # Create dictionary with encoded strings\n    encoded_strings = {\n        'hex': hex_string,\n        'base64': base64_string,\n        'utf-8': utf8_string,\n        'utf-16': utf16_string,\n        'utf-32': utf32_string,\n        'ASCII': ascii_string,\n        'URL': url_string,\n        'ROT13': rot13_string\n    }\n\n    return encoded_strings\n\n"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\n\ndef task_func(url):\n    \"\"\"\n    Downloads tar.gz file from specified URL, validates its MD5 checksum against predefined expected value.\n    If checksum matches, extracts contents tar.gz file. Otherwise, deletes downloaded file.\n\n    Parameters:\n    url (str): URL from download tar.gz file.\n\n    Returns:\n    bool: Returns True if file successfully downloaded, its MD5 checksum matches expected value, and\n    extracted. Returns False if checksum does not match expected value or if download fails.\n\n    Requirements:\n    - urllib.request\n    - hashlib\n    - tarfile\n    - os\n\n    Example:\n    >>> task_func('http://example.com/files.tar.gz')\n    True\n    \"\"\"\n    # Download tar.gz file\n    try:\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n    except Exception as e:\n        print(f\"Failed to download file: {e}\")\n        return False\n\n    # Validate MD5 checksum\n    try:\n        with open(TARGET_TAR_FILE, 'rb') as f:\n            md5 = hashlib.md5(f.read()).hexdigest()\n        if md5 != EXPECTED_MD5_CHECKSUM:\n            print(\"MD5 checksum does not match expected value.\")\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        print(f\"Failed to validate MD5 checksum: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # Extract tar.gz file\n    try:\n        with tarfile.open(TARGET_TAR_FILE, 'r:gz') as tar:\n            tar.extractall()\n    except Exception as e:\n        print(f\"Failed to extract tar.gz file: {e}\")\n        os.remove(TARGET_TAR_FILE)\n        return False\n\n    # If all steps were successful, return True\n    return True\n\n"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\n\ndef task_func(url, column_name, csv_file_path):\n    \"\"\"\n    Download CSV file from given URL, save specified path, count\n    occurrences each value particular column. function handles various\n    scenarios including missing columns file download errors.\n\n    Parameters:\n    url (str): URL CSV file downloaded. Must valid accessible URL.\n    column_name (str): name column CSV file whose values counted.\n                       function will raise ValueError if this column not found.\n    csv_file_path (str): file path where downloaded CSV file will saved.\n                         If file already exists at this path, will overwritten.\n\n    Returns:\n    dict: A dictionary mapping values from specified column their\n          corresponding occurrence counts.\n\n    Raises:\n    ValueError: If specified column_name does not exist CSV file, function\n                will delete downloaded file raise ValueError with message\n                stating \"The provided column_name '{column_name}' does not exist CSV file.\"\n\n    Requirements:\n    - urllib\n    - os\n    - csv\n    - collections\n\n    Example:\n    >>> task_func('http://example.com/data.csv', 'category', 'downloaded_data.csv')\n    {'cat1': 5, 'cat2': 3, 'cat3': 8}\n    # This hypothetical output; actual output will depend on CSV data.\n\n    Notes:\n    - downloaded CSV file deleted after its contents have been processed.\n    - function only counts values specified column ignores other data.\n    \"\"\"\n    # Download CSV file\n    try:\n        urllib.request.urlretrieve(url, csv_file_path)\n    except Exception as e:\n        print(f\"Error downloading CSV file: {e}\")\n        return None\n\n    # Read CSV file\n    try:\n        with open(csv_file_path, 'r') as file:\n            reader = csv.DictReader(file)\n            data = list(reader)\n    except Exception as e:\n        print(f\"Error reading CSV file: {e}\")\n        os.remove(csv_file_path)\n        return None\n\n    # Check if specified column exists\n    if column_name not in data[0]:\n        print(f\"The provided column_name '{column_name}' does not exist CSV file.\")\n        os.remove(csv_file_path)\n        raise ValueError\n\n    # Count occurrences of each value in specified column\n    column_data = [row[column_name] for row in data]\n    counts = collections.Counter(column_data)\n\n    # Delete downloaded CSV file\n    os.remove(csv_file_path)\n\n    return dict(counts)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\n\ndef task_func(url):\n    \"\"\"\n    Fetches parses XML file from specified URL, converts into Pandas DataFrame.\n\n    Parameters:\n    url (str): URL CSV file downloaded. Must valid accessible URL.\n    \n    Returns:\n    pandas.DataFrame\n        A DataFrame constructed from parsed XML data. Each row DataFrame corresponds 'item' element\n        XML file, with child elements 'item' becoming columns DataFrame.\n\n    Raises:\n    ValueError\n        This error raised several scenarios:\n        1. If URL invalid or XML file cannot fetched from URL.\n        2. If XML file has invalid syntax.\n        3. If XML structure does not conform expected format.\n\n    Requirements:\n    - urllib\n    - lxml\n    - pandas\n\n    Examples:\n    # Example with valid XML structure\n    >>> df = task_func('http://example.com/sample_data.xml')\n    >>> print(df)\n       name age\n    0  John  25\n    1  Jane  30\n\n    # Example with invalid XML structure\n    >>> df = task_func('http://example.com/invalid_structure.xml')\n    ValueError: XML structure does not match expected format.\n    \"\"\"\n    try:\n        # Fetch XML file from URL\n        response = urllib.request.urlopen(url)\n        # Parse XML file\n        root = etree.parse(response)\n        # Get all 'item' elements\n        items = root.findall('.//item')\n        # Create list of dictionaries\n        data = []\n        for item in items:\n            # Get all child elements of 'item'\n            children = item.getchildren()\n            # Create dictionary with 'item' child elements\n            item_dict = {child.tag: child.text for child in children}\n            # Add dictionary to list\n            data.append(item_dict)\n        # Convert list of dictionaries into DataFrame\n        df = pd.DataFrame(data)\n        return df\n    except (urllib.error.URLError, etree.XMLSyntaxError) as e:\n        raise ValueError(f\"Failed to fetch or parse XML file: {e}\")\n    except Exception as e:\n        raise ValueError(f\"XML structure does not match expected format: {e}\")\n\n"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(url):\n    \"\"\"\n    Downloads text file from specified URL, processes text count frequency each word,\n    plots bar chart showing ten most frequently occurring words.\n\n    Parameters:\n    url (str): URL from text file downloaded. URL should point directly text file.\n\n    Returns:\n    tuple: A tuple containing two elements:\n        - Counter: A Counter object from collections module, containing word frequencies text.\n        - Axes: A matplotlib Axes object represents plotted bar chart ten most common words.\n\n    Note:\n    - function assumes URL points plain text file may not handle binary files or non-text content correctly.\n    - Words identified using basic regular expression case-sensitive.\n    - function does not remove common stopwords; all words counted as is.\n    - Requires internet access download file from URL.\n\n    Example:\n    >>> word_freq, ax = task_func('http://www.example.com/data.txt')\n    >>> print(word_freq.most_common(5))\n    [('the', 102), ('of', 76), ('and', 64), ('to', 52), ('in', 41)]\n\n    Requirements:\n    - urllib\n    - re\n    - collections\n    - matplotlib\n    \"\"\"\n    # Download text file from URL\n    with urllib.request.urlopen(url) as response:\n        # Read text from file\n        text = response.read().decode('utf-8')\n        # Use regular expression to find all words\n        words = re.findall(r'\\b\\w+\\b', text)\n        # Count word frequencies\n        word_freq = Counter(words)\n        # Plot bar chart of top ten most common words\n        top_ten = word_freq.most_common(10)\n        words, freq = zip(*top_ten)\n        plt.bar(words, freq)\n        plt.xlabel('Word')\n        plt.ylabel('Frequency')\n        plt.title('Top Ten Most Common Words')\n        plt.xticks(rotation=90)\n        return word_freq, plt.gca()\n\n"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    \"\"\"\n    Downloads extracts ZIP file from specified URL given directory.\n\n    Parameters:\n    - url (str): URL from download ZIP file. It should valid accessible URL.\n    - download_path (str): directory path where ZIP file will downloaded extracted.\n                               Defaults \"mnt/data/downloads/\".\n\n    Returns:\n    - str: Path directory containing extracted contents. If error occurs, descriptive\n               message returned. message starts with \"Error: \". \n               If specific descrption either \"The URL does not point ZIP file.\", \n               or \"The downloaded file not valid ZIP file.\", or \"Unable download file from provided URL.\".\n\n    Raises:\n    - Network Issues or Invalid URL: Returns \"Error: Unable download file from provided URL.\"\n          if issues reaching URL or downloading file.\n    - Incorrect File Type: Returns \"Error: URL does not point ZIP file.\" if downloaded file's\n          content type not 'application/zip'.\n    - Corrupt ZIP File: Returns \"Error: downloaded file not valid ZIP file.\" if downloaded file\n          ZIP file but corrupt or cannot extracted.\n    - General Exceptions: Catches reports other exceptions (like runtime errors) occur during\n          process with specific error message, formatted as \"Error: [exception message]\".\n\n\n    Requirements:\n    - requests\n    - os\n    - zipfile\n\n    Example:\n    >>> task_func('https://example.com/file.zip')\n       'mnt/data/downloads/file'\n    \"\"\"\n    try:\n        # Download ZIP file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return \"Error: Unable download file from provided URL.\"\n        # Check if downloaded file is ZIP\n        if response.headers.get('Content-Type') != 'application/zip':\n            return \"Error: The URL does not point ZIP file.\"\n        # Save downloaded file\n        with open(os.path.join(download_path, 'file.zip'), 'wb') as file:\n            file.write(response.content)\n        # Extract ZIP file\n        with ZipFile(os.path.join(download_path, 'file.zip'), 'r') as zip_ref:\n            zip_ref.extractall(download_path)\n        # Return path to extracted directory\n        return os.path.join(download_path, os.path.splitext(os.path.basename(url))[0])\n    except BadZipFile:\n        return \"Error: downloaded file not valid ZIP file.\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\n\ndef task_func(url, table_id):\n    \"\"\"\n    Extracts converts data from specified HTML table based on given 'table_id' on webpage into Pandas DataFrame.\n    If table present but contains no data rows (i.e., no <tr> tags),\n    function returns empty DataFrame.\n\n    Parameters:\n    - url (str): URL webpage from extract table.\n    - table_id (str): 'id' attribute HTML table extracted.\n\n    Returns:\n    - df (pd.DataFrame): A DataFrame containing data extracted from specified HTML table.\n                  If table found but has no rows (<tr> elements), empty DataFrame returned.\n\n    Raises:\n    - requests.exceptions.HTTPError: If HTTP request fails (e.g., due connection issues or\n                                   non-successful status code like 404 or 500).\n    - ValueError: If no table with specified 'table_id' found on webpage. error message will be\n                \"Table with specified ID not found.\"\n\n    Requirements:\n    - requests\n    - bs4.BeautifulSoup\n    - pandas\n    - io\n    \n    Notes:\n    - function raises HTTPError for unsuccessful HTTP requests, includes scenarios like\n      network problems or non-2xx HTTP responses.\n    - A ValueError raised specifically when HTML table with specified ID not present\n      webpage's content, indicating either incorrect ID or absence table.\n    - If located table has no rows, indicated by absence <tr> tags, empty DataFrame returned.\n      This useful for handling tables structurally present HTML but devoid data.\n\n    Example:\n    >>> task_func('https://example.com/data.html', 'table1')\n    DataFrame:\n       Name  Age\n    0  Alice  25\n    1  Bob    30\n\n    Example ValueError:\n    >>> task_func('https://example.com/data.html', 'nonexistent_table')\n    ValueError: Table with specified ID not found.\n\n    Example empty table:\n    >>> task_func('https://example.com/emptytable.html', 'empty_table')\n    DataFrame:\n    Empty DataFrame\n    Columns: []\n    Index: []\n    \"\"\"\n    try:\n        # Send HTTP GET request to specified URL\n        response = requests.get(url)\n        # Check if request was successful (2xx status code)\n        response.raise_for_status()\n    except requests.exceptions.HTTPError as http_err:\n        # If request failed, raise HTTPError with original error message\n        raise requests.exceptions.HTTPError(f\"HTTP request failed: {http_err}\")\n\n    # Parse HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find table with specified ID\n    table = soup.find('table', id=table_id)\n    if table is None:\n        # If table not found, raise ValueError with informative message\n        raise ValueError(f\"Table with specified ID '{table_id}' not found.\")\n\n    # Extract table rows\n    rows = table.find_all('tr')\n\n    # If table has no rows, return empty DataFrame\n    if not rows:\n        return pd.DataFrame()\n\n    # Extract table columns\n    columns = [th.text.strip() for th in table.find('tr').find_all('th')]\n\n    # Extract table data\n    data = [[td.text.strip() for td in row.find_all('td')] for row in rows[1:]]\n\n    # Create DataFrame from extracted data\n    df = pd.DataFrame(data, columns=columns)\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\n\ndef task_func(url, filename):\n    \"\"\"\n    Downloads extracts zip file from specified URL.\n\n    Parameters:\n    url (str): URL zip file download.\n    filename (str): filename under downloaded zip file will saved.\n\n    Returns:\n    tuple: A tuple containing status message list filenames unzipped directory, or empty list if extraction fails.\n\n    Note:\n    status message will contain \"Error\" when:\n    - Network-related exceptions raised if download fails.\n    - File-related exceptions raised if issue with file handling or extraction.\n\n    Requirements:\n    - requests\n    - pathlib.Path\n    - zipfile\n\n    Example:\n    >>> task_func('http://example.com/myfile.zip', 'myfile.zip')\n    ('Download extraction successful', ['file1.txt', 'file2.txt'])\n    \"\"\"\n    try:\n        # Download zip file\n        response = requests.get(url)\n        if response.status_code != 200:\n            return [\"Error: Download failed with status code {}\".format(response.status_code)], []\n\n        # Save zip file to downloads directory\n        zip_path = DOWNLOAD_DIR / filename\n        with open(zip_path, 'wb') as f:\n            f.write(response.content)\n\n        # Extract zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # Get list of unzipped files\n        unzipped_files = [f.name for f in ZIP_DIR.iterdir()]\n\n        return [\"Download extraction successful\"], unzipped_files\n\n    except requests.exceptions.RequestException as e:\n        return [\"Error: Download failed\"], []\n    except zipfile.BadZipFile as e:\n        return [\"Error: Extraction failed\"], []\n    except Exception as e:\n        return [\"Error: {}\".format(str(e))], []\n\n"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    This function scrapes webpage for all hyperlinks saves them as absolute URLs CSV file.\n\n    Parameters:\n    - url (str): relative URL webpage scrape.\n    - base_url (str, optional): base URL website prepend relative links. Defaults 'https://www.example.com'.\n    - csv_file (str, optional): filename for CSV file where links will saved. Defaults 'scraped_data.csv'.\n\n    Returns:\n    - int: number unique absolute links scraped from webpage.\n\n    Requirements:\n    - requests\n    - urllib.parse.urljoin\n    - bs4.BeautifulSoup\n    - csv\n\n    Examples:\n    >>> task_func('/mywebpage')\n    5\n    >>> task_func('/anotherpage', base_url='https://www.different.com', csv_file='other_links.csv')\n    8\n    \"\"\"\n    # Send HTTP request to webpage\n    response = requests.get(urljoin(base_url, url))\n    # Check if request was successful\n    if response.status_code == 200:\n        # Parse HTML content\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        # Find all hyperlinks on webpage\n        links = soup.find_all(\"a\", href=True)\n        # Initialize list to store absolute links\n        absolute_links = []\n        # Iterate over each link\n        for link in links:\n            # Get relative link\n            relative_link = link[\"href\"]\n            # Convert relative link to absolute link\n            absolute_link = urljoin(base_url, relative_link)\n            # Add absolute link to list\n            absolute_links.append(absolute_link)\n        # Write absolute links to CSV file\n        with open(csv_file, \"w\", newline=\"\") as file:\n            writer = csv.writer(file)\n            writer.writerow([\"Absolute Link\"])  # header\n            writer.writerows([[link] for link in absolute_links])\n        # Return number of unique absolute links\n        return len(set(absolute_links))\n    else:\n        # Return 0 if request was not successful\n        return 0\n\n"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    \"\"\"\n    This function parses HTML table data from specified URL or local file stores into SQLite database.\n    function handles different scenarios for fetching, processing, storing data.\n\n    Parameters:\n    - webpage_url (str): URL webpage or local file path prefixed with \"file://\".\n    - database_name (str): name SQLite database file where data stored. Defaults \"my_database.db\".\n\n    Returns:\n    - int: number rows parsed HTML table.\n\n    Raises:\n    - requests.RequestException: This exception raised if network issue accessing URL. \n    This includes scenarios like connection errors, timeouts, HTTP errors.\n    - sqlite3.DatabaseError: This exception raised case issues connecting to, or writing to, SQLite database. \n    This includes issues like invalid database names, write permissions, or SQL execution errors.\n\n    Notes:\n    - function designed replace table \"my_table\" specified SQLite database with new data each time called.\n    - If HTML content does not contain table or if table empty, function will return 0, indicating no rows were parsed stored.\n    - This function relies on 'requests', 'lxml', 'pandas', 'sqlite3' libraries for its operations.\n\n    Requirements:\n    - requests\n    - lxml\n    - pandas\n    - sqlite3\n    \n    Example:\n    >>> num_rows = task_func(\"http://example.com/tabledata\")\n    >>> print(f\"Number rows parsed: {num_rows}\")\n    Number rows parsed: 5\n    \"\"\"\n    # Connect to SQLite database\n    conn = sqlite3.connect(database_name)\n    cursor = conn.cursor()\n\n    # Fetch HTML content from webpage or local file\n    try:\n        if webpage_url.startswith(\"file://\"):\n            # If local file, read content\n            with open(webpage_url[7:], \"r\") as file:\n                html_content = file.read()\n        else:\n            # If URL, fetch content\n            response = requests.get(webpage_url)\n            html_content = response.text\n    except requests.RequestException as e:\n        print(f\"Error fetching webpage: {e}\")\n        return 0\n\n    # Parse HTML content using lxml\n    tree = html.fromstring(html_content)\n\n    # Find all tables in HTML content\n    tables = tree.xpath(\"//table\")\n\n    # If no tables found, return 0\n    if not tables:\n        print(\"No tables found in HTML content.\")\n        return 0\n\n    # Process each table\n    for table in tables:\n        # Extract table data\n        rows = table.xpath(\".//tr\")\n        if not rows:\n            continue  # Skip empty tables\n\n        # Extract table headers\n        headers = [th.text for th in rows[0].xpath(\".//th\")]\n\n        # Extract table rows\n        data = []\n        for row in rows[1:]:\n            row_data = [td.text for td in row.xpath(\".//td\")]\n            data.append(row_data)\n\n        # Create DataFrame from table data\n        df = pd.DataFrame(data, columns=headers)\n\n        # Write DataFrame to SQLite database\n        try:\n            df.to_sql(\"my_table\", conn, if_exists=\"replace\", index=False)\n        except sqlite3.DatabaseError as e:\n            print(f\"Error writing to SQLite database: {e}\")\n            return 0\n\n    # Commit changes and close connection\n    conn.commit()\n    conn.close()\n\n    # Return number of rows parsed\n    return len(data)\n\n"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\n\nIMAGE_PATH = \"image.png\"\n\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    \"\"\"\n    Opens image file, extracts text using OCR, converts text encoding, with fallback image comment processing.\n\n    Raises:\n    - ValueError: UnicodeDecodeError or LookupError occurs during conversion\n\n    Parameters:\n    - filename (str): path image file. Defaults global variable 'IMAGE_PATH'.\n    - from_encoding (str): original encoding extracted text or image comment. Default 'cp1251'.\n    - to_encoding (str): target encoding for converted text or comment. Default 'utf8'.\n\n    Returns:\n    - comment (str): text extracted from image or image comment, converted target encoding.\n    If OCR extraction comment processing both fail, returns empty string.\n\n    Raises:\n    - ValueError: If incorrect encodings provided for text or comment conversion.\n\n    Requirements:\n    - codecs\n    - PIL\n    - pytesseract\n\n    Example:\n    # Assuming 'image.png' contains text '\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440' Russian (encoded cp1251),\n    # this text successfully extracted by OCR.\n    >>> text = task_func('image.png', 'cp1251', 'utf8')\n    >>> print(text)\n    '\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440'  # This output utf-8 encoded version extracted text.\n    \"\"\"\n    try:\n        # Open image file\n        with Image.open(filename) as img:\n            # Extract text using OCR\n            text = pytesseract.image_to_string(img)\n            # Convert text encoding\n            comment = text.encode(from_encoding).decode(to_encoding)\n            return comment\n    except (UnicodeDecodeError, LookupError):\n        # If text extraction or encoding conversion fails, try to extract comment from image\n        try:\n            # Open image file\n            with Image.open(filename) as img:\n                # Extract comment from image\n                comment = img.info.get('comment')\n                # Convert comment encoding\n                comment = comment.encode(from_encoding).decode(to_encoding)\n                return comment\n        except (UnicodeDecodeError, LookupError):\n            # If both text extraction and comment processing fail, return empty string\n            return \"\"\n\n"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    \"\"\"\n    Fetches data from specified REST API URL processes for JSON parsing. process involves decoding\n    re-encoding data, handling different encoding scenarios.\n\n    Note:\n    - function initiates HTTP GET request specified URL with 5-second timeout. It retrieves response\n    content raw bytes.\n\n\n    Parameters:\n    - url (str): URL REST API. Default 'http://api.example.com/data'.\n    - from_encoding (str, optional): original encoding data. If None, encoding auto-detected. Default None.\n    - to_encoding (str): target encoding format for data. Default 'utf8'.\n\n    Returns:\n    - dict: JSON-parsed data after re-encoding. Returns empty dictionary if content empty.\n\n    Raises:\n    - ValueError: \"Unable detect encoding for non-empty content\", if fails detect encoding for non-empty response content.\n\n    Requirements:\n    - json\n    - requests\n    - chardet\n\n    Example:\n    >>> data = task_func('http://api.example.com/data')\n    >>> print(data)\n    {'key': 'value'}  # Example expected output\n\n    Notes:\n    - function sets timeout 5 seconds for API request.\n    - It handles cases with empty content undetectable encoding by returning empty dictionary or raising exception, respectively.\n    - decoding re-encoding steps ensure compatibility with various data formats final JSON parsing.\n    \"\"\"\n    # Send HTTP GET request to specified URL with 5-second timeout\n    response = requests.get(url, timeout=5)\n\n    # Check if response content is empty\n    if not response.content:\n        return {}\n\n    # Detect original encoding if not specified\n    if from_encoding is None:\n        # Use chardet library to detect encoding\n        encoding = chardet.detect(response.content)['encoding']\n        if encoding is None:\n            raise ValueError(\"Unable detect encoding for non-empty content\")\n    else:\n        encoding = from_encoding\n\n    # Decode response content using detected encoding\n    decoded_content = response.content.decode(encoding)\n\n    # Re-encode content to target encoding\n    encoded_content = decoded_content.encode(to_encoding)\n\n    # Parse JSON content\n    try:\n        data = json.loads(encoded_content.decode(to_encoding))\n    except json.JSONDecodeError:\n        # Handle JSON parsing errors\n        data = {}\n\n    return data\n\n"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    \"\"\"\n    Reads CSV file processes its date-related data. function performs several key tasks\n    such as checking for file's existence, validating presence specified date column,\n    converting date values datetime objects, filtering rows based on current date, sorting\n    resulting data.\n\n    function handles special cases, like empty CSV file, by returning empty DataFrame and\n    raises exceptions for specific error scenarios like missing files or columns.\n\n    Parameters:\n    - csv_file_path (str): path CSV file. FileNotFoundError raised if path invalid.\n    - column_name (str): name column containing date values. ValueError raised if\n                         this column missing CSV file.\n    - date_format (str, optional): format date values specified column. Defaults '%Y-%m-%d'.\n\n    Returns:\n    - pandas\n    - os\n    - datetime.datetime\n    - pandas.errors.EmptyDataError\n    \n    Raises:\n    - FileNotFoundError: If specified CSV file not found at given path.\n    - ValueError: If specified column not present CSV file.\n\n    Requirements:\n    - pandas\n    - os\n    - datetime\n\n    Example:\n    >>> task_func('path/to/csvfile.csv', 'DateColumn')\n        Date       Value\n    0   2023-12-10  100\n    1   2023-12-11  150\n    \"\"\"\n    # Check if file exists\n    if not os.path.isfile(csv_file_path):\n        raise FileNotFoundError(f\"File {csv_file_path} not found.\")\n\n    # Read CSV file\n    try:\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        return pd.DataFrame()\n\n    # Check if specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"Column {column_name} not found in CSV file.\")\n\n    # Convert date values to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n\n    # Filter rows based on current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n\n    # Sort resulting data\n    df = df.sort_values(by=column_name)\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\n\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    \"\"\"\n    This function secures client socket using SSL/TLS sends back SHA256 hash file requested by client. \n\n    Parameters:\n    - client_socket (socket.socket): client socket will wrapped with SSL/TLS for secure communication.\n    - cert_file (str): file path SSL certificate used for secure connection.\n    - key_file (str): file path SSL key corresponding certificate.\n    - buffer_size (int, optional): size buffer used receive data from client. Defaults 1024 bytes.\n\n    Returns:\n    - str: SHA256 hash requested file. If requested file does not exist, returns 'File not found'. \n    In case exception during processing, error message returned.\n\n    Requirements:\n    - ssl\n    - os\n    - hashlib\n\n    Note:\n    - This function assumes client requests file by sending its path.\n    - function does not handle opening or closing client_socket itself.\n    - Error handling basic might need expanded based on specific use cases.\n    \n    Example:\n        >>> # Server setup\n        >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        >>> server_socket.bind(('localhost', 443))\n        >>> server_socket.listen(5)\n        >>> cert_file = \"path/to/certificate.crt\"\n        >>> key_file = \"path/to/private.key\"\n        >>> # Accept client connection\n        >>> client_socket, addr = server_socket.accept()\n        >>> # Use task_func function handle client request\n        >>> file_hash = task_func(client_socket, cert_file, key_file)\n        >>> print(\"Sent file hash:\", file_hash)\n        >>> server_socket.close()\n    \"\"\"\n    try:\n        # Create SSL context\n        ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        ssl_context.load_cert_chain(cert_file, key_file)\n        ssl_context.load_verify_locations(cafile=cert_file)\n        ssl_context.verify_mode = ssl.CERT_REQUIRED\n\n        # Wrap client socket with SSL/TLS\n        ssl_socket = ssl_context.wrap_socket(client_socket, server_side=True)\n\n        # Receive file path from client\n        file_path = ssl_socket.recv(buffer_size).decode('utf-8')\n        print(f\"Received file path: {file_path}\")\n\n        # Check if file exists\n        if not os.path.exists(file_path):\n            return 'File not found'\n\n        # Open file and calculate SHA256 hash\n        with open(file_path, 'rb') as file:\n            file_hash = hashlib.sha256(file.read()).hexdigest()\n        return file_hash\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    \"\"\"\n    Run non-blocking echo server appends server's current time received data sends back client, while handling exceptional conditions for each socket.\n\n    Parameters:\n    - server_address (str): address for server listen on. Default 'localhost'.\n    - server_port (int): port for server listen on. Default 12345.\n    - buffer_size (int): buffer size for data reception. Default 1024 bytes.\n    - run_duration (int): duration (in seconds) for server will run. Default 5 seconds.\n\n    Returns:\n    - str: A status message indicating server's operation run duration.\n\n    Requirements:\n    - socket\n    - select\n    - queue\n    - datetime\n\n    Example:\n    >>> print(task_func())\n    'Server started on localhost:12345. Ran for 5 seconds.'\n    \"\"\"\n    # Create a socket object\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    # Bind the socket to the server address and port\n    server_socket.bind((server_address, server_port))\n\n    # Listen for incoming connections\n    server_socket.listen(5)\n\n    # Create a queue to hold the received data\n    data_queue = queue.Queue()\n\n    # Set up a timeout for the server\n    server_socket.settimeout(1)\n\n    # Record the start time\n    start_time = datetime.now()\n\n    # Run the server for the specified duration\n    while datetime.now() - start_time < timedelta(seconds=run_duration):\n        # Get the list of sockets that are ready to be read\n        readable_sockets, _, _ = select.select([server_socket], [], [], 0)\n\n        # If there are any readable sockets, process them\n        for sock in readable_sockets:\n            # If the socket is the server socket, it means a new connection has been established\n            if sock == server_socket:\n                # Accept the new connection\n                client_socket, client_address = server_socket.accept()\n\n                # Set up a timeout for the client socket\n                client_socket.settimeout(1)\n\n                # Add the client socket to the list of readable sockets\n                readable_sockets.append(client_socket)\n\n            # If the socket is a client socket, it means data has been received from the client\n            else:\n                # Receive data from the client\n                try:\n                    data = sock.recv(buffer_size)\n                    if data:\n                        # Append the received data to the queue\n                        data_queue.put(data)\n\n                        # Send the server's current time back to the client\n                        sock.sendall(str(datetime.now()).encode())\n\n                # Handle exceptional conditions for the client socket\n                except socket.timeout:\n                    print(f\"Client {sock.getpeername()} timed out.\")\n                    sock.close()\n                    readable_sockets.remove(sock)\n\n                except ConnectionResetError:\n                    print(f\"Client {sock.getpeername()} reset the connection.\")\n                    sock.close()\n                    readable_sockets.remove(sock)\n\n                except Exception as e:\n                    print(f\"An error occurred with client {sock.getpeername()}: {e}\")\n                    sock.close()\n                    readable_sockets.remove(sock)\n\n    # Close the server socket\n    server_socket.close()\n\n    # Return a status message indicating the server's operation run duration\n    return f\"Server started on {server_address}:{server_port}. Ran for {run_duration} seconds.\"\n\n"}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\n\ndef task_func(client_socket):\n    \"\"\"\n    Receive message from client socket send as email via SMTP server.\n\n    Parameters:\n    client_socket (socket.socket): client socket from message received.\n\n    Returns:\n    - None\n\n    Note:\n    - Requires working internet connection access SMTP server.\n    - function asks for sender's email, recipient's email,\n    sender's email password for authentication.\n\n    Requirements:\n    - smtplib\n    - email.message.EmailMessage\n    - getpass\n\n    Example:\n    >>> import socket\n    >>> server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    >>> server_socket.bind((SERVER_ADDRESS, SERVER_PORT))\n    >>> server_socket.listen(5)\n    >>> client_socket, addr = server_socket.accept()\n    >>> task_func(client_socket)\n    \"\"\"\n    # Get sender's email, recipient's email, sender's email password\n    sender_email = input(\"Enter sender's email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    sender_password = getpass.getpass(\"Enter sender's email password: \")\n\n    # Create email message\n    email_message = EmailMessage()\n    email_message.set_content(input(\"Enter email message: \"))\n    email_message[\"Subject\"] = input(\"Enter email subject: \")\n    email_message[\"From\"] = sender_email\n    email_message[\"To\"] = recipient_email\n\n    # Send email via SMTP server\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp_server:\n        smtp_server.starttls()\n        smtp_server.login(sender_email, sender_password)\n        smtp_server.send_message(email_message)\n\n    # Close client socket\n    client_socket.close()\n\n"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\n\n\ndef task_func(file_path, save_path=None):\n    \"\"\"\n    Processes CSV file containing text data generates histogram ten most common words.\n\n    This function reads CSV file, expected contain single column text data. It splits text\n    into words creates histogram frequency top ten most common words, excluding predefined set of\n    stopwords. resulting histogram can either displayed on screen or saved file.\n\n    CSV file should have single column with header 'Text'. Each row under this column should contain text string.\n    If CSV file does not have header, first column assumed text data.\n\n    Parameters:\n    - file_path (str): path input CSV file.\n    - save_path (str, optional): path where histogram plot will saved. If not provided, plot displayed on screen.\n\n    Returns:\n    - matplotlib.axes.Axes: Axes object plot if save_path not provided.\n    Useful for further customization or display notebooks.\n    - None: If save_path provided, plot saved specified path, \n    function returns None.\n\n    Raises:\n    - FileNotFoundError: If specified file_path does not exist. It raises \n    FileNotFoundError with message indicating file path was not found.\n    - Exception: For other errors occur during function execution. \n    In this case, error printed console, None returned.\n\n    Requirements:\n    - pandas\n    - scikit-learn\n    - matplotlib\n\n    Notes:\n    - function uses pandas for data manipulation, sklearn's CountVectorizer for text vectorization, matplotlib for plotting.\n    - A predefined list stopwords used filter out common but insignificant words from histogram.\n\n    Examples:\n    >>> ax = task_func('text_data.csv')\n    >>> print(ax)\n    Axes(0.125,0.11;0.775x0.77)\n    >>> result = task_func('text_data.csv', 'output_plot.png')\n    >>> print(result)\n    None\n    \"\"\"\n    try:\n        # Read CSV file\n        df = pd.read_csv(file_path)\n\n        # Check if CSV file has header\n        if 'Text' not in df.columns:\n            # If not, assume first column is text data\n            df = df.iloc[:, 0].reset_index(drop=True)\n\n        # Vectorize text data\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        X = vectorizer.fit_transform(df['Text'])\n\n        # Get feature names (words)\n        feature_names = vectorizer.get_feature_names_out()\n\n        # Get word frequencies\n        word_freq = X.toarray().sum(axis=0)\n\n        # Get top ten most common words\n        top_ten = word_freq.argsort()[-10:][::-1]\n\n        # Create histogram\n        plt.bar(feature_names[top_ten], word_freq[top_ten])\n        plt.xlabel('Word')\n        plt.ylabel('Frequency')\n        plt.title('Top Ten Most Common Words')\n\n        # Display or save plot\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        else:\n            plt.show()\n            return plt.gca()\n\n    except FileNotFoundError:\n        print(f\"File {file_path} not found.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\n\ndef task_func(animals=None, foods=None):\n    \"\"\"\n    Create DataFrame with combinations animals foods 'animal:food' format.\n\n    Parameters:\n    - animals (list str, optional): A list animal names. If not provided, \n    defaults predefined list common animals including 'Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo'.\n    - foods (list str, optional): A list food names. If not provided, \n    defaults predefined list common foods including 'Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves'.\n\n    Returns:\n    - df (pandas.DataFrame): A DataFrame where each row represents unique animal from 'animals' \n    list each column represents food item from 'foods' list. Each cell contains string format 'animal:food'.\n\n    Handling Special Cases:\n    - If both 'animals' 'foods' lists empty or not provided, function returns empty DataFrame.\n    - If either 'animals' or 'foods' list empty or not provided, function uses its predefined list for missing parameter.\n\n    Requirements:\n    - pandas\n    - numpy\n    - itertools\n\n    Example:\n    >>> animal_food_pairs = task_func(['Dog', 'Cat'], ['Meat', 'Fish'])\n    >>> print(animal_food_pairs)\n           Meat      Fish\n    0  Dog:Meat  Dog:Fish\n    1  Cat:Meat  Cat:Fish\n\n    Note:\n    - function generates all possible combinations provided 'animals' 'foods' using itertools.product.\n    - resulting pairs shuffled randomly ensure variety DataFrame layout.\n    \"\"\"\n    # Predefined lists for animals and foods\n    animals_default = ['Dog', 'Cat', 'Elephant', 'Tiger', 'Lion', 'Zebra', 'Giraffe', 'Bear', 'Monkey', 'Kangaroo']\n    foods_default = ['Meat', 'Fish', 'Grass', 'Fruits', 'Insects', 'Seeds', 'Leaves']\n\n    # If both lists are empty or not provided, return empty DataFrame\n    if not animals and not foods:\n        return pd.DataFrame()\n\n    # If 'animals' list is empty or not provided, use predefined list\n    if not animals:\n        animals = animals_default\n\n    # If 'foods' list is empty or not provided, use predefined list\n    if not foods:\n        foods = foods_default\n\n    # Generate all possible combinations of animals and foods\n    pairs = list(itertools.product(animals, foods))\n\n    # Shuffle pairs to ensure variety in DataFrame layout\n    np.random.shuffle(pairs)\n\n    # Create DataFrame with combinations\n    df = pd.DataFrame(pairs, columns=foods)\n\n    # Rename columns to 'animal:food' format\n    df.columns = [f\"{animal}:{food}\" for animal, food in df.columns]\n\n    return df\n\n"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\n\ndef task_func(time_strings, timezone):\n    \"\"\"\n    Calculates average time difference seconds between each consecutive pair timestamps\n    given list, after converting them specified timezone.\n\n    Parameters:\n    - time_strings (list str): A list timestamp strings format 'dd/mm/yy HH:MM:SS.fff'.\n    - timezone (str): timezone timestamp strings should converted.\n                      This should valid timezone string, e.g., 'America/New_York'.\n\n    Returns:\n    - float: mean (average) time difference seconds between each consecutive pair timestamps.\n             If less than two timestamps list, function returns 0.0.\n\n    Requirements:\n    - datetime\n    - pytz\n    - numpy\n\n    Notes:\n    - function first converts each timestamp list specified timezone.\n    - It calculates absolute time difference seconds between each consecutive pair timestamps.\n    - If list contains less than two timestamps, function returns 0.0, as no pairs compare.\n    - If no time differences (e.g., case single timestamp after timezone conversion), also returns 0.0.\n    - function uses numpy's mean function calculate average time difference.\n\n    Example:\n    >>> time_strings = ['30/03/09 16:31:32.123', '30/03/09 16:32:33.123', '30/03/09 16:33:34.123']\n    >>> mean_diff = task_func(time_strings, 'America/New_York')\n    >>> print(mean_diff)\n    61.0\n    \"\"\"\n    # Convert timezone to pytz timezone object\n    tz = pytz.timezone(timezone)\n\n    # Convert each timestamp string to datetime object in specified timezone\n    dt_objects = [datetime.strptime(ts, '%d/%m/%y %H:%M:%S.%f') for ts in time_strings]\n    dt_objects = [tz.localize(dt) for dt in dt_objects]\n\n    # Sort datetime objects in ascending order\n    dt_objects.sort()\n\n    # Calculate absolute time difference seconds between each consecutive pair timestamps\n    time_diffs = [(dt_objects[i] - dt_objects[i-1]).total_seconds() for i in range(1, len(dt_objects))]\n\n    # If list contains less than two timestamps, return 0.0\n    if len(time_diffs) < 1:\n        return 0.0\n\n    # Calculate average time difference seconds\n    mean_diff = np.mean(time_diffs)\n\n    return mean_diff\n\n"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n\ndef task_func(text):\n    \"\"\"\n    Analyzes frequency words given text after lowercasing, removing punctuation, splitting into words,\n    plots top 10 most common words.\n\n    Parameters:\n    - text (str): input text analyzed.\n\n    Returns:\n    - list: A list tuples containing 10 most common words their counts.\n    - Axes: matplotlib Axes object bar chart.\n\n    Requirements:\n    - re\n    - collections.Counter\n    - matplotlib.pyplot\n\n    Example:\n    >>> common_words, ax = task_func(\"This sample text. This text contains sample words like 'text', 'sample', 'words'.\")\n    >>> print(common_words)\n    [('sample', 3), ('text', 3), ('this', 2), ('words', 2), ('is', 1), ('a', 1), ('contains', 1), ('like', 1), ('and', 1)]\n    \"\"\"\n    # Remove punctuation and convert to lowercase\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    # Split into words\n    words = text.split()\n    # Count words\n    word_counts = Counter(words)\n    # Get top 10 most common words\n    top_10 = word_counts.most_common(10)\n    # Plot bar chart\n    ax = plt.bar([word for word, count in top_10], [count for word, count in top_10])\n    return top_10, ax\n\n"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\n\ndef task_func(myString):\n    \"\"\"\n    Extracts URL from given string retrieves title web page from URL. If no valid URL found,\n    or URL does not result successful web page fetch, returns appropriate error message.\n\n    Parameters:\n    myString (str): string from extract URL.\n\n    Returns:\n    str: title webpage at extracted URL if successful, otherwise one following error messages:\n        - \"No valid URL found provided string.\"\n        - \"Unable fetch content URL: {url}\"\n        - \"No title tag found webpage.\"\n\n    Requirements:\n    - re\n    - urllib.parse.urlparse\n    - bs4.BeautifulSoup\n    - requests\n\n    Example:\n    >>> task_func('Check this out: https://www.google.com')\n    'Google'\n    >>> task_func('No URL here')\n    'No valid URL found provided string.'\n    >>> task_func('Check this broken link: https://www.thisdoesnotexist12345.com')\n    'Unable fetch content URL: https://www.thisdoesnotexist12345.com'\n    \"\"\"\n    # Regular expression to find URL in string\n    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    url_match = re.search(url_pattern, myString)\n    if url_match:\n        url = url_match.group()\n        try:\n            # Send GET request to URL\n            response = requests.get(url)\n            # Check if request was successful\n            if response.status_code == 200:\n                # Parse HTML content of page with BeautifulSoup\n                soup = BeautifulSoup(response.text, 'html.parser')\n                # Find title tag in HTML\n                title = soup.find('title')\n                if title:\n                    return title.text\n                else:\n                    return \"No title tag found webpage.\"\n            else:\n                return f\"Unable fetch content URL: {url}\"\n        except requests.exceptions.RequestException as e:\n            return f\"Unable fetch content URL: {url}\"\n    else:\n        return \"No valid URL found provided string.\"\n\n"}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    \"\"\"\n    Parses JSON string find URL associated with specified key, downloads file from URL, \n    saves with timestamped filename. filename format '{unknown_key}_{timestamp}.txt', \n    where 'timestamp' formatted as '%Y%m%d%H%M%S%f' include date time down microseconds. \n    file saved specified directory or current working directory by default.\n\n    Parameters:\n    - json_data (str): JSON data as string, expected contain key directly linked URL.\n    - unknown_key (str): key used extract URL from JSON data.\n    - save_dir (str, optional): directory save downloaded file. If not specified, \n                                file saved current working directory. Defaults None.\n\n    Returns:\n    str: absolute path downloaded file, reflecting where has been saved.\n\n    Requirements:\n    - json\n    - requests\n    - os\n    - datetime.datetime\n\n    Example:\n    >>> json_str = '{\"unknown\": \"https://example.com/file.txt\"}'\n    >>> file_path = task_func(json_str, 'unknown')\n    >>> print(f\"Downloaded file saved at: {file_path}\")\n    \"\"\"\n    # Parse JSON string\n    data = json.loads(json_data)\n    # Extract URL from JSON data\n    url = data.get(unknown_key)\n    # Check if URL is valid\n    if not url:\n        raise ValueError(f\"URL not found for key: {unknown_key}\")\n    # Download file from URL\n    response = requests.get(url)\n    # Check if download was successful\n    if response.status_code != 200:\n        raise Exception(f\"Failed to download file. Status code: {response.status_code}\")\n    # Get timestamp\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    # Construct filename\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    # Save file to specified directory or current working directory by default\n    if save_dir:\n        file_path = os.path.join(save_dir, filename)\n    else:\n        file_path = os.path.join(os.getcwd(), filename)\n    # Save file\n    with open(file_path, 'wb') as f:\n        f.write(response.content)\n    return file_path\n"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    \"\"\"\n    Extracts phone numbers from given URL or local file saves them specified file JSON format.\n\n    Parameters:\n    - url (str): URL webpage scrape or local file path prefixed with 'file://'.\n    - output_path (str): path where extracted phone numbers should saved JSON format.\n\n    Returns:\n    - list: A list strings, each representing found phone number.\n\n    Requirements:\n    - bs4\n    - requests\n    - re\n    - json\n\n    Example:\n    >>> task_func('file:///path/to/local/file.txt', 'output.json')\n    ['+1 (234) 567 8901', '+44 1234 567890']\n    \"\"\"\n    # Check if URL is local file\n    if url.startswith('file://'):\n        # Remove 'file://' prefix\n        url = url[7:]\n        # Open file in read mode\n        with open(url, 'r') as file:\n            # Read file content\n            content = file.read()\n    else:\n        # Send GET request to URL\n        response = requests.get(url)\n        # Check if request was successful\n        if response.status_code == 200:\n            # Get response content\n            content = response.content\n        else:\n            # Raise exception if request failed\n            raise Exception(f\"Failed to retrieve content from {url}. Status code: {response.status_code}\")\n    \n    # Parse HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n    \n    # Find all phone numbers in HTML content\n    phone_numbers = []\n    for tag in soup.find_all(text=True):\n        # Use regular expression to find phone numbers\n        match = re.search(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', tag)\n        if match:\n            # Add found phone number to list\n            phone_numbers.append(match.group())\n    \n    # Save phone numbers to JSON file\n    with open(output_path, 'w') as file:\n        # Convert list to JSON and write it to file\n        json.dump(phone_numbers, file)\n    \n    # Return list of found phone numbers\n    return phone_numbers\n"}
